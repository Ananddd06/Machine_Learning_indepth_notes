{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf6c8ecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1.00000000e+00, -6.59205679e-15, -4.67971914e-18,\n",
       "          7.98693583e-15, -1.19854600e-15, -5.05624827e-18,\n",
       "         -1.07445276e-16,  7.98698424e-14, -3.87935041e-13],\n",
       "        [-6.59205679e-15,  1.00000000e+00, -1.21711069e-01,\n",
       "          3.36012863e-01, -7.25495867e-02,  4.12244117e-03,\n",
       "          2.20611204e-02, -7.65711343e-02, -1.70397462e-02],\n",
       "        [-4.67971914e-18, -1.21711069e-01,  1.00000000e+00,\n",
       "         -1.60891609e-01, -8.79826412e-02, -2.92283114e-01,\n",
       "          1.62452854e-02,  5.29569256e-03, -1.01817869e-01],\n",
       "        [ 7.98693583e-15,  3.36012863e-01, -1.60891609e-01,\n",
       "          1.00000000e+00,  8.36162859e-01, -7.37328153e-02,\n",
       "         -4.92165010e-03,  1.10694803e-01, -2.93386718e-02],\n",
       "        [-1.19854600e-15, -7.25495867e-02, -8.79826412e-02,\n",
       "          8.36162859e-01,  1.00000000e+00, -7.12668016e-02,\n",
       "         -6.03488296e-03,  7.31608106e-02,  1.66466751e-02],\n",
       "        [-5.05624827e-18,  4.12244117e-03, -2.92283114e-01,\n",
       "         -7.37328153e-02, -7.12668016e-02,  1.00000000e+00,\n",
       "          7.23295206e-02, -1.02498680e-01,  9.42760742e-02],\n",
       "        [-1.07445276e-16,  2.20611204e-02,  1.62452854e-02,\n",
       "         -4.92165010e-03, -6.03488296e-03,  7.23295206e-02,\n",
       "          1.00000000e+00,  5.83742329e-03, -5.97683144e-04],\n",
       "        [ 7.98698424e-14, -7.65711343e-02,  5.29569256e-03,\n",
       "          1.10694803e-01,  7.31608106e-02, -1.02498680e-01,\n",
       "          5.83742329e-03,  1.00000000e+00, -9.24485214e-01],\n",
       "        [-3.87935041e-13, -1.70397462e-02, -1.01817869e-01,\n",
       "         -2.93386718e-02,  1.66466751e-02,  9.42760742e-02,\n",
       "         -5.97683144e-04, -9.24485214e-01,  1.00000000e+00]]),\n",
       " array([2.02510609, 1.87225453, 0.04728328, 0.08273287, 1.        ,\n",
       "        0.65732137, 1.27337393, 1.00377637, 1.03815156]),\n",
       " '🟢 Positive Definite → Strictly Convex',\n",
       " 0.555891598695244)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from numpy.linalg import eigvals\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load California Housing Dataset (modern replacement for Boston Housing)\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target.reshape(-1, 1)\n",
    "\n",
    "# Add intercept term (bias)\n",
    "X = np.hstack([np.ones((X.shape[0], 1)), X])  # shape: (n_samples, n_features + 1)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features (excluding intercept)\n",
    "scaler = StandardScaler()\n",
    "X_train[:, 1:] = scaler.fit_transform(X_train[:, 1:])\n",
    "X_test[:, 1:] = scaler.transform(X_test[:, 1:])\n",
    "\n",
    "# Compute Hessian\n",
    "n_train = X_train.shape[0]\n",
    "Hessian = (1 / n_train) * (X_train.T @ X_train)\n",
    "eigenvalues = eigvals(Hessian)\n",
    "\n",
    "# Determine convexity\n",
    "if np.all(eigenvalues > 0):\n",
    "    convexity = \"🟢 Positive Definite → Strictly Convex\"\n",
    "elif np.all(eigenvalues >= 0) and np.any(eigenvalues == 0):\n",
    "    convexity = \"🟡 Positive Semi-Definite → Convex (Flat directions)\"\n",
    "elif np.any(eigenvalues > 0) and np.any(eigenvalues < 0):\n",
    "    convexity = \"⚠️ Indefinite → Non-convex or Saddle Points\"\n",
    "else:\n",
    "    convexity = \"❓ Could not classify\"\n",
    "\n",
    "# Train a Linear Regression model\n",
    "model = LinearRegression(fit_intercept=False)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "Hessian, eigenvalues, convexity, mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2193a26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 🧱 Base class with shared methods\n",
    "class BaseLinearRegression:\n",
    "    def __init__(self, alpha=0.01, epochs=1000):\n",
    "        self.alpha = alpha\n",
    "        self.epochs = epochs\n",
    "        self.theta = None\n",
    "        self.cost_history = []\n",
    "\n",
    "    def hypothesis(self, X):\n",
    "        return X @ self.theta\n",
    "\n",
    "    def compute_cost(self, X, y):\n",
    "        m = len(y)\n",
    "        predictions = self.hypothesis(X)\n",
    "        errors = predictions - y\n",
    "        return (1 / (2 * m)) * np.sum(errors ** 2)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.hypothesis(X)\n",
    "\n",
    "    def plot_cost(self):\n",
    "        plt.plot(range(len(self.cost_history)), self.cost_history)\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Cost\")\n",
    "        plt.title(\"Cost over Epochs\")\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# 📦 1. Batch Gradient Descent\n",
    "class BatchGDRegressor(BaseLinearRegression):\n",
    "    def fit(self, X, y):\n",
    "        m, n = X.shape\n",
    "        self.theta = np.zeros(n)\n",
    "        for _ in range(self.epochs):\n",
    "            predictions = self.hypothesis(X)\n",
    "            gradient = (1 / m) * (X.T @ (predictions - y))\n",
    "            self.theta -= self.alpha * gradient\n",
    "            self.cost_history.append(self.compute_cost(X, y))\n",
    "\n",
    "\n",
    "# 🧩 2. Mini-Batch Gradient Descent\n",
    "class MiniBatchGDRegressor(BaseLinearRegression):\n",
    "    def __init__(self, alpha=0.01, epochs=1000, batch_size=32):\n",
    "        super().__init__(alpha, epochs)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        m, n = X.shape\n",
    "        self.theta = np.zeros(n)  # shape: (n,)\n",
    "        y = y.flatten()  # Ensure shape: (m,)\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            X_shuffled, y_shuffled = shuffle(X, y)\n",
    "            for i in range(0, m, self.batch_size):\n",
    "                end = i + self.batch_size\n",
    "                X_batch = X_shuffled[i:end]\n",
    "                y_batch = y_shuffled[i:end]\n",
    "\n",
    "                predictions = X_batch @ self.theta  # shape: (batch_size,)\n",
    "                errors = predictions - y_batch      # shape: (batch_size,)\n",
    "                gradient = (1 / len(X_batch)) * (X_batch.T @ errors)  # shape: (n,)\n",
    "                self.theta -= self.alpha * gradient\n",
    "\n",
    "            self.cost_history.append(self.compute_cost(X, y.reshape(-1, 1)))\n",
    "\n",
    "\n",
    "\n",
    "# 🔁 3. Stochastic Gradient Descent (SGD)\n",
    "class SGDRegressor(BaseLinearRegression):\n",
    "    def fit(self, X, y):\n",
    "        m, n = X.shape\n",
    "        self.theta = np.zeros(n)\n",
    "        for epoch in range(self.epochs):\n",
    "            X_shuffled, y_shuffled = shuffle(X, y)\n",
    "            for i in range(m):\n",
    "                xi = X_shuffled[i].reshape(1, -1)   # (1, n)\n",
    "                yi = y_shuffled[i]\n",
    "                prediction = self.hypothesis(xi)\n",
    "                gradient = (prediction - yi) * xi\n",
    "                self.theta -= self.alpha * gradient.flatten()\n",
    "            self.cost_history.append(self.compute_cost(X, y))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
