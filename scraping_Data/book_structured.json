{
    "page_1": {
        "content_preview": "CS229 Lecture NotesAndrew Ng and Tengyu MaJune 11, 2023...",
        "python_code": [],
        "formulas": [],
        "explanations": [
            "CS229 Lecture NotesAndrew Ng and Tengyu MaJune 11, 2023"
        ]
    },
    "page_2": {
        "content_preview": "ContentsI Supervised learning 51 Linear regression 81.1 LMS algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . 91.2 The normal equations . . . . . . . . . . . . . . . . . . . . . . . 131.2.1 Matrix derivatives . . . . . . . . . . . . . . . . . . . . . 131.2.2 Least squares revisited . . . . . . . . . . . . . . . . . . 141.3 Probabilistic interpretation . . . . . . . . . . . . . . . . . . . . 151.4 Locally weighted linear regression (optional reading) . . . . . . 172 Classication and l...",
        "python_code": [],
        "formulas": [],
        "explanations": [
            "ContentsI Supervised learning 51 Linear regression 81",
            "2 The normal equations",
            "2 Least squares revisited",
            "3 Probabilistic interpretation",
            "4 Locally weighted linear regression (optional reading)",
            "172 Classication and logistic regression 202",
            "1 Logistic regression",
            "2 Digression: the perceptron learning algorithm",
            "3 Multi-class classication",
            "4 Another algorithm for maximizing `()",
            "273 Generalized linear models 293",
            "1 The exponential family",
            "1 Ordinary least squares",
            "2 Logistic regression",
            "334 Generative learning algorithms 344",
            "1 Gaussian discriminant analysis",
            "1 The multivariate normal distribution",
            "2 The Gaussian discriminant analysis model",
            "3 Discussion: GDA and logistic regression",
            "2 Naive bayes (Option Reading)",
            "2 Event models for text classication"
        ]
    },
    "page_3": {
        "content_preview": "CS229 Spring 20223 25 Kernel methods 485.1 Feature maps . . . . . . . . . . . . . . . . . . . . . . . . . . . 485.2 LMS (least mean squares) with features . . . . . . . . . . . . . 495.3 LMS with the kernel trick . . . . . . . . . . . . . . . . . . . . 495.4 Properties of kernels . . . . . . . . . . . . . . . . . . . . . . . 536 Support vector machines 596.1 Margins: intuition . . . . . . . . . . . . . . . . . . . . . . . . . 596.2 Notation (option reading) . . . . . . . . . . . . . . . . . . . ...",
        "python_code": [],
        "formulas": [],
        "explanations": [
            "CS229 Spring 20223 25 Kernel methods 485",
            "2 LMS (least mean squares) with features",
            "3 LMS with the kernel trick",
            "4 Properties of kernels",
            "536 Support vector machines 596",
            "2 Notation (option reading)",
            "3 Functional and geometric margins (option reading)",
            "4 The optimal margin classier (option reading)",
            "5 Lagrange duality (optional reading)",
            "6 Optimal margin classiers: the dual form (option reading)",
            "7 Regularization and the non-separable case (optional reading)",
            "8 The SMO algorithm (optional reading)",
            "75II Deep learning 797 Deep learning 807",
            "1 Supervised learning with non-linear models",
            "3 Modules in Modern Neural Networks",
            "1 Preliminaries on partial derivatives",
            "2 General strategy of backpropagation",
            "3 Backward functions for basic modules",
            "4 Back-propagation for MLPs",
            "5 Vectorization over training examples",
            "109III Generalization and regularization 1128 Generalization 1138",
            "1 Bias-variance tradeo",
            "1 A mathematical decomposition (for regression)",
            "2 The double descent phenomenon",
            "3 Sample complexity bounds (optional readings)"
        ]
    },
    "page_4": {
        "content_preview": "CS229 Spring 20223 38.3.1 Preliminaries . . . . . . . . . . . . . . . . . . . . . . . 1268.3.2 The case of nite H. . . . . . . . . . . . . . . . . . . . 1288.3.3 The case of innite H. . . . . . . . . . . . . . . . . . 1319 Regularization and model selection 1359.1 Regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . 1359.2 Implicit regularization eect (optional reading) . . . . . . . . . 1379.3 Model selection via cross validation . . . . . . . . . . . . . . . 1399.4 Bayesian sta...",
        "python_code": [],
        "formulas": [],
        "explanations": [
            "CS229 Spring 20223 38",
            "3 The case of innite H",
            "1319 Regularization and model selection 1359",
            "2 Implicit regularization eect (optional reading)",
            "3 Model selection via cross validation",
            "4 Bayesian statistics and regularization",
            "142IV Unsupervised learning 14410 Clustering and the k-means algorithm 14511 EM algorithms 14811",
            "1 EM for mixture of Gaussians",
            "2 Jensen's inequality",
            "3 General EM algorithms",
            "1 Other interpretation of ELBO",
            "4 Mixture of Gaussians revisited",
            "5 Variational inference and variational auto-encoder (optionalreading)",
            "16012 Principal components analysis 16513 Independent components analysis 17113",
            "2 Densities and linear transformations",
            "17414 Self-supervised learning and foundation models 17714",
            "1 Pretraining and adaptation",
            "2 Pretraining methods in computer vision",
            "3 Pretrained large language models",
            "1 Open up the blackbox of Transformers",
            "2 Zero-shot learning and in-context learning"
        ]
    },
    "page_5": {
        "content_preview": "CS229 Spring 20223 4V Reinforcement Learning and Control 18815 Reinforcement learning 18915.1 Markov decision processes . . . . . . . . . . . . . . . . . . . . 19015.2 Value iteration and policy iteration . . . . . . . . . . . . . . . 19215.3 Learning a model for an MDP . . . . . . . . . . . . . . . . . . 19415.4 Continuous state MDPs . . . . . . . . . . . . . . . . . . . . . 19615.4.1 Discretization . . . . . . . . . . . . . . . . . . . . . . . 19615.4.2 Value function approximation . . . . . ....",
        "python_code": [],
        "formulas": [],
        "explanations": [
            "CS229 Spring 20223 4V Reinforcement Learning and Control 18815 Reinforcement learning 18915",
            "1 Markov decision processes",
            "2 Value iteration and policy iteration",
            "3 Learning a model for an MDP",
            "4 Continuous state MDPs",
            "2 Value function approximation",
            "5 Connections between Policy and Value Iteration (Optional)",
            "20316 LQR, DDP and LQG 20616",
            "1 Finite-horizon MDPs",
            "2 Linear Quadratic Regulation (LQR)",
            "3 From non-linear dynamics to LQR",
            "1 Linearization of dynamics",
            "2 Dierential Dynamic Programming (DDP)",
            "4 Linear Quadratic Gaussian (LQG)",
            "21617 Policy Gradient (REINFORCE) 220"
        ]
    },
    "page_6": {
        "content_preview": "Part ISupervised learning5...",
        "python_code": [],
        "formulas": [],
        "explanations": [
            "Part ISupervised learning5"
        ]
    },
    "page_7": {
        "content_preview": "6Let's start by talking about a few examples of supervised learning prob-lems. Suppose we have a dataset giving the living areas and prices of 47houses from Portland, Oregon:Living area (feet2)Price (1000 $s)2104 4001600 3302400 3691416 2323000 540......We can plot this data:500 1000 1500 2000 2500 3000 3500 4000 4500 500001002003004005006007008009001000housing pricessquare feetprice (in $1000)Given data like this, how can we learn to predict the prices of other housesin Portland, as a function ...",
        "python_code": [
            "6Let's start by talking about a few examples of supervised learning prob-lems. Suppose we have a dataset giving the living areas and prices of 47houses from Portland, Oregon:Living area (feet2)Price (1000 $s)2104 4001600 3302400 3691416 2323000 540......We can plot this data:500 1000 1500 2000 2500 3000 3500 4000 4500 500001002003004005006007008009001000housing pricessquare feetprice (in $1000)Given data like this, how can we learn to predict the prices of other housesin Portland, as a function of the size of their living areas?To establish notation for future use, we'll use x(i)to denote the \\input\"variables (living area in this example), also called input features , andy(i)to denote the \\output\" or target variable that we are trying to predict(price). A pair ( x(i);y(i)) is called a training example , and the datasetthat we'll be using to learn|a list of ntraining examples f(x(i);y(i));i=1;:::;ng|is called a training set . Note that the superscript \\( i)\" in thenotation is simply an index into the training set, and has nothing to do withexponentiation. We will also use Xdenote the space of input values, and Ythe space of output values. In this example, X=Y=R.To describe the supervised learning problem slightly more formally, ourgoal is, given a training set, to learn a function h:X7!Y so thath(x) is a\\good\" predictor for the corresponding value of y. For historical reasons, this"
        ],
        "formulas": [
            "i=1;:::;ng|is",
            "X=Y=R.To"
        ],
        "explanations": []
    },
    "page_8": {
        "content_preview": "7functionhis called a hypothesis . Seen pictorially, the process is thereforelike this:Training set house.)(living area ofLearning algorithmh predicted y x(predicted price)of house)When the target variable that we're trying to predict is continuous, suchas in our housing example, we call the learning problem a regression prob-lem. When ycan take on only a small number of discrete values (such asif, given the living area, we wanted to predict if a dwelling is a house or anapartment, say), we call...",
        "python_code": [],
        "formulas": [],
        "explanations": [
            "7functionhis called a hypothesis",
            "Seen pictorially, the process is thereforelike this:Training set house",
            ")(living area ofLearning algorithmh predicted y x(predicted price)of house)When the target variable that we're trying to predict is continuous, suchas in our housing example, we call the learning problem a regression prob-lem",
            "When ycan take on only a small number of discrete values (such asif, given the living area, we wanted to predict if a dwelling is a house or anapartment, say), we call it a classication problem"
        ]
    },
    "page_9": {
        "content_preview": "Chapter 1Linear regressionTo make our housing example more interesting, let's consider a slightly richerdataset in which we also know the number of bedrooms in each house:Living area (feet2)#bedrooms Price (1000 $s)2104 3 4001600 3 3302400 3 3691416 2 2323000 4 540.........Here, thex's are two-dimensional vectors in R2. For instance, x(i)1is theliving area of the i-th house in the training set, and x(i)2is its number ofbedrooms. (In general, when designing a learning problem, it will be up toyou...",
        "python_code": [
            "Chapter 1Linear regressionTo make our housing example more interesting, let's consider a slightly richerdataset in which we also know the number of bedrooms in each house:Living area (feet2)#bedrooms Price (1000 $s)2104 3 4001600 3 3302400 3 3691416 2 2323000 4 540.........Here, thex's are two-dimensional vectors in R2. For instance, x(i)1is theliving area of the i-th house in the training set, and x(i)2is its number ofbedrooms. (In general, when designing a learning problem, it will be up toyou to decide what features to choose, so if you are out in Portland gatheringhousing data, you might also decide to include other features such as whethereach house has a replace, the number of bathrooms, and so on. We'll saymore about feature selection later, but for now let's take the features asgiven.)To perform supervised learning, we must decide how we're going to rep-resent functions/hypotheses hin a computer. As an initial choice, let's saywe decide to approximate yas a linear function of x:h(x) =0+1x1+2x2Here, thei's are the parameters (also called weights ) parameterizing thespace of linear functions mapping from XtoY. When there is no risk of8"
        ],
        "formulas": [],
        "explanations": []
    },
    "page_10": {
        "content_preview": "9confusion, we will drop the subscript in h(x), and write it more simply ash(x). To simplify our notation, we also introduce the convention of lettingx0= 1 (this is the intercept term ), so thath(x) =dXi=0ixi=Tx;where on the right-hand side above we are viewing andxboth as vectors,and heredis the number of input variables (not counting x0).Now, given a training set, how do we pick, or learn, the parameters ?One reasonable method seems to be to make h(x) close toy, at least forthe training exampl...",
        "python_code": [
            "9confusion, we will drop the subscript in h(x), and write it more simply ash(x). To simplify our notation, we also introduce the convention of lettingx0= 1 (this is the intercept term ), so thath(x) =dXi=0ixi=Tx;where on the right-hand side above we are viewing andxboth as vectors,and heredis the number of input variables (not counting x0).Now, given a training set, how do we pick, or learn, the parameters ?One reasonable method seems to be to make h(x) close toy, at least forthe training examples we have. To formalize this, we will dene a functionthat measures, for each value of the 's, how close the h(x(i))'s are to thecorresponding y(i)'s. We dene the cost function :J() =12nXi=1(h(x(i))y(i))2:If you've seen linear regression before, you may recognize this as the familiarleast-squares cost function that gives rise to the ordinary least squaresregression model. Whether or not you have seen it previously, let's keepgoing, and we'll eventually show this to be a special case of a much broaderfamily of algorithms.1.1 LMS algorithmWe want to choose so as to minimize J(). To do so, let's use a searchalgorithm that starts with some \\initial guess\" for , and that repeatedlychangesto makeJ() smaller, until hopefully we converge to a value ofthat minimizes J(). Specically, let's consider the gradient descentalgorithm, which starts with some initial , and repeatedly performs theupdate:j:=j@@jJ():(This update is simultaneously performed for all values of j= 0;:::;d .)Here,is called the learning rate . This is a very natural algorithm thatrepeatedly takes a step in the direction of steepest decrease of J.In order to implement this algorithm, we have to work out what is thepartial derivative term on the right hand side. Let's rst work it out for the"
        ],
        "formulas": [
            "lettingx0= 1",
            "dXi=0ixi=Tx;where",
            "12nXi=1(h(x(i))y(i))2:If",
            "j= 0;:::;d"
        ],
        "explanations": []
    },
    "page_11": {
        "content_preview": "10case of if we have only one training example ( x;y), so that we can neglectthe sum in the denition of J. We have:@@jJ() =@@j12(h(x)y)2= 212(h(x)y)@@j(h(x)y)= (h(x)y)@@j dXi=0ixiy!= (h(x)y)xjFor a single training example, this gives the update rule:1j:=j+y(i)h(x(i))x(i)j:The rule is called the LMS update rule (LMS stands for \\least mean squares\"),and is also known as the Widrow-Ho learning rule. This rule has severalproperties that seem natural and intuitive. For instance, the magnitude ofthe u...",
        "python_code": [
            "10case of if we have only one training example ( x;y), so that we can neglectthe sum in the denition of J. We have:@@jJ() =@@j12(h(x)y)2= 212(h(x)y)@@j(h(x)y)= (h(x)y)@@j dXi=0ixiy!= (h(x)y)xjFor a single training example, this gives the update rule:1j:=j+y(i)h(x(i))x(i)j:The rule is called the LMS update rule (LMS stands for \\least mean squares\"),and is also known as the Widrow-Ho learning rule. This rule has severalproperties that seem natural and intuitive. For instance, the magnitude ofthe update is proportional to the error term (y(i)h(x(i))); thus, for in-stance, if we are encountering a training example on which our predictionnearly matches the actual value of y(i), then we nd that there is little needto change the parameters; in contrast, a larger change to the parameters willbe made if our prediction h(x(i)) has a large error (i.e., if it is very far fromy(i)).We'd derived the LMS rule for when there was only a single trainingexample. There are two ways to modify this method for a training set ofmore than one example. The rst is replace it with the following algorithm:Repeat until convergence fj:=j+nXi=1y(i)h(x(i))x(i)j;(for everyj) (1.1)g1We use the notation \\ a:=b\" to denote an operation (in a computer program) inwhich we setthe value of a variable ato be equal to the value of b. In other words, thisoperation overwrites awith the value of b. In contrast, we will write \\ a=b\" when we areasserting a statement of fact, that the value of ais equal to the value of b."
        ],
        "formulas": [
            "2= 212(h(x)y)@@j(h(x)y)=",
            "dXi=0ixiy!=",
            "nXi=1y(i)h(x(i))x(i)j;(for",
            "a=b\""
        ],
        "explanations": []
    },
    "page_12": {
        "content_preview": "11By grouping the updates of the coordinates into an update of the vector, we can rewrite update (1.1) in a slightly more succinct way::=+nXi=1y(i)h(x(i))x(i)The reader can easily verify that the quantity in the summation in theupdate rule above is just @J()=@j(for the original denition of J). So, thisis simply gradient descent on the original cost function J. This method looksat every example in the entire training set on every step, and is called batchgradient descent . Note that, while gradie...",
        "python_code": [
            "11By grouping the updates of the coordinates into an update of the vector, we can rewrite update (1.1) in a slightly more succinct way::=+nXi=1y(i)h(x(i))x(i)The reader can easily verify that the quantity in the summation in theupdate rule above is just @J()=@j(for the original denition of J). So, thisis simply gradient descent on the original cost function J. This method looksat every example in the entire training set on every step, and is called batchgradient descent . Note that, while gradient descent can be susceptibleto local minima in general, the optimization problem we have posed herefor linear regression has only one global, and no other local, optima; thusgradient descent always converges (assuming the learning rate is not toolarge) to the global minimum. Indeed, Jis a convex quadratic function.Here is an example of gradient descent as it is run to minimize a quadraticfunction.5 10 15 20 25 30 35 40 45 505101520253035404550The ellipses shown above are the contours of a quadratic function. Alsoshown is the trajectory taken by gradient descent, which was initialized at(48,30). The x's in the gure (joined by straight lines) mark the successivevalues ofthat gradient descent went through.When we run batch gradient descent to t on our previous dataset,to learn to predict housing price as a function of living area, we obtain0= 71:27,1= 0:1345. If we plot h(x) as a function of x(area), alongwith the training data, we obtain the following gure:"
        ],
        "formulas": [
            "nXi=1y(i)h(x(i))x(i)The",
            "obtain0= 71:27,1="
        ],
        "explanations": []
    },
    "page_13": {
        "content_preview": "12500 1000 1500 2000 2500 3000 3500 4000 4500 500001002003004005006007008009001000housing pricessquare feetprice (in $1000)If the number of bedrooms were included as one of the input features as well,we get0= 89:60;1= 0:1392,2=8:738.The above results were obtained with batch gradient descent. There isan alternative to batch gradient descent that also works very well. Considerthe following algorithm:Loopffori= 1 ton,fj:=j+y(i)h(x(i))x(i)j;(for everyj) (1.2)ggBy grouping the updates of the coordin...",
        "python_code": [
            "12500 1000 1500 2000 2500 3000 3500 4000 4500 500001002003004005006007008009001000housing pricessquare feetprice (in $1000)If the number of bedrooms were included as one of the input features as well,we get0= 89:60;1= 0:1392,2=8:738.The above results were obtained with batch gradient descent. There isan alternative to batch gradient descent that also works very well. Considerthe following algorithm:Loopffori= 1 ton,fj:=j+y(i)h(x(i))x(i)j;(for everyj) (1.2)ggBy grouping the updates of the coordinates into an update of the vector, we can rewrite update (1.2) in a slightly more succinct way::=+y(i)h(x(i))x(i)In this algorithm, we repeatedly run through the training set, and eachtime we encounter a training example, we update the parameters accordingto the gradient of the error with respect to that single training example only.This algorithm is called stochastic gradient descent (also incrementalgradient descent ). Whereas batch gradient descent has to scan throughthe entire training set before taking a single step|a costly operation if nislarge|stochastic gradient descent can start making progress right away, and"
        ],
        "formulas": [
            "get0= 89:60;1=",
            "2=8:738.The",
            "Loopffori= 1"
        ],
        "explanations": []
    },
    "page_14": {
        "content_preview": "13continues to make progress with each example it looks at. Often, stochasticgradient descent gets \\close\" to the minimum much faster than batch gra-dient descent. (Note however that it may never \\converge\" to the minimum,and the parameters will keep oscillating around the minimum of J(); butin practice most of the values near the minimum will be reasonably goodapproximations to the true minimum.2) For these reasons, particularly whenthe training set is large, stochastic gradient descent is ofte...",
        "python_code": [
            "13continues to make progress with each example it looks at. Often, stochasticgradient descent gets \\close\" to the minimum much faster than batch gra-dient descent. (Note however that it may never \\converge\" to the minimum,and the parameters will keep oscillating around the minimum of J(); butin practice most of the values near the minimum will be reasonably goodapproximations to the true minimum.2) For these reasons, particularly whenthe training set is large, stochastic gradient descent is often preferred overbatch gradient descent.1.2 The normal equationsGradient descent gives one way of minimizing J. Let's discuss a second wayof doing so, this time performing the minimization explicitly and withoutresorting to an iterative algorithm. In this method, we will minimize Jbyexplicitly taking its derivatives with respect to the j's, and setting them tozero. To enable us to do this without having to write reams of algebra andpages full of matrices of derivatives, let's introduce some notation for doingcalculus with matrices.1.2.1 Matrix derivativesFor a function f:Rnd7!Rmapping from n-by-dmatrices to the realnumbers, we dene the derivative of fwith respect to Ato be:rAf(A) =264@f@A11@f@A1d.........@f@An1@f@And375Thus, the gradient rAf(A) is itself an n-by-dmatrix, whose ( i;j)-element is@f=@Aij. For example, suppose A=A11A12A21A22is a 2-by-2 matrix, andthe function f:R227!Ris given byf(A) =32A11+ 5A212+A21A22:2By slowly letting the learning rate decrease to zero as the algorithm runs, it is alsopossible to ensure that the parameters will converge to the global minimum rather thanmerely oscillate around the minimum."
        ],
        "formulas": [
            "f=@Aij.",
            "A=A11A12A21A22is"
        ],
        "explanations": []
    },
    "page_15": {
        "content_preview": "14Here,Aijdenotes the ( i;j) entry of the matrix A. We then haverAf(A) =3210A12A22A21:1.2.2 Least squares revisitedArmed with the tools of matrix derivatives, let us now proceed to nd inclosed-form the value of that minimizes J(). We begin by re-writing Jinmatrix-vectorial notation.Given a training set, dene the design matrix Xto be then-by-dmatrix(actuallyn-by-d+ 1, if we include the intercept term) that contains thetraining examples' input values in its rows:X=26664| (x(1))T|| (x(2))T|...| (x(...",
        "python_code": [
            "14Here,Aijdenotes the ( i;j) entry of the matrix A. We then haverAf(A) =3210A12A22A21:1.2.2 Least squares revisitedArmed with the tools of matrix derivatives, let us now proceed to nd inclosed-form the value of that minimizes J(). We begin by re-writing Jinmatrix-vectorial notation.Given a training set, dene the design matrix Xto be then-by-dmatrix(actuallyn-by-d+ 1, if we include the intercept term) that contains thetraining examples' input values in its rows:X=26664| (x(1))T|| (x(2))T|...| (x(n))T|37775:Also, let~ ybe then-dimensional vector containing all the target values fromthe training set:~ y=26664y(1)y(2)...y(n)37775:Now, since h(x(i)) = (x(i))T, we can easily verify thatX~ y=264(x(1))T...(x(n))T375264y(1)...y(n)375=264h(x(1))y(1)...h(x(n))y(n)375:Thus, using the fact that for a vector z, we have that zTz=Piz2i:12(X~ y)T(X~ y) =12nXi=1(h(x(i))y(i))2=J()"
        ],
        "formulas": [
            "X=26664|",
            "y=26664y(1)y(2)...y(n)37775:Now,",
            "y=264(x(1))T...(x(n))T375264y(1)...y(n)375=264h(x(1))y(1)...h(x(n))y(n)375:Thus,",
            "zTz=Piz2i:12(X~",
            "12nXi=1(h(x(i))y(i))2=J()"
        ],
        "explanations": []
    },
    "page_16": {
        "content_preview": "15Finally, to minimize J, let's nd its derivatives with respect to . Hence,rJ() =r12(X~ y)T(X~ y)=12r(X)TX(X)T~ y~ yT(X) +~ yT~ y=12rT(XTX)~ yT(X)~ yT(X)=12rT(XTX)2(XT~ y)T=122XTX2XT~ y=XTXXT~ yIn the third step, we used the fact that aTb=bTa, and in the fth stepused the factsrxbTx=bandrxxTAx= 2Axfor symmetric matrix A(formore details, see Section 4.3 of \\Linear Algebra Review and Reference\"). TominimizeJ, we set its derivatives to zero, and obtain the normal equations :XTX=XT~ yThus, the value ...",
        "python_code": [
            "15Finally, to minimize J, let's nd its derivatives with respect to . Hence,rJ() =r12(X~ y)T(X~ y)=12r(X)TX(X)T~ y~ yT(X) +~ yT~ y=12rT(XTX)~ yT(X)~ yT(X)=12rT(XTX)2(XT~ y)T=122XTX2XT~ y=XTXXT~ yIn the third step, we used the fact that aTb=bTa, and in the fth stepused the factsrxbTx=bandrxxTAx= 2Axfor symmetric matrix A(formore details, see Section 4.3 of \\Linear Algebra Review and Reference\"). TominimizeJ, we set its derivatives to zero, and obtain the normal equations :XTX=XT~ yThus, the value of that minimizes J() is given in closed form by theequation= (XTX)1XT~ y:31.3 Probabilistic interpretationWhen faced with a regression problem, why might linear regression, andspecically why might the least-squares cost function J, be a reasonablechoice? In this section, we will give a set of probabilistic assumptions, underwhich least-squares regression is derived as a very natural algorithm.Let us assume that the target variables and the inputs are related via theequationy(i)=Tx(i)+(i);3Note that in the above step, we are implicitly assuming that XTXis an invertiblematrix. This can be checked before calculating the inverse. If either the number oflinearly independent examples is fewer than the number of features, or if the featuresare not linearly independent, then XTXwill not be invertible. Even in such cases, it ispossible to \\x\" the situation with additional techniques, which we skip here for the sakeof simplicty."
        ],
        "formulas": [
            "y=12rT(XTX)~",
            "T=122XTX2XT~",
            "y=XTXXT~",
            "aTb=bTa,",
            "factsrxbTx=bandrxxTAx=",
            "XTX=XT~",
            "theequation= (XTX)1XT~"
        ],
        "explanations": []
    },
    "page_17": {
        "content_preview": "16where(i)is an error term that captures either unmodeled eects (such asif there are some features very pertinent to predicting housing price, butthat we'd left out of the regression), or random noise. Let us further assumethat the(i)are distributed IID (independently and identically distributed)according to a Gaussian distribution (also called a Normal distribution) withmean zero and some variance 2. We can write this assumption as \\ (i)N(0;2).\" I.e., the density of (i)is given byp((i)) =1p2exp...",
        "python_code": [
            "16where(i)is an error term that captures either unmodeled eects (such asif there are some features very pertinent to predicting housing price, butthat we'd left out of the regression), or random noise. Let us further assumethat the(i)are distributed IID (independently and identically distributed)according to a Gaussian distribution (also called a Normal distribution) withmean zero and some variance 2. We can write this assumption as \\ (i)N(0;2).\" I.e., the density of (i)is given byp((i)) =1p2exp((i))222:This implies thatp(y(i)jx(i);) =1p2exp(y(i)Tx(i))222:The notation \\ p(y(i)jx(i);)\" indicates that this is the distribution of y(i)givenx(i)and parameterized by . Note that we should not condition on (\\p(y(i)jx(i);)\"), sinceis not a random variable. We can also write thedistribution of y(i)asy(i)jx(i);N(Tx(i);2).GivenX(the design matrix, which contains all the x(i)'s) and, whatis the distribution of the y(i)'s? The probability of the data is given byp(~ yjX;). This quantity is typically viewed a function of ~ y(and perhaps X),for a xed value of . When we wish to explicitly view this as a function of, we will instead call it the likelihood function:L() =L(;X;~ y) =p(~ yjX;):Note that by the independence assumption on the (i)'s (and hence also they(i)'s given the x(i)'s), this can also be writtenL() =nYi=1p(y(i)jx(i);)=nYi=11p2exp(y(i)Tx(i))222:Now, given this probabilistic model relating the y(i)'s and thex(i)'s, whatis a reasonable way of choosing our best guess of the parameters ? Theprincipal of maximum likelihood says that we should choose so as tomake the data as high probability as possible. I.e., we should choose tomaximizeL()."
        ],
        "formulas": [
            "nYi=1p(y(i)jx(i);)=nYi=11p2exp(y(i)Tx(i))222:Now,"
        ],
        "explanations": []
    },
    "page_18": {
        "content_preview": "17Instead of maximizing L(), we can also maximize any strictly increasingfunction of L(). In particular, the derivations will be a bit simpler if weinstead maximize the log likelihood `():`() = logL()= lognYi=11p2exp(y(i)Tx(i))222=nXi=1log1p2exp(y(i)Tx(i))222=nlog1p21212nXi=1(y(i)Tx(i))2:Hence, maximizing `() gives the same answer as minimizing12nXi=1(y(i)Tx(i))2;which we recognize to be J(), our original least-squares cost function.To summarize: Under the previous probabilistic assumptions on t...",
        "python_code": [
            "17Instead of maximizing L(), we can also maximize any strictly increasingfunction of L(). In particular, the derivations will be a bit simpler if weinstead maximize the log likelihood `():`() = logL()= lognYi=11p2exp(y(i)Tx(i))222=nXi=1log1p2exp(y(i)Tx(i))222=nlog1p21212nXi=1(y(i)Tx(i))2:Hence, maximizing `() gives the same answer as minimizing12nXi=1(y(i)Tx(i))2;which we recognize to be J(), our original least-squares cost function.To summarize: Under the previous probabilistic assumptions on the data,least-squares regression corresponds to nding the maximum likelihood esti-mate of. This is thus one set of assumptions under which least-squares re-gression can be justied as a very natural method that's just doing maximumlikelihood estimation. (Note however that the probabilistic assumptions areby no means necessary for least-squares to be a perfectly good and rationalprocedure, and there may|and indeed there are|other natural assumptionsthat can also be used to justify it.)Note also that, in our previous discussion, our nal choice of did notdepend on what was 2, and indeed we'd have arrived at the same resulteven if2were unknown. We will use this fact again later, when we talkabout the exponential family and generalized linear models.1.4 Locally weighted linear regression (optionalreading)Consider the problem of predicting yfromx2R. The leftmost gure belowshows the result of tting a y=0+1xto a dataset. We see that the datadoesn't really lie on straight line, and so the t is not very good."
        ],
        "formulas": [
            "lognYi=11p2exp(y(i)Tx(i))222=nXi=1log1p2exp(y(i)Tx(i))222=nlog1p21212nXi=1(y(i)Tx(i))2:Hence,",
            "minimizing12nXi=1(y(i)Tx(i))2;which",
            "y=0+1xto"
        ],
        "explanations": []
    },
    "page_19": {
        "content_preview": "180 1 2 3 4 5 6 700.511.522.533.544.5xy0 1 2 3 4 5 6 700.511.522.533.544.5xy0 1 2 3 4 5 6 700.511.522.533.544.5xyInstead, if we had added an extra feature x2, and ty=0+1x+2x2,then we obtain a slightly better t to the data. (See middle gure) Naively, itmight seem that the more features we add, the better. However, there is alsoa danger in adding too many features: The rightmost gure is the result oftting a 5-th order polynomial y=P5j=0jxj. We see that even though thetted curve passes through the ...",
        "python_code": [
            "180 1 2 3 4 5 6 700.511.522.533.544.5xy0 1 2 3 4 5 6 700.511.522.533.544.5xy0 1 2 3 4 5 6 700.511.522.533.544.5xyInstead, if we had added an extra feature x2, and ty=0+1x+2x2,then we obtain a slightly better t to the data. (See middle gure) Naively, itmight seem that the more features we add, the better. However, there is alsoa danger in adding too many features: The rightmost gure is the result oftting a 5-th order polynomial y=P5j=0jxj. We see that even though thetted curve passes through the data perfectly, we would not expect this tobe a very good predictor of, say, housing prices ( y) for dierent living areas(x). Without formally dening what these terms mean, we'll say the gureon the left shows an instance of undertting |in which the data clearlyshows structure not captured by the model|and the gure on the right isan example of overtting . (Later in this class, when we talk about learningtheory we'll formalize some of these notions, and also dene more carefullyjust what it means for a hypothesis to be good or bad.)As discussed previously, and as shown in the example above, the choice offeatures is important to ensuring good performance of a learning algorithm.(When we talk about model selection, we'll also see algorithms for automat-ically choosing a good set of features.) In this section, let us briey talkabout the locally weighted linear regression (LWR) algorithm which, assum-ing there is sucient training data, makes the choice of features less critical.This treatment will be brief, since you'll get a chance to explore some of theproperties of the LWR algorithm yourself in the homework.In the original linear regression algorithm, to make a prediction at a querypointx(i.e., to evaluate h(x)), we would:1. Fitto minimizePi(y(i)Tx(i))2.2. OutputTx.In contrast, the locally weighted linear regression algorithm does the fol-lowing:1. Fitto minimizePiw(i)(y(i)Tx(i))2.2. OutputTx."
        ],
        "formulas": [
            "ty=0+1x+2x2,then",
            "y=P5j=0jxj."
        ],
        "explanations": []
    },
    "page_20": {
        "content_preview": "19Here, thew(i)'s are non-negative valued weights . Intuitively, if w(i)is largefor a particular value of i, then in picking , we'll try hard to make ( y(i)Tx(i))2small. Ifw(i)is small, then the ( y(i)Tx(i))2error term will bepretty much ignored in the t.A fairly standard choice for the weights is4w(i)= exp(x(i)x)222Note that the weights depend on the particular point xat which we're tryingto evaluate x. Moreover, ifjx(i)xjis small, then w(i)is close to 1; andifjx(i)xjis large, then w(i)is small...",
        "python_code": [
            "19Here, thew(i)'s are non-negative valued weights . Intuitively, if w(i)is largefor a particular value of i, then in picking , we'll try hard to make ( y(i)Tx(i))2small. Ifw(i)is small, then the ( y(i)Tx(i))2error term will bepretty much ignored in the t.A fairly standard choice for the weights is4w(i)= exp(x(i)x)222Note that the weights depend on the particular point xat which we're tryingto evaluate x. Moreover, ifjx(i)xjis small, then w(i)is close to 1; andifjx(i)xjis large, then w(i)is small. Hence, is chosen giving a muchhigher \\weight\" to the (errors on) training examples close to the query pointx. (Note also that while the formula for the weights takes a form that iscosmetically similar to the density of a Gaussian distribution, the w(i)'s donot directly have anything to do with Gaussians, and in particular the w(i)are not random variables, normally distributed or otherwise.) The parametercontrols how quickly the weight of a training example falls o with distanceof itsx(i)from the query point x;is called the bandwidth parameter, andis also something that you'll get to experiment with in your homework.Locally weighted linear regression is the rst example we're seeing of anon-parametric algorithm. The (unweighted) linear regression algorithmthat we saw earlier is known as a parametric learning algorithm, becauseit has a xed, nite number of parameters (the i's), which are t to thedata. Once we've t the i's and stored them away, we no longer need tokeep the training data around to make future predictions. In contrast, tomake predictions using locally weighted linear regression, we need to keepthe entire training set around. The term \\non-parametric\" (roughly) refersto the fact that the amount of stu we need to keep in order to represent thehypothesis hgrows linearly with the size of the training set.4Ifxis vector-valued, this is generalized to be w(i)= exp((x(i)x)T(x(i)x)=(22)),orw(i)= exp((x(i)x)T1(x(i)x)=(22)), for an appropriate choice of or ."
        ],
        "formulas": [],
        "explanations": []
    },
    "page_21": {
        "content_preview": "Chapter 2Classication and logisticregressionLet's now talk about the classication problem. This is just like the regressionproblem, except that the values ywe now want to predict take on onlya small number of discrete values. For now, we will focus on the binaryclassication problem in which ycan take on only two values, 0 and 1.(Most of what we say here will also generalize to the multiple-class case.)For instance, if we are trying to build a spam classier for email, then x(i)may be some feature...",
        "python_code": [
            "Chapter 2Classication and logisticregressionLet's now talk about the classication problem. This is just like the regressionproblem, except that the values ywe now want to predict take on onlya small number of discrete values. For now, we will focus on the binaryclassication problem in which ycan take on only two values, 0 and 1.(Most of what we say here will also generalize to the multiple-class case.)For instance, if we are trying to build a spam classier for email, then x(i)may be some features of a piece of email, and ymay be 1 if it is a pieceof spam mail, and 0 otherwise. 0 is also called the negative class , and 1thepositive class , and they are sometimes also denoted by the symbols \\-\"and \\+.\" Given x(i), the corresponding y(i)is also called the label for thetraining example.2.1 Logistic regressionWe could approach the classication problem ignoring the fact that yisdiscrete-valued, and use our old linear regression algorithm to try to predictygivenx. However, it is easy to construct examples where this methodperforms very poorly. Intuitively, it also doesn't make sense for h(x) to takevalues larger than 1 or smaller than 0 when we know that y2f0;1g.To x this, let's change the form for our hypotheses h(x). We will chooseh(x) =g(Tx) =11 +eTx;whereg(z) =11 +ez20"
        ],
        "formulas": [],
        "explanations": []
    },
    "page_22": {
        "content_preview": "21is called the logistic function or the sigmoid function . Here is a plotshowingg(z):−5 −4 −3 −2 −1 0 1 2 3 4 500.10.20.30.40.50.60.70.80.91zg(z)Notice that g(z) tends towards 1 as z!1 , andg(z) tends towards 0 asz!1 . Moreover, g(z), and hence also h(x), is always bounded between0 and 1. As before, we are keeping the convention of letting x0= 1, so thatTx=0+Pdj=1jxj.For now, let's take the choice of gas given. Other functions that smoothlyincrease from 0 to 1 can also be used, but for a couple...",
        "python_code": [
            "21is called the logistic function or the sigmoid function . Here is a plotshowingg(z):−5 −4 −3 −2 −1 0 1 2 3 4 500.10.20.30.40.50.60.70.80.91zg(z)Notice that g(z) tends towards 1 as z!1 , andg(z) tends towards 0 asz!1 . Moreover, g(z), and hence also h(x), is always bounded between0 and 1. As before, we are keeping the convention of letting x0= 1, so thatTx=0+Pdj=1jxj.For now, let's take the choice of gas given. Other functions that smoothlyincrease from 0 to 1 can also be used, but for a couple of reasons that we'll seelater (when we talk about GLMs, and when we talk about generative learningalgorithms), the choice of the logistic function is a fairly natural one. Beforemoving on, here's a useful property of the derivative of the sigmoid function,which we write as g0:g0(z) =ddz11 +ez=1(1 +ez)2ez=1(1 +ez)11(1 +ez)=g(z)(1g(z)):So, given the logistic regression model, how do we t for it? Followinghow we saw least squares regression could be derived as the maximum like-lihood estimator under a set of assumptions, let's endow our classicationmodel with a set of probabilistic assumptions, and then t the parametersvia maximum likelihood."
        ],
        "formulas": [
            "x0= 1,",
            "thatTx=0+Pdj=1jxj.For",
            "ez=1(1",
            "2ez=1(1"
        ],
        "explanations": []
    },
    "page_23": {
        "content_preview": "22Let us assume thatP(y= 1jx;) =h(x)P(y= 0jx;) = 1h(x)Note that this can be written more compactly asp(yjx;) = (h(x))y(1h(x))1yAssuming that the ntraining examples were generated independently, wecan then write down the likelihood of the parameters asL() =p(~ yjX;)=nYi=1p(y(i)jx(i);)=nYi=1h(x(i))y(i)1h(x(i))1y(i)As before, it will be easier to maximize the log likelihood:`() = logL() =nXi=1y(i)logh(x(i)) + (1y(i)) log(1h(x(i))) (2.1)How do we maximize the likelihood? Similar to our derivation in...",
        "python_code": [
            "22Let us assume thatP(y= 1jx;) =h(x)P(y= 0jx;) = 1h(x)Note that this can be written more compactly asp(yjx;) = (h(x))y(1h(x))1yAssuming that the ntraining examples were generated independently, wecan then write down the likelihood of the parameters asL() =p(~ yjX;)=nYi=1p(y(i)jx(i);)=nYi=1h(x(i))y(i)1h(x(i))1y(i)As before, it will be easier to maximize the log likelihood:`() = logL() =nXi=1y(i)logh(x(i)) + (1y(i)) log(1h(x(i))) (2.1)How do we maximize the likelihood? Similar to our derivation in the caseof linear regression, we can use gradient ascent. Written in vectorial notation,our updates will therefore be given by :=+r`(). (Note the positiverather than negative sign in the update formula, since we're maximizing,rather than minimizing, a function now.) Let's start by working with justone training example ( x;y), and take derivatives to derive the stochasticgradient ascent rule:@@j`() =y1g(Tx)(1y)11g(Tx)@@jg(Tx)=y1g(Tx)(1y)11g(Tx)g(Tx)(1g(Tx))@@jTx=y(1g(Tx))(1y)g(Tx)xj= (yh(x))xj (2.2)"
        ],
        "formulas": [
            "y= 1jx;)",
            "y= 0jx;)",
            "nYi=1p(y(i)jx(i);)=nYi=1h(x(i))y(i)1h(x(i))1y(i)As",
            "nXi=1y(i)logh(x(i))",
            "jTx=y(1g(Tx))(1y)g(Tx)xj="
        ],
        "explanations": []
    },
    "page_24": {
        "content_preview": "23Above, we used the fact that g0(z) =g(z)(1g(z)). This therefore gives usthe stochastic gradient ascent rulej:=j+y(i)h(x(i))x(i)jIf we compare this to the LMS update rule, we see that it looks identical; butthis is notthe same algorithm, because h(x(i)) is now dened as a non-linearfunction of Tx(i). Nonetheless, it's a little surprising that we end up withthe same update rule for a rather dierent algorithm and learning problem.Is this coincidence, or is there a deeper reason behind this? We'll ...",
        "python_code": [
            "23Above, we used the fact that g0(z) =g(z)(1g(z)). This therefore gives usthe stochastic gradient ascent rulej:=j+y(i)h(x(i))x(i)jIf we compare this to the LMS update rule, we see that it looks identical; butthis is notthe same algorithm, because h(x(i)) is now dened as a non-linearfunction of Tx(i). Nonetheless, it's a little surprising that we end up withthe same update rule for a rather dierent algorithm and learning problem.Is this coincidence, or is there a deeper reason behind this? We'll answer thiswhen we get to GLM models.Remark 2.1.1: An alternative notational viewpoint of the same loss func-tion is also useful, especially for Section 7.1 where we study nonlinear models.Let`logistic :Rf0;1g!R0be the logistic loss dened as`logistic (t;y),ylog(1 + exp(t)) + (1y) log(1 + exp( t)): (2.3)One can verify by plugging in h(x) = 1=(1 +e>x) that the negative log-likelihood (the negation of `() in equation (2.1)) can be re-written as`() =`logistic (>x;y): (2.4)Oftentimes >xortis called the logit. Basic calculus gives us that@`logistic (t;y)@t=yexp(t)1 + exp(t)+ (1y)11 + exp(t)(2.5)= 1=(1 + exp(t))y: (2.6)Then, using the chain rule, we have that@@j`() =@`logistic (t;y)@t@t@j(2.7)= (y1=(1 + exp(t)))xj= (yh(x))xj; (2.8)which is consistent with the derivation in equation (2.2). We will see thisviewpoint can be extended nonlinear models in Section 7.1.2.2 Digression: the perceptron learning algo-rithmWe now digress to talk briey about an algorithm that's of some historicalinterest, and that we will also return to later when we talk about learning"
        ],
        "formulas": [
            "1=(1",
            "t=yexp(t)1",
            "1=(1",
            "y1=(1",
            "xj= (yh(x))xj;"
        ],
        "explanations": []
    },
    "page_25": {
        "content_preview": "24theory. Consider modifying the logistic regression method to \\force\" it tooutput values that are either 0 or 1 or exactly. To do so, it seems natural tochange the denition of gto be the threshold function:g(z) =1 ifz00 ifz <0If we then let h(x) =g(Tx) as before but using this modied denition ofg, and if we use the update rulej:=j+y(i)h(x(i))x(i)j:then we have the perceptron learning algorithn .In the 1960s, this \\perceptron\" was argued to be a rough model for howindividual neurons in the brain...",
        "python_code": [
            "24theory. Consider modifying the logistic regression method to \\force\" it tooutput values that are either 0 or 1 or exactly. To do so, it seems natural tochange the denition of gto be the threshold function:g(z) =1 ifz00 ifz <0If we then let h(x) =g(Tx) as before but using this modied denition ofg, and if we use the update rulej:=j+y(i)h(x(i))x(i)j:then we have the perceptron learning algorithn .In the 1960s, this \\perceptron\" was argued to be a rough model for howindividual neurons in the brain work. Given how simple the algorithm is, itwill also provide a starting point for our analysis when we talk about learningtheory later in this class. Note however that even though the perceptron maybe cosmetically similar to the other algorithms we talked about, it is actuallya very dierent type of algorithm than logistic regression and least squareslinear regression; in particular, it is dicult to endow the perceptron's predic-tions with meaningful probabilistic interpretations, or derive the perceptronas a maximum likelihood estimation algorithm.2.3 Multi-class classicationConsider a classication problem in which the response variable ycan take onany one ofkvalues, soy2f1;2;:::;kg. For example, rather than classifyingemails into the two classes spam or not-spam|which would have been abinary classication problem|we might want to classify them into threeclasses, such as spam, personal mails, and work-related mails. The label /response variable is still discrete, but can now take on more than two values.We will thus model it as distributed according to a multinomial distribution.In this case, p(yjx;) is a distribution over kpossible discrete outcomesand is thus a multinomial distribution. Recall that a multinomial distribu-tion involves knumbers1;:::;kspecifying the probability of each of theoutcomes. Note that these numbers must satisfyPki=1i= 1. We will de-sign a parameterized model that outputs 1;:::;ksatisfying this constraintgiven the input x.We introduce kgroups of parameters 1;:::;k, each of them being avector in Rd. Intuitively, we would like to use >1x;:::;>kxto represent"
        ],
        "formulas": [
            "satisfyPki=1i="
        ],
        "explanations": []
    },
    "page_26": {
        "content_preview": "251;:::;k, the probabilities P(y= 1jx;);:::;P (y=kjx;). However,there are two issues with such a direct approach. First, >jxis not neces-sarily within [0 ;1]. Second, the summation of >jx's is not necessarily 1.Thus, instead, we will use the softmax function to turn ( >1x;;>kx) intoa probability vector with nonnegative entries that sum up to 1.Dene the softmax function softmax : Rk!Rkassoftmax(t1;:::;tk) =2664exp(t1)Pkj=1exp(tj)...exp(tk)Pkj=1exp(tj)3775: (2.9)The inputs to the softmax function,...",
        "python_code": [
            "251;:::;k, the probabilities P(y= 1jx;);:::;P (y=kjx;). However,there are two issues with such a direct approach. First, >jxis not neces-sarily within [0 ;1]. Second, the summation of >jx's is not necessarily 1.Thus, instead, we will use the softmax function to turn ( >1x;;>kx) intoa probability vector with nonnegative entries that sum up to 1.Dene the softmax function softmax : Rk!Rkassoftmax(t1;:::;tk) =2664exp(t1)Pkj=1exp(tj)...exp(tk)Pkj=1exp(tj)3775: (2.9)The inputs to the softmax function, the vector there, are often called log-its. Note that by denition, the output of the softmax function is always aprobability vector whose entries are nonnegative and sum up to 1.Let (t1;:::;tk) = (>1x;;>kx). We apply the softmax function to(t1;:::;tk), and use the output as the probabilities P(y= 1jx;);:::;P (y=kjx;). We obtain the following probabilistic model:264P(y= 1jx;)...P(y=kjx;)375= softmax( t1;;tk) =26664exp(>1x)Pkj=1exp(>jx)...exp(>kx)Pkj=1exp(>jx)37775: (2.10)For notational convenience, we will let i=exp(ti)Pkj=1exp(tj). More succinctly, theequation above can be written as:P(y=ijx;) =i=exp(ti)Pkj=1exp(tj)=exp(>ix)Pkj=1exp(>jx): (2.11)Next, we compute the negative log-likelihood of a single example ( x;y).logp(yjx;) =log exp(ty)Pkj=1exp(tj)!=log exp(>yx)Pkj=1exp(>jx)!(2.12)Thus, the loss function, the negative log-likelihood of the training data, isgiven as`() =nXi=1log exp(>y(i)x(i))Pkj=1exp(>jx(i))!: (2.13)"
        ],
        "formulas": [
            "y= 1jx;);:::;P",
            "y=kjx;).",
            "Pkj=1exp(tj)...exp(tk)Pkj=1exp(tj)3775:",
            "y= 1jx;);:::;P",
            "y=kjx;).",
            "y= 1jx;)...P(y=kjx;)375=",
            "Pkj=1exp(>jx)...exp(>kx)Pkj=1exp(>jx)37775:",
            "i=exp(ti)Pkj=1exp(tj).",
            "y=ijx;)",
            "i=exp(ti)Pkj=1exp(tj)=exp(>ix)Pkj=1exp(>jx):",
            "Pkj=1exp(tj)!=log",
            "Pkj=1exp(>jx)!(2.12)Thus,",
            "nXi=1log",
            "Pkj=1exp(>jx(i))!:"
        ],
        "explanations": []
    },
    "page_27": {
        "content_preview": "26It's convenient to dene the cross-entropy loss `ce:Rkf1;:::;kg!R0,which modularizes in the complex equation above:1`ce((t1;:::;tk);y) =log exp(ty)Pkj=1exp(tj)!: (2.14)With this notation, we can simply rewrite equation (2.13) as`() =nXi=1`ce((>1x(i);:::;>kx(i));y(i)): (2.15)Moreover, conveniently, the cross-entropy loss also has a simple gradient. Lett= (t1;:::;tk), and recall i=exp(ti)Pkj=1exp(tj). By basic calculus, we can derive@`ce(t;y)@ti=i1fy=ig; (2.16)where 1fgis the indicator function, ...",
        "python_code": [
            "26It's convenient to dene the cross-entropy loss `ce:Rkf1;:::;kg!R0,which modularizes in the complex equation above:1`ce((t1;:::;tk);y) =log exp(ty)Pkj=1exp(tj)!: (2.14)With this notation, we can simply rewrite equation (2.13) as`() =nXi=1`ce((>1x(i);:::;>kx(i));y(i)): (2.15)Moreover, conveniently, the cross-entropy loss also has a simple gradient. Lett= (t1;:::;tk), and recall i=exp(ti)Pkj=1exp(tj). By basic calculus, we can derive@`ce(t;y)@ti=i1fy=ig; (2.16)where 1fgis the indicator function, that is, 1 fy=ig= 1 ify=i, and1fy=ig= 0 ify6=i. Alternatively, in vectorized notations, we have thefollowing form which will be useful for Chapter 7:@`ce(t;y)@t=ey; (2.17)wherees2Rkis thes-th natural basis vector (where the s-th entry is 1 andall other entries are zeros.) Using Chain rule, we have that@`ce((>1x;:::;>kx);y)@i=@`(t;y)@ti@ti@i= (i1fy=ig)x: (2.18)Therefore, the gradient of the loss with respect to the part of parameter iis@`()@i=nXj=1((j)i1fy(j)=ig)x(j); (2.19)where(j)i=exp(>ix(j))Pks=1exp(>sx(j))is the probability that the model predicts item ifor example x(j). With the gradients above, one can implement (stochastic)gradient descent to minimize the loss function `().1There are some ambiguity in the naming here. Some people call the cross-entropy lossthe function that maps the probability vector (the in our language) and label yto thenal real number, and call our version of cross-entropy loss softmax-cross-entropy loss.We choose our current naming convention because it's consistent with the naming of mostmodern deep learning library such as PyTorch and Jax."
        ],
        "formulas": [
            "Pkj=1exp(tj)!:",
            "nXi=1`ce((>1x(i);:::;>kx(i));y(i)):",
            "Lett= (t1;:::;tk),",
            "i=exp(ti)Pkj=1exp(tj).",
            "ti=i1fy=ig;",
            "fy=ig=",
            "ify=i,",
            "and1fy=ig=",
            "ify6=i.",
            "t=ey;",
            "i=@`(t;y)@ti@ti@i=",
            "i1fy=ig)x:",
            "i=nXj=1((j)i1fy(j)=ig)x(j);",
            "i=exp(>ix(j))Pks=1exp(>sx(j))is"
        ],
        "explanations": []
    },
    "page_28": {
        "content_preview": "271 1.5 2 2.5 3 3.5 4 4.5 5−100102030405060xf(x)1 1.5 2 2.5 3 3.5 4 4.5 5−100102030405060xf(x)1 1.5 2 2.5 3 3.5 4 4.5 5−100102030405060xf(x)2.4 Another algorithm for maximizing `()Returning to logistic regression with g(z) being the sigmoid function, let'snow talk about a dierent algorithm for maximizing `().To get us started, let's consider Newton's method for nding a zero of afunction. Specically, suppose we have some function f:R7!R, and wewish to nd a value of so thatf() = 0. Here, 2Ris a re...",
        "python_code": [
            "271 1.5 2 2.5 3 3.5 4 4.5 5−100102030405060xf(x)1 1.5 2 2.5 3 3.5 4 4.5 5−100102030405060xf(x)1 1.5 2 2.5 3 3.5 4 4.5 5−100102030405060xf(x)2.4 Another algorithm for maximizing `()Returning to logistic regression with g(z) being the sigmoid function, let'snow talk about a dierent algorithm for maximizing `().To get us started, let's consider Newton's method for nding a zero of afunction. Specically, suppose we have some function f:R7!R, and wewish to nd a value of so thatf() = 0. Here, 2Ris a real number.Newton's method performs the following update::=f()f0():This method has a natural interpretation in which we can think of it asapproximating the function fvia a linear function that is tangent to fatthe current guess , solving for where that linear function equals to zero, andletting the next guess for be where that linear function is zero.Here's a picture of the Newton's method in action:In the leftmost gure, we see the function fplotted along with the liney= 0. We're trying to nd so thatf() = 0; the value of that achieves thisis about 1.3. Suppose we initialized the algorithm with = 4:5. Newton'smethod then ts a straight line tangent to fat= 4:5, and solves for thewhere that line evaluates to 0. (Middle gure.) This give us the next guessfor, which is about 2.8. The rightmost gure shows the result of runningone more iteration, which the updates to about 1.8. After a few moreiterations, we rapidly approach = 1:3.Newton's method gives a way of getting to f() = 0. What if we want touse it to maximize some function `? The maxima of `correspond to pointswhere its rst derivative `0() is zero. So, by letting f() =`0(), we can usethe same algorithm to maximize `, and we obtain update rule::=`0()`00():(Something to think about: How would this change if we wanted to useNewton's method to minimize rather than maximize a function?)"
        ],
        "formulas": [
            "liney= 0.",
            "with = 4:5.",
            "fat= 4:5,",
            "approach = 1:3.Newton's"
        ],
        "explanations": []
    },
    "page_29": {
        "content_preview": "28Lastly, in our logistic regression setting, is vector-valued, so we need togeneralize Newton's method to this setting. The generalization of Newton'smethod to this multidimensional setting (also called the Newton-Raphsonmethod) is given by:=H1r`():Here,r`() is, as usual, the vector of partial derivatives of `() with respectto thei's; andHis and-by-dmatrix (actually, d+1byd+1, assuming thatwe include the intercept term) called the Hessian , whose entries are givenbyHij=@2`()@i@j:Newton's method...",
        "python_code": [
            "28Lastly, in our logistic regression setting, is vector-valued, so we need togeneralize Newton's method to this setting. The generalization of Newton'smethod to this multidimensional setting (also called the Newton-Raphsonmethod) is given by:=H1r`():Here,r`() is, as usual, the vector of partial derivatives of `() with respectto thei's; andHis and-by-dmatrix (actually, d+1byd+1, assuming thatwe include the intercept term) called the Hessian , whose entries are givenbyHij=@2`()@i@j:Newton's method typically enjoys faster convergence than (batch) gra-dient descent, and requires many fewer iterations to get very close to theminimum. One iteration of Newton's can, however, be more expensive thanone iteration of gradient descent, since it requires nding and inverting and-by-dHessian; but so long as dis not too large, it is usually much fasteroverall. When Newton's method is applied to maximize the logistic regres-sion log likelihood function `(), the resulting method is also called Fisherscoring ."
        ],
        "formulas": [
            "givenbyHij=@2`()@i@j:Newton's"
        ],
        "explanations": []
    },
    "page_30": {
        "content_preview": "Chapter 3Generalized linear modelsSo far, we've seen a regression example, and a classication example. In theregression example, we had yjx;N (;2), and in the classication one,yjx;Bernoulli(), for some appropriate denitions of andas functionsofxand. In this section, we will show that both of these methods arespecial cases of a broader family of models, called Generalized Linear Models(GLMs).1We will also show how other models in the GLM family can bederived and applied to other classication and ...",
        "python_code": [
            "Chapter 3Generalized linear modelsSo far, we've seen a regression example, and a classication example. In theregression example, we had yjx;N (;2), and in the classication one,yjx;Bernoulli(), for some appropriate denitions of andas functionsofxand. In this section, we will show that both of these methods arespecial cases of a broader family of models, called Generalized Linear Models(GLMs).1We will also show how other models in the GLM family can bederived and applied to other classication and regression problems.3.1 The exponential familyTo work our way up to GLMs, we will begin by dening exponential familydistributions. We say that a class of distributions is in the exponential familyif it can be written in the formp(y;) =b(y) exp(TT(y)a()) (3.1)Here,is called the natural parameter (also called the canonical param-eter) of the distribution; T(y) is the sucient statistic (for the distribu-tions we consider, it will often be the case that T(y) =y); anda() is the logpartition function . The quantity ea()essentially plays the role of a nor-malization constant, that makes sure the distribution p(y;) sums/integratesoveryto 1.A xed choice of T,aandbdenes a family (or set) of distributions thatis parameterized by ; as we vary , we then get dierent distributions withinthis family.1The presentation of the material in this section takes inspiration from Michael I.Jordan, Learning in graphical models (unpublished book draft), and also McCullagh andNelder, Generalized Linear Models (2nd ed.) .29"
        ],
        "formulas": [],
        "explanations": []
    },
    "page_31": {
        "content_preview": "30We now show that the Bernoulli and the Gaussian distributions are ex-amples of exponential family distributions. The Bernoulli distribution withmean, written Bernoulli( ), species a distribution over y2f0;1g, so thatp(y= 1;) =;p(y= 0;) = 1. As we vary , we obtain Bernoullidistributions with dierent means. We now show that this class of Bernoullidistributions, ones obtained by varying , is in the exponential family; i.e.,that there is a choice of T,aandbso that Equation (3.1) becomes exactlythe...",
        "python_code": [
            "30We now show that the Bernoulli and the Gaussian distributions are ex-amples of exponential family distributions. The Bernoulli distribution withmean, written Bernoulli( ), species a distribution over y2f0;1g, so thatp(y= 1;) =;p(y= 0;) = 1. As we vary , we obtain Bernoullidistributions with dierent means. We now show that this class of Bernoullidistributions, ones obtained by varying , is in the exponential family; i.e.,that there is a choice of T,aandbso that Equation (3.1) becomes exactlythe class of Bernoulli distributions.We write the Bernoulli distribution as:p(y;) =y(1)1y= exp(ylog+ (1y) log(1))= explog1y+ log(1):Thus, the natural parameter is given by = log(=(1)). Interestingly, ifwe invert this denition for by solving for in terms of , we obtain =1=(1 +e). This is the familiar sigmoid function! This will come up againwhen we derive logistic regression as a GLM. To complete the formulationof the Bernoulli distribution as an exponential family distribution, we alsohaveT(y) =ya() =log(1)= log(1 + e)b(y) = 1This shows that the Bernoulli distribution can be written in the form ofEquation (3.1), using an appropriate choice of T,aandb.Let's now move on to consider the Gaussian distribution. Recall that,when deriving linear regression, the value of 2had no eect on our nalchoice ofandh(x). Thus, we can choose an arbitrary value for 2withoutchanging anything. To simplify the derivation below, let's set 2= 1.2We2If we leave 2as a variable, the Gaussian distribution can also be shown to be in theexponential family, where 2R2is now a 2-dimension vector that depends on both and. For the purposes of GLMs, however, the 2parameter can also be treated by consideringa more general denition of the exponential family: p(y;;) =b(a;) exp((TT(y)a())=c()). Here,is called the dispersion parameter , and for the Gaussian, c() =2;but given our simplication above, we won't need the more general denition for theexamples we will consider here."
        ],
        "formulas": [
            "y= 1;)",
            "y= 0;)",
            "1y= exp(ylog+",
            "by = log(=(1)).",
            "obtain =1=(1",
            "2= 1.2We2If"
        ],
        "explanations": []
    },
    "page_32": {
        "content_preview": "31then have:p(y;) =1p2exp12(y)2=1p2exp12y2expy122Thus, we see that the Gaussian is in the exponential family, with=T(y) =ya() =2=2=2=2b(y) = (1=p2) exp(y2=2):There're many other distributions that are members of the exponen-tial family: The multinomial (which we'll see later), the Poisson (for mod-elling count-data; also see the problem set); the gamma and the exponen-tial (for modelling continuous, non-negative random variables, such as time-intervals); the beta and the Dirichlet (for distribut...",
        "python_code": [
            "31then have:p(y;) =1p2exp12(y)2=1p2exp12y2expy122Thus, we see that the Gaussian is in the exponential family, with=T(y) =ya() =2=2=2=2b(y) = (1=p2) exp(y2=2):There're many other distributions that are members of the exponen-tial family: The multinomial (which we'll see later), the Poisson (for mod-elling count-data; also see the problem set); the gamma and the exponen-tial (for modelling continuous, non-negative random variables, such as time-intervals); the beta and the Dirichlet (for distributions over probabilities);and many more. In the next section, we will describe a general \\recipe\"for constructing models in which y(givenxand) comes from any of thesedistributions.3.2 Constructing GLMsSuppose you would like to build a model to estimate the number yof cus-tomers arriving in your store (or number of page-views on your website) inany given hour, based on certain features xsuch as store promotions, recentadvertising, weather, day-of-week, etc. We know that the Poisson distribu-tion usually gives a good model for numbers of visitors. Knowing this, howcan we come up with a model for our problem? Fortunately, the Poisson is anexponential family distribution, so we can apply a Generalized Linear Model(GLM). In this section, we will we will describe a method for constructingGLM models for problems such as these.More generally, consider a classication or regression problem where wewould like to predict the value of some random variable yas a function ofx. To derive a GLM for this problem, we will make the following threeassumptions about the conditional distribution of ygivenxand about ourmodel:"
        ],
        "formulas": [
            "2=1p2exp12y2expy122Thus,",
            "with=T(y)",
            "2=2=2=2b(y)",
            "1=p2)",
            "y2=2):There're"
        ],
        "explanations": []
    },
    "page_33": {
        "content_preview": "321.yjx;ExponentialFamily( ). I.e., given xand, the distribution ofyfollows some exponential family distribution, with parameter .2. Givenx, our goal is to predict the expected value of T(y) givenx.In most of our examples, we will have T(y) =y, so this means wewould like the prediction h(x) output by our learned hypothesis htosatisfyh(x) = E[yjx]. (Note that this assumption is satised in thechoices for h(x) for both logistic regression and linear regression. Forinstance, in logistic regression, ...",
        "python_code": [
            "321.yjx;ExponentialFamily( ). I.e., given xand, the distribution ofyfollows some exponential family distribution, with parameter .2. Givenx, our goal is to predict the expected value of T(y) givenx.In most of our examples, we will have T(y) =y, so this means wewould like the prediction h(x) output by our learned hypothesis htosatisfyh(x) = E[yjx]. (Note that this assumption is satised in thechoices for h(x) for both logistic regression and linear regression. Forinstance, in logistic regression, we had h(x) =p(y= 1jx;) = 0p(y=0jx;) + 1p(y= 1jx;) = E[yjx;].)3. The natural parameter and the inputs xare related linearly: =Tx.(Or, ifis vector-valued, then i=Tix.)The third of these assumptions might seem the least well justied ofthe above, and it might be better thought of as a \\design choice\" in ourrecipe for designing GLMs, rather than as an assumption per se. Thesethree assumptions/design choices will allow us to derive a very elegant classof learning algorithms, namely GLMs, that have many desirable propertiessuch as ease of learning. Furthermore, the resulting models are often veryeective for modelling dierent types of distributions over y; for example, wewill shortly show that both logistic regression and ordinary least squares canboth be derived as GLMs.3.2.1 Ordinary least squaresTo show that ordinary least squares is a special case of the GLM familyof models, consider the setting where the target variable y(also called theresponse variable in GLM terminology) is continuous, and we model theconditional distribution of ygivenxas a GaussianN(;2). (Here,maydependx.) So, we let the ExponentialFamily () distribution above bethe Gaussian distribution. As we saw previously, in the formulation of theGaussian as an exponential family distribution, we had =. So, we haveh(x) =E[yjx;]===Tx:The rst equality follows from Assumption 2, above; the second equalityfollows from the fact that yjx;N(;2), and so its expected value is given"
        ],
        "formulas": [
            "y= 1jx;)",
            "y=0jx;)",
            "y= 1jx;)",
            "i=Tix.)The",
            "had =."
        ],
        "explanations": []
    },
    "page_34": {
        "content_preview": "33by; the third equality follows from Assumption 1 (and our earlier derivationshowing that =in the formulation of the Gaussian as an exponentialfamily distribution); and the last equality follows from Assumption 3.3.2.2 Logistic regressionWe now consider logistic regression. Here we are interested in binary classi-cation, soy2f0;1g. Given that yis binary-valued, it therefore seems naturalto choose the Bernoulli family of distributions to model the conditional dis-tribution of ygivenx. In our for...",
        "python_code": [
            "33by; the third equality follows from Assumption 1 (and our earlier derivationshowing that =in the formulation of the Gaussian as an exponentialfamily distribution); and the last equality follows from Assumption 3.3.2.2 Logistic regressionWe now consider logistic regression. Here we are interested in binary classi-cation, soy2f0;1g. Given that yis binary-valued, it therefore seems naturalto choose the Bernoulli family of distributions to model the conditional dis-tribution of ygivenx. In our formulation of the Bernoulli distribution asan exponential family distribution, we had = 1=(1 +e). Furthermore,note that if yjx;Bernoulli(), then E[yjx;] =. So, following a similarderivation as the one for ordinary least squares, we get:h(x) =E[yjx;]== 1=(1 +e)= 1=(1 +eTx)So, this gives us hypothesis functions of the form h(x) = 1=(1 +eTx). Ifyou are previously wondering how we came up with the form of the logisticfunction 1=(1 +ez), this gives one answer: Once we assume that ycondi-tioned onxis Bernoulli, it arises as a consequence of the denition of GLMsand exponential family distributions.To introduce a little more terminology, the function ggiving the distri-bution's mean as a function of the natural parameter ( g() = E[T(y);])is called the canonical response function . Its inverse, g1, is called thecanonical link function . Thus, the canonical response function for theGaussian family is just the identify function; and the canonical responsefunction for the Bernoulli is the logistic function.33Many texts use gto denote the link function, and g1to denote the response function;but the notation we're using here, inherited from the early machine learning literature,will be more consistent with the notation used in the rest of the class."
        ],
        "formulas": [
            "that =in",
            "had = 1=(1",
            "1=(1",
            "1=(1",
            "1=(1",
            "1=(1"
        ],
        "explanations": []
    },
    "page_35": {
        "content_preview": "Chapter 4Generative learning algorithmsSo far, we've mainly been talking about learning algorithms that modelp(yjx;), the conditional distribution of ygivenx. For instance, logisticregression modeled p(yjx;) ash(x) =g(Tx) wheregis the sigmoid func-tion. In these notes, we'll talk about a dierent type of learning algorithm.Consider a classication problem in which we want to learn to distinguishbetween elephants ( y= 1) and dogs ( y= 0), based on some features ofan animal. Given a training set, an...",
        "python_code": [
            "Chapter 4Generative learning algorithmsSo far, we've mainly been talking about learning algorithms that modelp(yjx;), the conditional distribution of ygivenx. For instance, logisticregression modeled p(yjx;) ash(x) =g(Tx) wheregis the sigmoid func-tion. In these notes, we'll talk about a dierent type of learning algorithm.Consider a classication problem in which we want to learn to distinguishbetween elephants ( y= 1) and dogs ( y= 0), based on some features ofan animal. Given a training set, an algorithm like logistic regression orthe perceptron algorithm (basically) tries to nd a straight line|that is, adecision boundary|that separates the elephants and dogs. Then, to classifya new animal as either an elephant or a dog, it checks on which side of thedecision boundary it falls, and makes its prediction accordingly.Here's a dierent approach. First, looking at elephants, we can build amodel of what elephants look like. Then, looking at dogs, we can build aseparate model of what dogs look like. Finally, to classify a new animal, wecan match the new animal against the elephant model, and match it againstthe dog model, to see whether the new animal looks more like the elephantsor more like the dogs we had seen in the training set.Algorithms that try to learn p(yjx) directly (such as logistic regression),or algorithms that try to learn mappings directly from the space of inputs Xto the labelsf0;1g, (such as the perceptron algorithm) are called discrim-inative learning algorithms. Here, we'll talk about algorithms that insteadtry to model p(xjy) (andp(y)). These algorithms are called generativelearning algorithms. For instance, if yindicates whether an example is adog (0) or an elephant (1), then p(xjy= 0) models the distribution of dogs'features, and p(xjy= 1) models the distribution of elephants' features.After modeling p(y) (called the class priors ) andp(xjy), our algorithm34"
        ],
        "formulas": [
            "y= 1)",
            "y= 0),",
            "xjy= 0)",
            "xjy= 1)"
        ],
        "explanations": []
    },
    "page_36": {
        "content_preview": "35can then use Bayes rule to derive the posterior distribution on ygivenx:p(yjx) =p(xjy)p(y)p(x):Here, the denominator is given by p(x) =p(xjy= 1)p(y= 1) +p(xjy=0)p(y= 0) (you should be able to verify that this is true from the standardproperties of probabilities), and thus can also be expressed in terms of thequantitiesp(xjy) andp(y) that we've learned. Actually, if were calculatingp(yjx) in order to make a prediction, then we don't actually need to calculatethe denominator, sincearg maxyp(yjx)...",
        "python_code": [
            "35can then use Bayes rule to derive the posterior distribution on ygivenx:p(yjx) =p(xjy)p(y)p(x):Here, the denominator is given by p(x) =p(xjy= 1)p(y= 1) +p(xjy=0)p(y= 0) (you should be able to verify that this is true from the standardproperties of probabilities), and thus can also be expressed in terms of thequantitiesp(xjy) andp(y) that we've learned. Actually, if were calculatingp(yjx) in order to make a prediction, then we don't actually need to calculatethe denominator, sincearg maxyp(yjx) = arg maxyp(xjy)p(y)p(x)= arg maxyp(xjy)p(y):4.1 Gaussian discriminant analysisThe rst generative learning algorithm that we'll look at is Gaussian discrim-inant analysis (GDA). In this model, we'll assume that p(xjy) is distributedaccording to a multivariate normal distribution. Let's talk briey about theproperties of multivariate normal distributions before moving on to the GDAmodel itself.4.1.1 The multivariate normal distributionThe multivariate normal distribution in d-dimensions, also called the multi-variate Gaussian distribution, is parameterized by a mean vector 2Rdand a covariance matrix 2Rdd, where 0 is symmetric and positivesemi-denite. Also written \\ N(;)\", its density is given by:p(x;;) =1(2)d=2jj1=2exp12(x)T1(x):In the equation above, \\ jj\" denotes the determinant of the matrix .For a random variable XdistributedN(;), the mean is (unsurpris-ingly) given by :E[X] =Zxxp(x;;)dx=Thecovariance of a vector-valued random variable Zis dened as Cov( Z) =E[(ZE[Z])(ZE[Z])T]. This generalizes the notion of the variance of a"
        ],
        "formulas": [
            "xjy= 1)p(y=",
            "xjy=0)p(y=",
            "d=2jj1=2exp12(x)T1(x):In",
            "dx=Thecovariance"
        ],
        "explanations": []
    },
    "page_37": {
        "content_preview": "36real-valued random variable. The covariance can also be dened as Cov( Z) =E[ZZT](E[Z])(E[Z])T. (You should be able to prove to yourself that thesetwo denitions are equivalent.) If XN(;), thenCov(X) = :Here are some examples of what the density of a Gaussian distributionlooks like:−3−2−10123−3−2−101230.050.10.150.20.25−3−2−10123−3−2−101230.050.10.150.20.25−3−2−10123−3−2−101230.050.10.150.20.25The left-most gure shows a Gaussian with mean zero (that is, the 2x1zero-vector) and covariance matrix ...",
        "python_code": [
            "36real-valued random variable. The covariance can also be dened as Cov( Z) =E[ZZT](E[Z])(E[Z])T. (You should be able to prove to yourself that thesetwo denitions are equivalent.) If XN(;), thenCov(X) = :Here are some examples of what the density of a Gaussian distributionlooks like:−3−2−10123−3−2−101230.050.10.150.20.25−3−2−10123−3−2−101230.050.10.150.20.25−3−2−10123−3−2−101230.050.10.150.20.25The left-most gure shows a Gaussian with mean zero (that is, the 2x1zero-vector) and covariance matrix = I(the 2x2 identity matrix). A Gaus-sian with zero mean and identity covariance is also called the standard nor-mal distribution . The middle gure shows the density of a Gaussian withzero mean and = 0 :6I; and in the rightmost gure shows one with , = 2 I.We see that as becomes larger, the Gaussian becomes more \\spread-out,\"and as it becomes smaller, the distribution becomes more \\compressed.\"Let's look at some more examples.−3−2−10123−3−2−101230.050.10.150.20.25−3−2−10123−3−2−101230.050.10.150.20.25−3−2−10123−3−2−101230.050.10.150.20.25The gures above show Gaussians with mean 0, and with covariancematrices respectively =1 00 1; =1 0.50.5 1; =1 0.80.8 1:The leftmost gure shows the familiar standard normal distribution, and wesee that as we increase the o-diagonal entry in , the density becomes more"
        ],
        "formulas": [
            "matrix = I(the",
            "and = 0",
            "respectively =1"
        ],
        "explanations": []
    },
    "page_38": {
        "content_preview": "37\\compressed\" towards the 45line (given by x1=x2). We can see this moreclearly when we look at the contours of the same three densities:−3 −2 −1 0 1 2 3−3−2−10123−3 −2 −1 0 1 2 3−3−2−10123−3 −2 −1 0 1 2 3−3−2−10123Here's one last set of examples generated by varying :−3 −2 −1 0 1 2 3−3−2−10123−3 −2 −1 0 1 2 3−3−2−10123−3 −2 −1 0 1 2 3−3−2−10123The plots above used, respectively, =1 -0.5-0.5 1; =1 -0.8-0.8 1; =3 0.80.8 1:From the leftmost and middle gures, we see that by decreasing the o-diagona...",
        "python_code": [
            "37\\compressed\" towards the 45line (given by x1=x2). We can see this moreclearly when we look at the contours of the same three densities:−3 −2 −1 0 1 2 3−3−2−10123−3 −2 −1 0 1 2 3−3−2−10123−3 −2 −1 0 1 2 3−3−2−10123Here's one last set of examples generated by varying :−3 −2 −1 0 1 2 3−3−2−10123−3 −2 −1 0 1 2 3−3−2−10123−3 −2 −1 0 1 2 3−3−2−10123The plots above used, respectively, =1 -0.5-0.5 1; =1 -0.8-0.8 1; =3 0.80.8 1:From the leftmost and middle gures, we see that by decreasing the o-diagonal elements of the covariance matrix, the density now becomes \\com-pressed\" again, but in the opposite direction. Lastly, as we vary the pa-rameters, more generally the contours will form ellipses (the rightmost gureshowing an example).As our last set of examples, xing = I, by varying , we can also movethe mean of the density around.−3−2−10123−3−2−101230.050.10.150.20.25−3−2−10123−3−2−101230.050.10.150.20.25−3−2−10123−3−2−101230.050.10.150.20.25"
        ],
        "formulas": [
            "x1=x2).",
            "xing = I,"
        ],
        "explanations": []
    },
    "page_39": {
        "content_preview": "38The gures above were generated using = I, and respectively=10;=-0.50;=-1-1.5:4.1.2 The Gaussian discriminant analysis modelWhen we have a classication problem in which the input features xarecontinuous-valued random variables, we can then use the Gaussian Discrim-inant Analysis (GDA) model, which models p(xjy) using a multivariate nor-mal distribution. The model is:yBernoulli()xjy= 0 N (0;)xjy= 1 N (1;)Writing out the distributions, this is:p(y) =y(1)1yp(xjy= 0) =1(2)d=2jj1=2exp12(x0)T1(x0)p(x...",
        "python_code": [
            "38The gures above were generated using = I, and respectively=10;=-0.50;=-1-1.5:4.1.2 The Gaussian discriminant analysis modelWhen we have a classication problem in which the input features xarecontinuous-valued random variables, we can then use the Gaussian Discrim-inant Analysis (GDA) model, which models p(xjy) using a multivariate nor-mal distribution. The model is:yBernoulli()xjy= 0 N (0;)xjy= 1 N (1;)Writing out the distributions, this is:p(y) =y(1)1yp(xjy= 0) =1(2)d=2jj1=2exp12(x0)T1(x0)p(xjy= 1) =1(2)d=2jj1=2exp12(x1)T1(x1)Here, the parameters of our model are , ,0and1. (Note that whilethere're two dierent mean vectors 0and1, this model is usually appliedusing only one covariance matrix .) The log-likelihood of the data is givenby`(; 0;1;) = lognYi=1p(x(i);y(i);; 0;1;)= lognYi=1p(x(i)jy(i);0;1;)p(y(i);):"
        ],
        "formulas": [
            "using = I,",
            "respectively=10;=-0.50;=-1-1.5:4.1.2",
            "xjy= 0",
            "xjy= 1",
            "xjy= 0)",
            "d=2jj1=2exp12(x0)T1(x0)p(xjy=",
            "d=2jj1=2exp12(x1)T1(x1)Here,",
            "lognYi=1p(x(i);y(i);;",
            "lognYi=1p(x(i)jy(i);0;1;)p(y(i);):"
        ],
        "explanations": []
    },
    "page_40": {
        "content_preview": "39By maximizing `with respect to the parameters, we nd the maximum like-lihood estimate of the parameters (see problem set 1) to be:=1nnXi=11fy(i)= 1g0=Pni=11fy(i)= 0gx(i)Pni=11fy(i)= 0g1=Pni=11fy(i)= 1gx(i)Pni=11fy(i)= 1g =1nnXi=1(x(i)y(i))(x(i)y(i))T:Pictorially, what the algorithm is doing can be seen in as follows:−2 −1 0 1 2 3 4 5 6 7−7−6−5−4−3−2−101Shown in the gure are the training set, as well as the contours of thetwo Gaussian distributions that have been t to the data in each of thetwo...",
        "python_code": [
            "39By maximizing `with respect to the parameters, we nd the maximum like-lihood estimate of the parameters (see problem set 1) to be:=1nnXi=11fy(i)= 1g0=Pni=11fy(i)= 0gx(i)Pni=11fy(i)= 0g1=Pni=11fy(i)= 1gx(i)Pni=11fy(i)= 1g =1nnXi=1(x(i)y(i))(x(i)y(i))T:Pictorially, what the algorithm is doing can be seen in as follows:−2 −1 0 1 2 3 4 5 6 7−7−6−5−4−3−2−101Shown in the gure are the training set, as well as the contours of thetwo Gaussian distributions that have been t to the data in each of thetwo classes. Note that the two Gaussians have contours that are the sameshape and orientation, since they share a covariance matrix , but they havedierent means 0and1. Also shown in the gure is the straight linegiving the decision boundary at which p(y= 1jx) = 0:5. On one side ofthe boundary, we'll predict y= 1 to be the most likely outcome, and on theother side, we'll predict y= 0."
        ],
        "formulas": [
            "1nnXi=11fy(i)=",
            "1g0=Pni=11fy(i)=",
            "Pni=11fy(i)=",
            "0g1=Pni=11fy(i)=",
            "Pni=11fy(i)=",
            "1g =1nnXi=1(x(i)y(i))(x(i)y(i))T:Pictorially,",
            "y= 1jx)",
            "y= 1",
            "y= 0."
        ],
        "explanations": []
    },
    "page_41": {
        "content_preview": "404.1.3 Discussion: GDA and logistic regressionThe GDA model has an interesting relationship to logistic regression. If weview the quantity p(y= 1jx;; 0;1;) as a function of x, we'll nd that itcan be expressed in the formp(y= 1jx;;;0;1) =11 + exp(Tx);whereis some appropriate function of ;;0;1.1This is exactly the formthat logistic regression|a discriminative algorithm|used to model p(y=1jx).When would we prefer one model over another? GDA and logistic regres-sion will, in general, give dierent d...",
        "python_code": [
            "404.1.3 Discussion: GDA and logistic regressionThe GDA model has an interesting relationship to logistic regression. If weview the quantity p(y= 1jx;; 0;1;) as a function of x, we'll nd that itcan be expressed in the formp(y= 1jx;;;0;1) =11 + exp(Tx);whereis some appropriate function of ;;0;1.1This is exactly the formthat logistic regression|a discriminative algorithm|used to model p(y=1jx).When would we prefer one model over another? GDA and logistic regres-sion will, in general, give dierent decision boundaries when trained on thesame dataset. Which is better?We just argued that if p(xjy) is multivariate gaussian (with shared ),thenp(yjx) necessarily follows a logistic function. The converse, however,is not true; i.e., p(yjx) being a logistic function does not imply p(xjy) ismultivariate gaussian. This shows that GDA makes stronger modeling as-sumptions about the data than does logistic regression. It turns out thatwhen these modeling assumptions are correct, then GDA will nd better tsto the data, and is a better model. Specically, when p(xjy) is indeed gaus-sian (with shared ), then GDA is asymptotically ecient . Informally,this means that in the limit of very large training sets (large n), there is noalgorithm that is strictly better than GDA (in terms of, say, how accuratelythey estimate p(yjx)). In particular, it can be shown that in this setting,GDA will be a better algorithm than logistic regression; and more generally,even for small training set sizes, we would generally expect GDA to better.In contrast, by making signicantly weaker assumptions, logistic regres-sion is also more robust and less sensitive to incorrect modeling assumptions.There are many dierent sets of assumptions that would lead to p(yjx) takingthe form of a logistic function. For example, if xjy= 0Poisson(0), andxjy= 1Poisson(1), thenp(yjx) will be logistic. Logistic regression willalso work well on Poisson data like this. But if we were to use GDA on suchdata|and t Gaussian distributions to such non-Gaussian data|then theresults will be less predictable, and GDA may (or may not) do well.To summarize: GDA makes stronger modeling assumptions, and is moredata ecient (i.e., requires less training data to learn \\well\") when the mod-eling assumptions are correct or at least approximately correct. Logistic1This uses the convention of redening the x(i)'s on the right-hand-side to be ( d+ 1)-dimensional vectors by adding the extra coordinate x(i)0= 1; see problem set 1."
        ],
        "formulas": [
            "y= 1jx;;",
            "y= 1jx;;;0;1)",
            "y=1jx).When",
            "xjy= 0Poisson(0),",
            "andxjy= 1Poisson(1),",
            "0= 1;"
        ],
        "explanations": []
    },
    "page_42": {
        "content_preview": "41regression makes weaker assumptions, and is signicantly more robust todeviations from modeling assumptions. Specically, when the data is in-deed non-Gaussian, then in the limit of large datasets, logistic regression willalmost always do better than GDA. For this reason, in practice logistic re-gression is used more often than GDA. (Some related considerations aboutdiscriminative vs. generative models also apply for the Naive Bayes algo-rithm that we discuss next, but the Naive Bayes algorithm ...",
        "python_code": [
            "41regression makes weaker assumptions, and is signicantly more robust todeviations from modeling assumptions. Specically, when the data is in-deed non-Gaussian, then in the limit of large datasets, logistic regression willalmost always do better than GDA. For this reason, in practice logistic re-gression is used more often than GDA. (Some related considerations aboutdiscriminative vs. generative models also apply for the Naive Bayes algo-rithm that we discuss next, but the Naive Bayes algorithm is still considereda very good, and is certainly also a very popular, classication algorithm.)4.2 Naive bayes (Option Reading)In GDA, the feature vectors xwere continuous, real-valued vectors. Let'snow talk about a dierent learning algorithm in which the xj's are discrete-valued.For our motivating example, consider building an email spam lter usingmachine learning. Here, we wish to classify messages according to whetherthey are unsolicited commercial (spam) email, or non-spam email. Afterlearning to do this, we can then have our mail reader automatically lterout the spam messages and perhaps place them in a separate mail folder.Classifying emails is one example of a broader set of problems called textclassication .Let's say we have a training set (a set of emails labeled as spam or non-spam). We'll begin our construction of our spam lter by specifying thefeaturesxjused to represent an email.We will represent an email via a feature vector whose length is equal tothe number of words in the dictionary. Specically, if an email contains thej-th word of the dictionary, then we will set xj= 1; otherwise, we let xj= 0.For instance, the vectorx=26666666664100...1...037777777775aaardvarkaardwolf...buy...zygmurgyis used to represent an email that contains the words \\a\" and \\buy,\" but not"
        ],
        "formulas": [
            "xj= 1;",
            "xj= 0.For",
            "vectorx=26666666664100...1...037777777775aaardvarkaardwolf...buy...zygmurgyis"
        ],
        "explanations": []
    },
    "page_43": {
        "content_preview": "42\\aardvark,\" \\aardwolf\" or \\zygmurgy.\"2The set of words encoded into thefeature vector is called the vocabulary , so the dimension of xis equal tothe size of the vocabulary.Having chosen our feature vector, we now want to build a generativemodel. So, we have to model p(xjy). But if we have, say, a vocabulary of50000 words, then x2f0;1g50000(xis a 50000-dimensional vector of 0's and1's), and if we were to model xexplicitly with a multinomial distribution overthe 250000possible outcomes, then we'...",
        "python_code": [
            "42\\aardvark,\" \\aardwolf\" or \\zygmurgy.\"2The set of words encoded into thefeature vector is called the vocabulary , so the dimension of xis equal tothe size of the vocabulary.Having chosen our feature vector, we now want to build a generativemodel. So, we have to model p(xjy). But if we have, say, a vocabulary of50000 words, then x2f0;1g50000(xis a 50000-dimensional vector of 0's and1's), and if we were to model xexplicitly with a multinomial distribution overthe 250000possible outcomes, then we'd end up with a (2500001)-dimensionalparameter vector. This is clearly too many parameters.To modelp(xjy), we will therefore make a very strong assumption. We willassume that the xi's are conditionally independent given y. This assumptionis called the Naive Bayes (NB) assumption , and the resulting algorithm iscalled the Naive Bayes classier . For instance, if y= 1 means spam email;\\buy\" is word 2087 and \\price\" is word 39831; then we are assuming that ifI tell youy= 1 (that a particular piece of email is spam), then knowledgeofx2087(knowledge of whether \\buy\" appears in the message) will have noeect on your beliefs about the value of x39831 (whether \\price\" appears).More formally, this can be written p(x2087jy) =p(x2087jy;x 39831). (Note thatthis is notthe same as saying that x2087andx39831 are independent, whichwould have been written \\ p(x2087) =p(x2087jx39831)\"; rather, we are onlyassuming that x2087andx39831 are conditionally independent giveny.)We now have:p(x1;:::;x 50000jy)=p(x1jy)p(x2jy;x 1)p(x3jy;x 1;x2)p(x50000jy;x 1;:::;x 49999)=p(x1jy)p(x2jy)p(x3jy)p(x50000jy)=dYj=1p(xjjy)The rst equality simply follows from the usual properties of probabilities,and the second equality used the NB assumption. We note that even though2Actually, rather than looking through an English dictionary for the list of all Englishwords, in practice it is more common to look through our training set and encode in ourfeature vector only the words that occur at least once there. Apart from reducing thenumber of words modeled and hence reducing our computational and space requirements,this also has the advantage of allowing us to model/include as a feature many wordsthat may appear in your email (such as \\cs229\") but that you won't nd in a dictionary.Sometimes (as in the homework), we also exclude the very high frequency words (whichwill be words like \\the,\" \\of,\" \\and\"; these high frequency, \\content free\" words are calledstop words ) since they occur in so many documents and do little to indicate whether anemail is spam or non-spam."
        ],
        "formulas": [
            "y= 1",
            "youy= 1",
            "dYj=1p(xjjy)The"
        ],
        "explanations": []
    },
    "page_44": {
        "content_preview": "43the Naive Bayes assumption is an extremely strong assumptions, the resultingalgorithm works well on many problems.Our model is parameterized by jjy=1=p(xj= 1jy= 1),jjy=0=p(xj=1jy= 0), andy=p(y= 1). As usual, given a training set f(x(i);y(i));i=1;:::;ng, we can write down the joint likelihood of the data:L(y;jjy=0;jjy=1) =nYi=1p(x(i);y(i)):Maximizing this with respect to y;jjy=0andjjy=1gives the maximumlikelihood estimates:jjy=1=Pni=11fx(i)j= 1^y(i)= 1gPni=11fy(i)= 1gjjy=0=Pni=11fx(i)j= 1^y(i)=...",
        "python_code": [
            "43the Naive Bayes assumption is an extremely strong assumptions, the resultingalgorithm works well on many problems.Our model is parameterized by jjy=1=p(xj= 1jy= 1),jjy=0=p(xj=1jy= 0), andy=p(y= 1). As usual, given a training set f(x(i);y(i));i=1;:::;ng, we can write down the joint likelihood of the data:L(y;jjy=0;jjy=1) =nYi=1p(x(i);y(i)):Maximizing this with respect to y;jjy=0andjjy=1gives the maximumlikelihood estimates:jjy=1=Pni=11fx(i)j= 1^y(i)= 1gPni=11fy(i)= 1gjjy=0=Pni=11fx(i)j= 1^y(i)= 0gPni=11fy(i)= 0gy=Pni=11fy(i)= 1gnIn the equations above, the \\ ^\" symbol means \\and.\" The parameters havea very natural interpretation. For instance, jjy=1is just the fraction of thespam (y= 1) emails in which word jdoes appear.Having t all these parameters, to make a prediction on a new examplewith features x, we then simply calculatep(y= 1jx) =p(xjy= 1)p(y= 1)p(x)=Qdj=1p(xjjy= 1)p(y= 1)Qdj=1p(xjjy= 1)p(y= 1) +Qdj=1p(xjjy= 0)p(y= 0);and pick whichever class has the higher posterior probability.Lastly, we note that while we have developed the Naive Bayes algorithmmainly for the case of problems where the features xjare binary-valued, thegeneralization to where xjcan take values in f1;2;:::;kjgis straightforward.Here, we would simply model p(xjjy) as multinomial rather than as Bernoulli.Indeed, even if some original input attribute (say, the living area of a house,as in our earlier example) were continuous valued, it is quite common todiscretize it|that is, turn it into a small set of discrete values|and applyNaive Bayes. For instance, if we use some feature xjto represent living area,we might discretize the continuous values as follows:"
        ],
        "formulas": [
            "jjy=1=p(xj=",
            "1jy= 1),jjy=0=p(xj=1jy=",
            "andy=p(y=",
            "i=1;:::;ng,",
            "jjy=0;jjy=1)",
            "nYi=1p(x(i);y(i)):Maximizing",
            "jjy=0andjjy=1gives",
            "jjy=1=Pni=11fx(i)j=",
            "1gPni=11fy(i)=",
            "1gjjy=0=Pni=11fx(i)j=",
            "0gPni=11fy(i)=",
            "0gy=Pni=11fy(i)=",
            "jjy=1is",
            "y= 1)",
            "y= 1jx)",
            "xjy= 1)p(y=",
            "Qdj=1p(xjjy=",
            "y= 1)Qdj=1p(xjjy=",
            "y= 1)",
            "Qdj=1p(xjjy=",
            "y= 0);and"
        ],
        "explanations": []
    },
    "page_45": {
        "content_preview": "44Living area (sq. feet) <400 400-800 800-1200 1200-1600 >1600xi 1 2 3 4 5Thus, for a house with living area 890 square feet, we would set the valueof the corresponding feature xjto 3. We can then apply the Naive Bayesalgorithm, and model p(xjjy) with a multinomial distribution, as describedpreviously. When the original, continuous-valued attributes are not well-modeled by a multivariate normal distribution, discretizing the features andusing Naive Bayes (instead of GDA) will often result in a b...",
        "python_code": [
            "44Living area (sq. feet) <400 400-800 800-1200 1200-1600 >1600xi 1 2 3 4 5Thus, for a house with living area 890 square feet, we would set the valueof the corresponding feature xjto 3. We can then apply the Naive Bayesalgorithm, and model p(xjjy) with a multinomial distribution, as describedpreviously. When the original, continuous-valued attributes are not well-modeled by a multivariate normal distribution, discretizing the features andusing Naive Bayes (instead of GDA) will often result in a better classier.4.2.1 Laplace smoothingThe Naive Bayes algorithm as we have described it will work fairly wellfor many problems, but there is a simple change that makes it work muchbetter, especially for text classication. Let's briey discuss a problem withthe algorithm in its current form, and then talk about how we can x it.Consider spam/email classication, and let's suppose that, we are in theyear of 20xx, after completing CS229 and having done excellent work on theproject, you decide around May 20xx to submit work you did to the NeurIPSconference for publication.3Because you end up discussing the conferencein your emails, you also start getting messages with the word \\neurips\"in it. But this is your rst NeurIPS paper, and until this time, you hadnot previously seen any emails containing the word \\neurips\"; in particular\\neurips\" did not ever appear in your training set of spam/non-spam emails.Assuming that \\neurips\" was the 35000th word in the dictionary, your NaiveBayes spam lter therefore had picked its maximum likelihood estimates ofthe parameters 35000jyto be35000jy=1=Pni=11fx(i)35000 = 1^y(i)= 1gPni=11fy(i)= 1g= 035000jy=0=Pni=11fx(i)35000 = 1^y(i)= 0gPni=11fy(i)= 0g= 0I.e., because it has never seen \\neurips\" before in either spam or non-spamtraining examples, it thinks the probability of seeing it in either type of emailis zero. Hence, when trying to decide if one of these messages containing3NeurIPS is one of the top machine learning conferences. The deadline for submittinga paper is typically in May-June."
        ],
        "formulas": [
            "be35000jy=1=Pni=11fx(i)35000",
            "1gPni=11fy(i)=",
            "1g= 035000jy=0=Pni=11fx(i)35000",
            "0gPni=11fy(i)=",
            "0g= 0I.e.,"
        ],
        "explanations": []
    },
    "page_46": {
        "content_preview": "45\\neurips\" is spam, it calculates the class posterior probabilities, and obtainsp(y= 1jx) =Qdj=1p(xjjy= 1)p(y= 1)Qdj=1p(xjjy= 1)p(y= 1) +Qdj=1p(xjjy= 0)p(y= 0)=00:This is because each of the terms \\Qdj=1p(xjjy)\" includes a term p(x35000jy) =0 that is multiplied into it. Hence, our algorithm obtains 0 =0, and doesn'tknow how to make a prediction.Stating the problem more broadly, it is statistically a bad idea to esti-mate the probability of some event to be zero just because you haven't seenit b...",
        "python_code": [
            "45\\neurips\" is spam, it calculates the class posterior probabilities, and obtainsp(y= 1jx) =Qdj=1p(xjjy= 1)p(y= 1)Qdj=1p(xjjy= 1)p(y= 1) +Qdj=1p(xjjy= 0)p(y= 0)=00:This is because each of the terms \\Qdj=1p(xjjy)\" includes a term p(x35000jy) =0 that is multiplied into it. Hence, our algorithm obtains 0 =0, and doesn'tknow how to make a prediction.Stating the problem more broadly, it is statistically a bad idea to esti-mate the probability of some event to be zero just because you haven't seenit before in your nite training set. Take the problem of estimating the meanof a multinomial random variable ztaking values inf1;:::;kg. We can pa-rameterize our multinomial with j=p(z=j). Given a set of nindependentobservationsfz(1);:::;z(n)g, the maximum likelihood estimates are given byj=Pni=11fz(i)=jgn:As we saw previously, if we were to use these maximum likelihood estimates,then some of the j's might end up as zero, which was a problem. To avoidthis, we can use Laplace smoothing , which replaces the above estimatewithj=1 +Pni=11fz(i)=jgk+n:Here, we've added 1 to the numerator, and kto the denominator. Note thatPkj=1j= 1 still holds (check this yourself!), which is a desirable propertysince thej's are estimates for probabilities that we know must sum to 1.Also,j6= 0 for all values of j, solving our problem of probabilities beingestimated as zero. Under certain (arguably quite strong) conditions, it canbe shown that the Laplace smoothing actually gives the optimal estimatorof thej's.Returning to our Naive Bayes classier, with Laplace smoothing, wetherefore obtain the following estimates of the parameters:jjy=1=1 +Pni=11fx(i)j= 1^y(i)= 1g2 +Pni=11fy(i)= 1gjjy=0=1 +Pni=11fx(i)j= 1^y(i)= 0g2 +Pni=11fy(i)= 0g"
        ],
        "formulas": [
            "y= 1jx)",
            "Qdj=1p(xjjy=",
            "y= 1)Qdj=1p(xjjy=",
            "y= 1)",
            "Qdj=1p(xjjy=",
            "y= 0)=00:This",
            "Qdj=1p(xjjy)\"",
            "0 =0,",
            "j=p(z=j).",
            "byj=Pni=11fz(i)=jgn:As",
            "estimatewithj=1",
            "Pni=11fz(i)=jgk+n:Here,",
            "thatPkj=1j=",
            "j6= 0",
            "jjy=1=1",
            "Pni=11fx(i)j=",
            "Pni=11fy(i)=",
            "1gjjy=0=1",
            "Pni=11fx(i)j=",
            "Pni=11fy(i)="
        ],
        "explanations": []
    },
    "page_47": {
        "content_preview": "46(In practice, it usually doesn't matter much whether we apply Laplace smooth-ing toyor not, since we will typically have a fair fraction each of spam andnon-spam messages, so ywill be a reasonable estimate of p(y= 1) and willbe quite far from 0 anyway.)4.2.2 Event models for text classicationTo close o our discussion of generative learning algorithms, let's talk aboutone more model that is specically for text classication. While Naive Bayesas we've presented it will work well for many classica...",
        "python_code": [
            "46(In practice, it usually doesn't matter much whether we apply Laplace smooth-ing toyor not, since we will typically have a fair fraction each of spam andnon-spam messages, so ywill be a reasonable estimate of p(y= 1) and willbe quite far from 0 anyway.)4.2.2 Event models for text classicationTo close o our discussion of generative learning algorithms, let's talk aboutone more model that is specically for text classication. While Naive Bayesas we've presented it will work well for many classication problems, for textclassication, there is a related model that does even better.In the specic context of text classication, Naive Bayes as presented usesthe what's called the Bernoulli event model (or sometimes multi-variateBernoulli event model ). In this model, we assumed that the way an emailis generated is that rst it is randomly determined (according to the classpriorsp(y)) whether a spammer or non-spammer will send you your nextmessage. Then, the person sending the email runs through the dictionary,deciding whether to include each word jin that email independently andaccording to the probabilities p(xj= 1jy) =jjy. Thus, the probability of amessage was given by p(y)Qdj=1p(xjjy).Here's a dierent model, called the Multinomial event model . Todescribe this model, we will use a dierent notation and set of features forrepresenting emails. We let xjdenote the identity of the j-th word in theemail. Thus, xjis now an integer taking values in f1;:::;jVjg, wherejVjis the size of our vocabulary (dictionary). An email of dwords is now rep-resented by a vector ( x1;x2;:::;xd) of length d; note that dcan vary fordierent documents. For instance, if an email starts with \\A NeurIPS . . . ,\"thenx1= 1 (\\a\" is the rst word in the dictionary), and x2= 35000 (if\\neurips\" is the 35000th word in the dictionary).In the multinomial event model, we assume that the way an email isgenerated is via a random process in which spam/non-spam is rst deter-mined (according to p(y)) as before. Then, the sender of the email writes theemail by rst generating x1from some multinomial distribution over words(p(x1jy)). Next, the second word x2is chosen independently of x1but fromthe same multinomial distribution, and similarly for x3,x4, and so on, untilalldwords of the email have been generated. Thus, the overall probability ofa message is given by p(y)Qdj=1p(xjjy). Note that this formula looks like theone we had earlier for the probability of a message under the Bernoulli eventmodel, but that the terms in the formula now mean very dierent things. Inparticularxjjyis now a multinomial, rather than a Bernoulli distribution."
        ],
        "formulas": [
            "y= 1)",
            "xj= 1jy)",
            "Qdj=1p(xjjy).Here's",
            "thenx1= 1",
            "x2= 35000",
            "Qdj=1p(xjjy)."
        ],
        "explanations": []
    },
    "page_48": {
        "content_preview": "47The parameters for our new model are y=p(y) as before, kjy=1=p(xj=kjy= 1) (for any j) andkjy=0=p(xj=kjy= 0). Note that we haveassumed that p(xjjy) is the same for all values of j(i.e., that the distributionaccording to which a word is generated does not depend on its position jwithin the email).If we are given a training set f(x(i);y(i));i= 1;:::;ngwherex(i)=(x(i)1;x(i)2;:::;x(i)di) (here,diis the number of words in the i-training example),the likelihood of the data is given byL(y;kjy=0;kjy=1)...",
        "python_code": [
            "47The parameters for our new model are y=p(y) as before, kjy=1=p(xj=kjy= 1) (for any j) andkjy=0=p(xj=kjy= 0). Note that we haveassumed that p(xjjy) is the same for all values of j(i.e., that the distributionaccording to which a word is generated does not depend on its position jwithin the email).If we are given a training set f(x(i);y(i));i= 1;:::;ngwherex(i)=(x(i)1;x(i)2;:::;x(i)di) (here,diis the number of words in the i-training example),the likelihood of the data is given byL(y;kjy=0;kjy=1) =nYi=1p(x(i);y(i))=nYi=1 diYj=1p(x(i)jjy;kjy=0;kjy=1)!p(y(i);y):Maximizing this yields the maximum likelihood estimates of the parameters:kjy=1=Pni=1Pdij=11fx(i)j=k^y(i)= 1gPni=11fy(i)= 1gdikjy=0=Pni=1Pdij=11fx(i)j=k^y(i)= 0gPni=11fy(i)= 0gdiy=Pni=11fy(i)= 1gn:If we were to apply Laplace smoothing (which is needed in practice for goodperformance) when estimating kjy=0andkjy=1, we add 1 to the numeratorsandjVjto the denominators, and obtain:kjy=1=1 +Pni=1Pdij=11fx(i)j=k^y(i)= 1gjVj+Pni=11fy(i)= 1gdikjy=0=1 +Pni=1Pdij=11fx(i)j=k^y(i)= 0gjVj+Pni=11fy(i)= 0gdi:While not necessarily the very best classication algorithm, the Naive Bayesclassier often works surprisingly well. It is often also a very good \\rst thingto try,\" given its simplicity and ease of implementation."
        ],
        "formulas": [
            "y=p(y)",
            "kjy=1=p(xj=kjy=",
            "andkjy=0=p(xj=kjy=",
            "i= 1;:::;ngwherex(i)=(x(i)1;x(i)2;:::;x(i)di)",
            "kjy=0;kjy=1)",
            "nYi=1p(x(i);y(i))=nYi=1",
            "diYj=1p(x(i)jjy;kjy=0;kjy=1)!p(y(i);y):Maximizing",
            "kjy=1=Pni=1Pdij=11fx(i)j=k^y(i)=",
            "1gPni=11fy(i)=",
            "1gdikjy=0=Pni=1Pdij=11fx(i)j=k^y(i)=",
            "0gPni=11fy(i)=",
            "0gdiy=Pni=11fy(i)=",
            "kjy=0andkjy=1,",
            "kjy=1=1",
            "Pni=1Pdij=11fx(i)j=k^y(i)=",
            "Pni=11fy(i)=",
            "1gdikjy=0=1",
            "Pni=1Pdij=11fx(i)j=k^y(i)=",
            "Pni=11fy(i)="
        ],
        "explanations": []
    },
    "page_49": {
        "content_preview": "Chapter 5Kernel methods5.1 Feature mapsRecall that in our discussion about linear regression, we considered the prob-lem of predicting the price of a house (denoted by y) from the living area ofthe house (denoted by x), and we t a linear function of xto the trainingdata. What if the price ycan be more accurately represented as a non-linearfunction of x? In this case, we need a more expressive family of models thanlinear models.We start by considering tting cubic functions y=3x3+2x2+1x+0.It turns...",
        "python_code": [
            "Chapter 5Kernel methods5.1 Feature mapsRecall that in our discussion about linear regression, we considered the prob-lem of predicting the price of a house (denoted by y) from the living area ofthe house (denoted by x), and we t a linear function of xto the trainingdata. What if the price ycan be more accurately represented as a non-linearfunction of x? In this case, we need a more expressive family of models thanlinear models.We start by considering tting cubic functions y=3x3+2x2+1x+0.It turns out that we can view the cubic function as a linear function overthe a dierent set of feature variables (dened below). Concretely, let thefunction:R!R4be dened as(x) =26641xx2x337752R4: (5.1)Let2R4be the vector containing 0;1;2;3as entries. Then we canrewrite the cubic function in xas:3x3+2x2+1x+0=T(x)Thus, a cubic function of the variable xcan be viewed as a linear functionover the variables (x). To distinguish between these two sets of variables,in the context of kernel methods, we will call the \\original\" input value theinput attributes of a problem (in this case, x, the living area). When the48"
        ],
        "formulas": [
            "y=3x3+2x2+1x+0.It",
            "0=T(x)Thus,"
        ],
        "explanations": []
    },
    "page_50": {
        "content_preview": "49original input is mapped to some new set of quantities (x), we will call thosenew quantities the features variables. (Unfortunately, dierent authors usedierent terms to describe these two things in dierent contexts.) We willcallafeature map , which maps the attributes to the features.5.2 LMS (least mean squares) with featuresWe will derive the gradient descent algorithm for tting the model T(x).First recall that for ordinary least square problem where we were to t Tx,the batch gradient descent...",
        "python_code": [
            "49original input is mapped to some new set of quantities (x), we will call thosenew quantities the features variables. (Unfortunately, dierent authors usedierent terms to describe these two things in dierent contexts.) We willcallafeature map , which maps the attributes to the features.5.2 LMS (least mean squares) with featuresWe will derive the gradient descent algorithm for tting the model T(x).First recall that for ordinary least square problem where we were to t Tx,the batch gradient descent update is (see the rst lecture note for its deriva-tion)::=+nXi=1y(i)h(x(i))x(i):=+nXi=1y(i)Tx(i)x(i): (5.2)Let:Rd!Rpbe a feature map that maps attribute x(inRd) to thefeatures(x) inRp. (In the motivating example in the previous subsection,we haved= 1 andp= 4.) Now our goal is to t the function T(x), withbeing a vector in Rpinstead of Rd. We can replace all the occurrences ofx(i)in the algorithm above by (x(i)) to obtain the new update::=+nXi=1y(i)T(x(i))(x(i)) (5.3)Similarly, the corresponding stochastic gradient descent update rule is:=+y(i)T(x(i))(x(i)) (5.4)5.3 LMS with the kernel trickThe gradient descent update, or stochastic gradient update above becomescomputationally expensive when the features (x) is high-dimensional. Forexample, consider the direct extension of the feature map in equation (5.1)to high-dimensional input x: supposex2Rd, and let(x) be the vector that"
        ],
        "formulas": [
            "nXi=1y(i)h(x(i))x(i):=+nXi=1y(i)Tx(i)x(i):",
            "haved= 1",
            "andp= 4.)",
            "nXi=1y(i)T(x(i))(x(i))"
        ],
        "explanations": []
    },
    "page_51": {
        "content_preview": "50contains all the monomials of xwith degree3(x) =26666666666666666666666641x1x2...x21x1x2x1x3...x2x1...x31x21x2...3777777777777777777777775: (5.5)The dimension of the features (x) is on the order of d3.1This is a pro-hibitively long vector for computational purpose | when d= 1000, eachupdate requires at least computing and storing a 10003= 109dimensionalvector, which is 106times slower than the update rule for for ordinary leastsquares updates (5.2).It may appear at rst that such d3runtime per ...",
        "python_code": [
            "50contains all the monomials of xwith degree3(x) =26666666666666666666666641x1x2...x21x1x2x1x3...x2x1...x31x21x2...3777777777777777777777775: (5.5)The dimension of the features (x) is on the order of d3.1This is a pro-hibitively long vector for computational purpose | when d= 1000, eachupdate requires at least computing and storing a 10003= 109dimensionalvector, which is 106times slower than the update rule for for ordinary leastsquares updates (5.2).It may appear at rst that such d3runtime per update and memory usageare inevitable, because the vector itself is of dimension pd3, and we mayneed to update every entry of and store it. However, we will introduce thekernel trick with which we will not need to store explicitly, and the runtimecan be signicantly improved.For simplicity, we assume the initialize the value = 0, and we focuson the iterative update (5.3). The main observation is that at any time, can be represented as a linear combination of the vectors (x(1));:::; (x(n)).Indeed, we can show this inductively as follows. At initialization, = 0 =Pni=10(x(i)). Assume at some point, can be represented as=nXi=1i(x(i)) (5.6)1Here, for simplicity, we include all the monomials with repetitions (so that, e.g., x1x2x3andx2x3x1both appear in (x)). Therefore, there are totally 1 + d+d2+d3entries in(x)."
        ],
        "formulas": [
            "d= 1000,",
            "10003= 109dimensionalvector,",
            "value = 0,",
            "0 =Pni=10(x(i)).",
            "as=nXi=1i(x(i))"
        ],
        "explanations": []
    },
    "page_52": {
        "content_preview": "51for some1;:::;n2R. Then we claim that in the next round, is still alinear combination of (x(1));:::; (x(n)) because:=+nXi=1y(i)T(x(i))(x(i))=nXi=1i(x(i)) +nXi=1y(i)T(x(i))(x(i))=nXi=1(i+y(i)T(x(i)))|{z}newi(x(i)) (5.7)You may realize that our general strategy is to implicitly represent the p-dimensional vector by a set of coecients 1;:::;n. Towards doing this,we derive the update rule of the coecients 1;:::;n. Using the equationabove, we see that the new idepends on the old one viai:=i+y(i)T(x...",
        "python_code": [
            "51for some1;:::;n2R. Then we claim that in the next round, is still alinear combination of (x(1));:::; (x(n)) because:=+nXi=1y(i)T(x(i))(x(i))=nXi=1i(x(i)) +nXi=1y(i)T(x(i))(x(i))=nXi=1(i+y(i)T(x(i)))|{z}newi(x(i)) (5.7)You may realize that our general strategy is to implicitly represent the p-dimensional vector by a set of coecients 1;:::;n. Towards doing this,we derive the update rule of the coecients 1;:::;n. Using the equationabove, we see that the new idepends on the old one viai:=i+y(i)T(x(i))(5.8)Here we still have the old on the RHS of the equation. Replacing by=Pnj=1j(x(j)) gives8i2f1;:::;ng;i:=i+ y(i)nXj=1j(x(j))T(x(i))!We often rewrite (x(j))T(x(i)) ash(x(j));(x(i))ito emphasize that it's theinner product of the two feature vectors. Viewing i's as the new representa-tion of, we have successfully translated the batch gradient descent algorithminto an algorithm that updates the value of iteratively. It may appear thatat every iteration, we still need to compute the values of h(x(j));(x(i))iforall pairs of i;j, each of which may take roughly O(p) operation. However,two important properties come to rescue:1. We can pre-compute the pairwise inner products h(x(j));(x(i))ifor allpairs ofi;jbefore the loop starts.2. For the feature map dened in (5.5) (or many other interesting fea-ture maps), computing h(x(j));(x(i))ican be ecient and does not"
        ],
        "formulas": [
            "nXi=1y(i)T(x(i))(x(i))=nXi=1i(x(i))",
            "nXi=1y(i)T(x(i))(x(i))=nXi=1(i+y(i)T(x(i)))|{z}newi(x(i))",
            "by=Pnj=1j(x(j))",
            "nXj=1j(x(j))T(x(i))!We"
        ],
        "explanations": []
    },
    "page_53": {
        "content_preview": "52necessarily require computing (x(i)) explicitly. This is because:h(x);(z)i= 1 +dXi=1xizi+Xi;j2f1;:::;dgxixjzizj+Xi;j;k2f1;:::;dgxixjxkzizjzk= 1 +dXi=1xizi+ dXi=1xizi!2+ dXi=1xizi!3= 1 +hx;zi+hx;zi2+hx;zi3(5.9)Therefore, to compute h(x);(z)i, we can rst compute hx;ziwithO(d) time and then take another constant number of operations to com-pute 1 +hx;zi+hx;zi2+hx;zi3.As you will see, the inner products between the features h(x);(z)iareessential here. We dene the Kernel corresponding to the featur...",
        "python_code": [
            "52necessarily require computing (x(i)) explicitly. This is because:h(x);(z)i= 1 +dXi=1xizi+Xi;j2f1;:::;dgxixjzizj+Xi;j;k2f1;:::;dgxixjxkzizjzk= 1 +dXi=1xizi+ dXi=1xizi!2+ dXi=1xizi!3= 1 +hx;zi+hx;zi2+hx;zi3(5.9)Therefore, to compute h(x);(z)i, we can rst compute hx;ziwithO(d) time and then take another constant number of operations to com-pute 1 +hx;zi+hx;zi2+hx;zi3.As you will see, the inner products between the features h(x);(z)iareessential here. We dene the Kernel corresponding to the feature map asa function that maps XX! Rsatisfying:2K(x;z),h(x);(z)i (5.10)To wrap up the discussion, we write the down the nal algorithm asfollows:1. Compute all the values K(x(i);x(j)),h(x(i));(x(j))iusing equa-tion (5.9) for all i;j2f1;:::;ng. Set:= 0.2.Loop:8i2f1;:::;ng;i:=i+ y(i)nXj=1jK(x(i);x(j))!(5.11)Or in vector notation, letting Kbe thennmatrix with Kij=K(x(i);x(j)), we have:=+(~ yK)With the algorithm above, we can update the representation of thevectoreciently with O(n) time per update. Finally, we need to show that2Recall thatXis the space of the input x. In our running example, X=Rd"
        ],
        "formulas": [
            "i= 1",
            "dXi=1xizi+Xi;j2f1;:::;dgxixjzizj+Xi;j;k2f1;:::;dgxixjxkzizjzk=",
            "dXi=1xizi+",
            "dXi=1xizi!2+",
            "dXi=1xizi!3=",
            "nXj=1jK(x(i);x(j))!(5.11)Or",
            "Kij=K(x(i);x(j)),",
            "X=Rd"
        ],
        "explanations": []
    },
    "page_54": {
        "content_preview": "53the knowledge of the representation suces to compute the predictionT(x). Indeed, we haveT(x) =nXi=1i(x(i))T(x) =nXi=1iK(x(i);x) (5.12)You may realize that fundamentally all we need to know about the featuremap() is encapsulated in the corresponding kernel function K(;). Wewill expand on this in the next section.5.4 Properties of kernelsIn the last subsection, we started with an explicitly dened feature map ,which induces the kernel function K(x;z),h(x);(z)i. Then we saw thatthe kernel function...",
        "python_code": [
            "53the knowledge of the representation suces to compute the predictionT(x). Indeed, we haveT(x) =nXi=1i(x(i))T(x) =nXi=1iK(x(i);x) (5.12)You may realize that fundamentally all we need to know about the featuremap() is encapsulated in the corresponding kernel function K(;). Wewill expand on this in the next section.5.4 Properties of kernelsIn the last subsection, we started with an explicitly dened feature map ,which induces the kernel function K(x;z),h(x);(z)i. Then we saw thatthe kernel function is so intrinsic so that as long as the kernel function isdened, the whole training algorithm can be written entirely in the languageof the kernel without referring to the feature map , so can the prediction ofa test example x(equation (5.12).)Therefore, it would be tempted to dene other kernel function K(;) andrun the algorithm (5.11). Note that the algorithm (5.11) does not need toexplicitly access the feature map , and therefore we only need to ensure theexistence of the feature map , but do not necessarily need to be able toexplicitly write down.What kinds of functions K(;) can correspond to some feature map ? Inother words, can we tell if there is some feature mapping so thatK(x;z) =(x)T(z) for allx,z?If we can answer this question by giving a precise characterization of validkernel functions, then we can completely change the interface of selectingfeature maps to the interface of selecting kernel function K. Concretely,we can pick a function K, verify that it satises the characterization (sothat there exists a feature map thatKcorresponds to), and then we canrun update rule (5.11). The benet here is that we don't have to be ableto compute or write it down analytically, and we only need to know itsexistence. We will answer this question at the end of this subsection afterwe go through several concrete examples of kernels.Supposex;z2Rd, and let's rst consider the function K(;) dened as:K(x;z) = (xTz)2:"
        ],
        "formulas": [
            "nXi=1i(x(i))T(x)",
            "nXi=1iK(x(i);x)"
        ],
        "explanations": []
    },
    "page_55": {
        "content_preview": "54We can also write this asK(x;z) = dXi=1xizi! dXj=1xjzj!=dXi=1dXj=1xixjzizj=dXi;j=1(xixj)(zizj)Thus, we see that K(x;z) =h(x);(z)iis the kernel function that corre-sponds to the the feature mapping given (shown here for the case of d= 3)by(x) =26666666666664x1x1x1x2x1x3x2x1x2x2x2x3x3x1x3x2x3x337777777777775:Revisiting the computational eciency perspective of kernel, note that whereascalculating the high-dimensional (x) requiresO(d2) time, nding K(x;z)takes onlyO(d) time|linear in the dimension ...",
        "python_code": [
            "54We can also write this asK(x;z) = dXi=1xizi! dXj=1xjzj!=dXi=1dXj=1xixjzizj=dXi;j=1(xixj)(zizj)Thus, we see that K(x;z) =h(x);(z)iis the kernel function that corre-sponds to the the feature mapping given (shown here for the case of d= 3)by(x) =26666666666664x1x1x1x2x1x3x2x1x2x2x2x3x3x1x3x2x3x337777777777775:Revisiting the computational eciency perspective of kernel, note that whereascalculating the high-dimensional (x) requiresO(d2) time, nding K(x;z)takes onlyO(d) time|linear in the dimension of the input attributes.For another related example, also consider K(;) dened byK(x;z) = (xTz+c)2=dXi;j=1(xixj)(zizj) +dXi=1(p2cxi)(p2czi) +c2:(Check this yourself.) This function Kis a kernel function that corresponds"
        ],
        "formulas": [
            "dXi=1xizi!",
            "dXj=1xjzj!=dXi=1dXj=1xixjzizj=dXi;j=1(xixj)(zizj)Thus,",
            "d= 3)by(x)",
            "2=dXi;j=1(xixj)(zizj)",
            "dXi=1(p2cxi)(p2czi)"
        ],
        "explanations": []
    },
    "page_56": {
        "content_preview": "55to the feature mapping (again shown for d= 3)(x) =2666666666666666666664x1x1x1x2x1x3x2x1x2x2x2x3x3x1x3x2x3x3p2cx1p2cx2p2cx3c3777777777777777777775;and the parameter ccontrols the relative weighting between the xi(rstorder) and the xixj(second order) terms.More broadly, the kernel K(x;z) = (xTz+c)kcorresponds to a featuremapping to and+kkfeature space, corresponding of all monomials of theformxi1xi2:::xikthat are up to order k. However, despite working in thisO(dk)-dimensional space, computing ...",
        "python_code": [
            "55to the feature mapping (again shown for d= 3)(x) =2666666666666666666664x1x1x1x2x1x3x2x1x2x2x2x3x3x1x3x2x3x3p2cx1p2cx2p2cx3c3777777777777777777775;and the parameter ccontrols the relative weighting between the xi(rstorder) and the xixj(second order) terms.More broadly, the kernel K(x;z) = (xTz+c)kcorresponds to a featuremapping to and+kkfeature space, corresponding of all monomials of theformxi1xi2:::xikthat are up to order k. However, despite working in thisO(dk)-dimensional space, computing K(x;z) still takes only O(d) time, andhence we never need to explicitly represent feature vectors in this very highdimensional feature space.Kernels as similarity metrics. Now, let's talk about a slightly dierentview of kernels. Intuitively, (and there are things wrong with this intuition,but nevermind), if (x) and(z) are close together, then we might expectK(x;z) =(x)T(z) to be large. Conversely, if (x) and(z) are far apart|say nearly orthogonal to each other|then K(x;z) =(x)T(z) will be small.So, we can think of K(x;z) as some measurement of how similar are (x)and(z), or of how similar are xandz.Given this intuition, suppose that for some learning problem that you'reworking on, you've come up with some function K(x;z) that you think mightbe a reasonable measure of how similar xandzare. For instance, perhapsyou choseK(x;z) = expjjxzjj222:This is a reasonable measure of xandz's similarity, and is close to 1 whenxandzare close, and near 0 when xandzare far apart. Does there exist"
        ],
        "formulas": [
            "d= 3)(x)"
        ],
        "explanations": []
    },
    "page_57": {
        "content_preview": "56a feature map such that the kernel Kdened above satises K(x;z) =(x)T(z)? In this particular example, the answer is yes. This kernel is calledtheGaussian kernel , and corresponds to an innite dimensional featuremapping. We will give a precise characterization about what propertiesa functionKneeds to satisfy so that it can be a valid kernel function thatcorresponds to some feature map .Necessary conditions for valid kernels. Suppose for now that Kisindeed a valid kernel corresponding to some fea...",
        "python_code": [
            "56a feature map such that the kernel Kdened above satises K(x;z) =(x)T(z)? In this particular example, the answer is yes. This kernel is calledtheGaussian kernel , and corresponds to an innite dimensional featuremapping. We will give a precise characterization about what propertiesa functionKneeds to satisfy so that it can be a valid kernel function thatcorresponds to some feature map .Necessary conditions for valid kernels. Suppose for now that Kisindeed a valid kernel corresponding to some feature mapping , and we willrst see what properties it satises. Now, consider some nite set of npoints(not necessarily the training set) fx(1);:::;x(n)g, and let a square, n-by-nmatrixKbe dened so that its ( i;j)-entry is given by Kij=K(x(i);x(j)).This matrix is called the kernel matrix . Note that we've overloaded thenotation and used Kto denote both the kernel function K(x;z) and thekernel matrix K, due to their obvious close relationship.Now, ifKis a valid kernel, then Kij=K(x(i);x(j)) =(x(i))T(x(j)) =(x(j))T(x(i)) =K(x(j);x(i)) =Kji, and hence Kmust be symmetric. More-over, letting k(x) denote the k-th coordinate of the vector (x), we nd thatfor any vector z, we havezTKz =XiXjziKijzj=XiXjzi(x(i))T(x(j))zj=XiXjziXkk(x(i))k(x(j))zj=XkXiXjzik(x(i))k(x(j))zj=Xk Xizik(x(i))!20:The second-to-last step uses the fact thatPi;jaiaj= (Piai)2forai=zik(x(i)). Sincezwas arbitrary, this shows that Kis positive semi-denite(K0).Hence, we've shown that if Kis a valid kernel (i.e., if it corresponds tosome feature mapping ), then the corresponding kernel matrix K2Rnnis symmetric positive semidenite."
        ],
        "formulas": [
            "Kij=K(x(i);x(j)).This",
            "Kij=K(x(i);x(j))",
            "havezTKz =XiXjziKijzj=XiXjzi(x(i))T(x(j))zj=XiXjziXkk(x(i))k(x(j))zj=XkXiXjzik(x(i))k(x(j))zj=Xk",
            "jaiaj= (Piai)2forai=zik(x(i))."
        ],
        "explanations": []
    },
    "page_58": {
        "content_preview": "57Sucient conditions for valid kernels. More generally, the conditionabove turns out to be not only a necessary, but also a sucient, conditionforKto be a valid kernel (also called a Mercer kernel). The following resultis due to Mercer.3Theorem (Mercer). LetK:RdRd7!Rbe given. Then for Kto be a valid (Mercer) kernel, it is necessary and sucient that for anyfx(1);:::;x(n)g, (n<1), the corresponding kernel matrix is symmetric pos-itive semi-denite.Given a function K, apart from trying to nd a featur...",
        "python_code": [
            "57Sucient conditions for valid kernels. More generally, the conditionabove turns out to be not only a necessary, but also a sucient, conditionforKto be a valid kernel (also called a Mercer kernel). The following resultis due to Mercer.3Theorem (Mercer). LetK:RdRd7!Rbe given. Then for Kto be a valid (Mercer) kernel, it is necessary and sucient that for anyfx(1);:::;x(n)g, (n<1), the corresponding kernel matrix is symmetric pos-itive semi-denite.Given a function K, apart from trying to nd a feature mapping thatcorresponds to it, this theorem therefore gives another way of testing if it isa valid kernel. You'll also have a chance to play with these ideas more inproblem set 2.In class, we also briey talked about a couple of other examples of ker-nels. For instance, consider the digit recognition problem, in which givenan image (16x16 pixels) of a handwritten digit (0-9), we have to gure outwhich digit it was. Using either a simple polynomial kernel K(x;z) = (xTz)kor the Gaussian kernel, SVMs were able to obtain extremely good perfor-mance on this problem. This was particularly surprising since the inputattributesxwere just 256-dimensional vectors of the image pixel intensityvalues, and the system had no prior knowledge about vision, or even aboutwhich pixels are adjacent to which other ones. Another example that webriey talked about in lecture was that if the objects xthat we are tryingto classify are strings (say, xis a list of amino acids, which strung togetherform a protein), then it seems hard to construct a reasonable, \\small\" set offeatures for most learning algorithms, especially if dierent strings have dif-ferent lengths. However, consider letting (x) be a feature vector that countsthe number of occurrences of each length- ksubstring in x. If we're consid-ering strings of English letters, then there are 26ksuch strings. Hence, (x)is a 26kdimensional vector; even for moderate values of k, this is probablytoo big for us to eciently work with. (e.g., 264460000.) However, using(dynamic programming-ish) string matching algorithms, it is possible to ef-ciently compute K(x;z) =(x)T(z), so that we can now implicitly workin this 26k-dimensional feature space, but without ever explicitly computingfeature vectors in this space.3Many texts present Mercer's theorem in a slightly more complicated form involvingL2functions, but when the input attributes take values in Rd, the version given here isequivalent."
        ],
        "formulas": [],
        "explanations": []
    },
    "page_59": {
        "content_preview": "58Application of kernel methods: We've seen the application of kernelsto linear regression. In the next part, we will introduce the support vectormachines to which kernels can be directly applied. dwell too much longer onit here. In fact, the idea of kernels has signicantly broader applicability thanlinear regression and SVMs. Specically, if you have any learning algorithmthat you can write in terms of only inner products hx;zibetween inputattribute vectors, then by replacing this with K(x;z) wh...",
        "python_code": [],
        "formulas": [],
        "explanations": [
            "58Application of kernel methods: We've seen the application of kernelsto linear regression",
            "In the next part, we will introduce the support vectormachines to which kernels can be directly applied",
            "dwell too much longer onit here",
            "In fact, the idea of kernels has signicantly broader applicability thanlinear regression and SVMs",
            "Specically, if you have any learning algorithmthat you can write in terms of only inner products hx;zibetween inputattribute vectors, then by replacing this with K(x;z) whereKis a kernel,you can \\magically\" allow your algorithm to work eciently in the highdimensional feature space corresponding to K",
            "For instance, this kernel trickcan be applied with the perceptron to derive a kernel perceptron algorithm",
            "Many of the algorithms that we'll see later in this class will also be amenableto this method, which has come to be known as the \\kernel trick"
        ]
    },
    "page_60": {
        "content_preview": "Chapter 6Support vector machinesThis set of notes presents the Support Vector Machine (SVM) learning al-gorithm. SVMs are among the best (and many believe are indeed the best)\\o-the-shelf\" supervised learning algorithms. To tell the SVM story, we'llneed to rst talk about margins and the idea of separating data with a large\\gap.\" Next, we'll talk about the optimal margin classier, which will leadus into a digression on Lagrange duality. We'll also see kernels, which givea way to apply SVMs ecient...",
        "python_code": [
            "Chapter 6Support vector machinesThis set of notes presents the Support Vector Machine (SVM) learning al-gorithm. SVMs are among the best (and many believe are indeed the best)\\o-the-shelf\" supervised learning algorithms. To tell the SVM story, we'llneed to rst talk about margins and the idea of separating data with a large\\gap.\" Next, we'll talk about the optimal margin classier, which will leadus into a digression on Lagrange duality. We'll also see kernels, which givea way to apply SVMs eciently in very high dimensional (such as innite-dimensional) feature spaces, and nally, we'll close o the story with theSMO algorithm, which gives an ecient implementation of SVMs.6.1 Margins: intuitionWe'll start our story on SVMs by talking about margins. This section willgive the intuitions about margins and about the \\condence\" of our predic-tions; these ideas will be made formal in Section 6.3.Consider logistic regression, where the probability p(y= 1jx;) is mod-eled byh(x) =g(Tx). We then predict \\1\" on an input xif and only ifh(x)0:5, or equivalently, if and only if Tx0. Consider a positivetraining example ( y= 1). The larger Txis, the larger also is h(x) =p(y=1jx;), and thus also the higher our degree of \\condence\" that the label is 1.Thus, informally we can think of our prediction as being very condent thaty= 1 ifTx0. Similarly, we think of logistic regression as condentlypredictingy= 0, ifTx0. Given a training set, again informally it seemsthat we'd have found a good t to the training data if we can nd so thatTx(i)0 whenever y(i)= 1, andTx(i)0 whenever y(i)= 0, since thiswould reect a very condent (and correct) set of classications for all the59"
        ],
        "formulas": [
            "y= 1jx;)",
            "y= 1).",
            "y=1jx;),",
            "thaty= 1",
            "condentlypredictingy= 0,"
        ],
        "explanations": []
    },
    "page_61": {
        "content_preview": "60training examples. This seems to be a nice goal to aim for, and we'll soonformalize this idea using the notion of functional margins.For a dierent type of intuition, consider the following gure, in which x'srepresent positive training examples, o's denote negative training examples,a decision boundary (this is the line given by the equation Tx= 0, andis also called the separating hyperplane ) is also shown, and three pointshave also been labeled A, B and C./0 /1/0 /1/0 /1BACNotice that the poi...",
        "python_code": [
            "60training examples. This seems to be a nice goal to aim for, and we'll soonformalize this idea using the notion of functional margins.For a dierent type of intuition, consider the following gure, in which x'srepresent positive training examples, o's denote negative training examples,a decision boundary (this is the line given by the equation Tx= 0, andis also called the separating hyperplane ) is also shown, and three pointshave also been labeled A, B and C./0 /1/0 /1/0 /1BACNotice that the point A is very far from the decision boundary. If we areasked to make a prediction for the value of yat A, it seems we should bequite condent that y= 1 there. Conversely, the point C is very close tothe decision boundary, and while it's on the side of the decision boundaryon which we would predict y= 1, it seems likely that just a small change tothe decision boundary could easily have caused out prediction to be y= 0.Hence, we're much more condent about our prediction at A than at C. Thepoint B lies in-between these two cases, and more broadly, we see that ifa point is far from the separating hyperplane, then we may be signicantlymore condent in our predictions. Again, informally we think it would benice if, given a training set, we manage to nd a decision boundary thatallows us to make all correct and condent (meaning far from the decisionboundary) predictions on the training examples. We'll formalize this laterusing the notion of geometric margins."
        ],
        "formulas": [
            "Tx= 0,",
            "y= 1",
            "y= 1,",
            "y= 0.Hence,"
        ],
        "explanations": []
    },
    "page_62": {
        "content_preview": "616.2 Notation (option reading)To make our discussion of SVMs easier, we'll rst need to introduce a newnotation for talking about classication. We will be considering a linearclassier for a binary classication problem with labels yand features x.From now, we'll use y2f 1;1g(instead off0;1g) to denote the class labels.Also, rather than parameterizing our linear classier with the vector , wewill use parameters w;b, and write our classier ashw;b(x) =g(wTx+b):Here,g(z) = 1 ifz0, andg(z) =1 otherwise...",
        "python_code": [
            "616.2 Notation (option reading)To make our discussion of SVMs easier, we'll rst need to introduce a newnotation for talking about classication. We will be considering a linearclassier for a binary classication problem with labels yand features x.From now, we'll use y2f 1;1g(instead off0;1g) to denote the class labels.Also, rather than parameterizing our linear classier with the vector , wewill use parameters w;b, and write our classier ashw;b(x) =g(wTx+b):Here,g(z) = 1 ifz0, andg(z) =1 otherwise. This \\ w;b\" notationallows us to explicitly treat the intercept term bseparately from the otherparameters. (We also drop the convention we had previously of letting x0= 1be an extra coordinate in the input feature vector.) Thus, btakes the role ofwhat was previously 0, andwtakes the role of [ 1:::d]T.Note also that, from our denition of gabove, our classier will directlypredict either 1 or 1 (cf. the perceptron algorithm), without rst goingthrough the intermediate step of estimating p(y= 1) (which is what logisticregression does).6.3 Functional and geometric margins (op-tion reading)Let's formalize the notions of the functional and geometric margins. Given atraining example ( x(i);y(i)), we dene the functional margin of (w;b) withrespect to the training example as^(i)=y(i)(wTx(i)+b):Note that if y(i)= 1, then for the functional margin to be large (i.e., forour prediction to be condent and correct), we need wTx(i)+bto be a largepositive number. Conversely, if y(i)=1, then for the functional marginto be large, we need wTx(i)+bto be a large negative number. Moreover, ify(i)(wTx(i)+b)>0, then our prediction on this example is correct. (Checkthis yourself.) Hence, a large functional margin represents a condent and acorrect prediction.For a linear classier with the choice of ggiven above (taking values inf1;1g), there's one property of the functional margin that makes it not avery good measure of condence, however. Given our choice of g, we note that"
        ],
        "formulas": [
            "x0= 1be",
            "y= 1)"
        ],
        "explanations": []
    },
    "page_63": {
        "content_preview": "62if we replace wwith 2wandbwith 2b, then since g(wTx+b) =g(2wTx+2b),this would not change hw;b(x) at all. I.e., g, and hence also hw;b(x), dependsonly on the sign, but not on the magnitude, of wTx+b. However, replacing(w;b) with (2w;2b) also results in multiplying our functional margin by afactor of 2. Thus, it seems that by exploiting our freedom to scale wandb,we can make the functional margin arbitrarily large without really changinganything meaningful. Intuitively, it might therefore make s...",
        "python_code": [
            "62if we replace wwith 2wandbwith 2b, then since g(wTx+b) =g(2wTx+2b),this would not change hw;b(x) at all. I.e., g, and hence also hw;b(x), dependsonly on the sign, but not on the magnitude, of wTx+b. However, replacing(w;b) with (2w;2b) also results in multiplying our functional margin by afactor of 2. Thus, it seems that by exploiting our freedom to scale wandb,we can make the functional margin arbitrarily large without really changinganything meaningful. Intuitively, it might therefore make sense to imposesome sort of normalization condition such as that jjwjj2= 1; i.e., we mightreplace (w;b) with (w=jjwjj2;b=jjwjj2), and instead consider the functionalmargin of ( w=jjwjj2;b=jjwjj2). We'll come back to this later.Given a training set S=f(x(i);y(i));i= 1;:::;ng, we also dene thefunction margin of ( w;b) with respect to Sas the smallest of the functionalmargins of the individual training examples. Denoted by ^ , this can thereforebe written:^= mini=1;:::;n^(i):Next, let's talk about geometric margins . Consider the picture below:w AγB(i)The decision boundary corresponding to ( w;b) is shown, along with thevectorw. Note that wis orthogonal (at 90) to the separating hyperplane.(You should convince yourself that this must be the case.) Consider thepoint at A, which represents the input x(i)of some training example withlabely(i)= 1. Its distance to the decision boundary, (i), is given by the linesegment AB.How can we nd the value of (i)? Well,w=jjwjjis a unit-length vectorpointing in the same direction as w. SinceArepresentsx(i), we therefore"
        ],
        "formulas": [
            "jjwjj2= 1;",
            "w=jjwjj2;b=jjwjj2),",
            "w=jjwjj2;b=jjwjj2).",
            "S=f(x(i);y(i));i=",
            "mini=1;:::;n^(i):Next,",
            "w=jjwjjis"
        ],
        "explanations": []
    },
    "page_64": {
        "content_preview": "63nd that the point Bis given by x(i)(i)w=jjwjj. But this point lies onthe decision boundary, and all points xon the decision boundary satisfy theequationwTx+b= 0. Hence,wTx(i)(i)wjjwjj+b= 0:Solving for (i)yields(i)=wTx(i)+bjjwjj=wjjwjjTx(i)+bjjwjj:This was worked out for the case of a positive training example at A in thegure, where being on the \\positive\" side of the decision boundary is good.More generally, we dene the geometric margin of ( w;b) with respect to atraining example ( x(i);y(i)) ...",
        "python_code": [
            "63nd that the point Bis given by x(i)(i)w=jjwjj. But this point lies onthe decision boundary, and all points xon the decision boundary satisfy theequationwTx+b= 0. Hence,wTx(i)(i)wjjwjj+b= 0:Solving for (i)yields(i)=wTx(i)+bjjwjj=wjjwjjTx(i)+bjjwjj:This was worked out for the case of a positive training example at A in thegure, where being on the \\positive\" side of the decision boundary is good.More generally, we dene the geometric margin of ( w;b) with respect to atraining example ( x(i);y(i)) to be(i)=y(i) wjjwjjTx(i)+bjjwjj!:Note that ifjjwjj= 1, then the functional margin equals the geometricmargin|this thus gives us a way of relating these two dierent notions ofmargin. Also, the geometric margin is invariant to rescaling of the parame-ters; i.e., if we replace wwith 2wandbwith 2b, then the geometric margindoes not change. This will in fact come in handy later. Specically, becauseof this invariance to the scaling of the parameters, when trying to t wandbto training data, we can impose an arbitrary scaling constraint on wwithoutchanging anything important; for instance, we can demand that jjwjj= 1, orjw1j= 5, orjw1+bj+jw2j= 2, and any of these can be satised simply byrescalingwandb.Finally, given a training set S=f(x(i);y(i));i= 1;:::;ng, we also denethe geometric margin of ( w;b) with respect to Sto be the smallest of thegeometric margins on the individual training examples:= mini=1;:::;n(i):6.4 The optimal margin classier (option read-ing)Given a training set, it seems from our previous discussion that a naturaldesideratum is to try to nd a decision boundary that maximizes the (ge-ometric) margin, since this would reect a very condent set of predictions"
        ],
        "formulas": [
            "w=jjwjj.",
            "b= 0.",
            "b= 0:Solving",
            "bjjwjj=wjjwjjTx(i)+bjjwjj:This",
            "ifjjwjj= 1,",
            "jjwjj= 1,",
            "orjw1j= 5,",
            "jw2j= 2,",
            "S=f(x(i);y(i));i=",
            "mini=1;:::;n(i):6.4"
        ],
        "explanations": []
    },
    "page_65": {
        "content_preview": "64on the training set and a good \\t\" to the training data. Specically, thiswill result in a classier that separates the positive and the negative trainingexamples with a \\gap\" (geometric margin).For now, we will assume that we are given a training set that is linearlyseparable; i.e., that it is possible to separate the positive and negative ex-amples using some separating hyperplane. How will we nd the one thatachieves the maximum geometric margin? We can pose the following opti-mization problem...",
        "python_code": [
            "64on the training set and a good \\t\" to the training data. Specically, thiswill result in a classier that separates the positive and the negative trainingexamples with a \\gap\" (geometric margin).For now, we will assume that we are given a training set that is linearlyseparable; i.e., that it is possible to separate the positive and negative ex-amples using some separating hyperplane. How will we nd the one thatachieves the maximum geometric margin? We can pose the following opti-mization problem:max;w;bs.t.y(i)(wTx(i)+b); i = 1;:::;njjwjj= 1:I.e., we want to maximize , subject to each training example having func-tional margin at least . Thejjwjj= 1 constraint moreover ensures that thefunctional margin equals to the geometric margin, so we are also guaranteedthat all the geometric margins are at least . Thus, solving this problem willresult in (w;b) with the largest possible geometric margin with respect to thetraining set.If we could solve the optimization problem above, we'd be done. But the\\jjwjj= 1\" constraint is a nasty (non-convex) one, and this problem certainlyisn't in any format that we can plug into standard optimization software tosolve. So, let's try transforming the problem into a nicer one. Consider:max ^;w;b^jjwjjs.t.y(i)(wTx(i)+b)^; i = 1;:::;nHere, we're going to maximize ^ =jjwjj, subject to the functional margins allbeing at least ^ . Since the geometric and functional margins are related by= ^=jjwj, this will give us the answer we want. Moreover, we've gotten ridof the constraintjjwjj= 1 that we didn't like. The downside is that we nowhave a nasty (again, non-convex) objective^jjwjjfunction; and, we still don'thave any o-the-shelf software that can solve this form of an optimizationproblem.Let's keep going. Recall our earlier discussion that we can add an arbi-trary scaling constraint on wandbwithout changing anything. This is thekey idea we'll use now. We will introduce the scaling constraint that thefunctional margin of w;bwith respect to the training set must be 1:^= 1:"
        ],
        "formulas": [
            "i = 1;:::;njjwjj=",
            "Thejjwjj= 1",
            "jjwjj= 1\"",
            "i = 1;:::;nHere,",
            "by= ^=jjwj,",
            "constraintjjwjj= 1"
        ],
        "explanations": []
    },
    "page_66": {
        "content_preview": "65Since multiplying wandbby some constant results in the functional marginbeing multiplied by that same constant, this is indeed a scaling constraint,and can be satised by rescaling w;b. Plugging this into our problem above,and noting that maximizing ^ =jjwjj= 1=jjwjjis the same thing as minimizingjjwjj2, we now have the following optimization problem:minw;b12jjwjj2s.t.y(i)(wTx(i)+b)1; i= 1;:::;nWe've now transformed the problem into a form that can be ecientlysolved. The above is an optimizatio...",
        "python_code": [
            "65Since multiplying wandbby some constant results in the functional marginbeing multiplied by that same constant, this is indeed a scaling constraint,and can be satised by rescaling w;b. Plugging this into our problem above,and noting that maximizing ^ =jjwjj= 1=jjwjjis the same thing as minimizingjjwjj2, we now have the following optimization problem:minw;b12jjwjj2s.t.y(i)(wTx(i)+b)1; i= 1;:::;nWe've now transformed the problem into a form that can be ecientlysolved. The above is an optimization problem with a convex quadratic ob-jective and only linear constraints. Its solution gives us the optimal mar-gin classier . This optimization problem can be solved using commercialquadratic programming (QP) code.1While we could call the problem solved here, what we will instead do ismake a digression to talk about Lagrange duality. This will lead us to ouroptimization problem's dual form, which will play a key role in allowing us touse kernels to get optimal margin classiers to work eciently in very highdimensional spaces. The dual form will also allow us to derive an ecientalgorithm for solving the above optimization problem that will typically domuch better than generic QP software.6.5 Lagrange duality (optional reading)Let's temporarily put aside SVMs and maximum margin classiers, and talkabout solving constrained optimization problems.Consider a problem of the following form:minwf(w)s.t.hi(w) = 0; i= 1;:::;l:Some of you may recall how the method of Lagrange multipliers can be usedto solve it. (Don't worry if you haven't seen it before.) In this method, wedene the Lagrangian to beL(w;) =f(w) +lXi=1ihi(w)1You may be familiar with linear programming, which solves optimization problemsthat have linear objectives and linear constraints. QP software is also widely available,which allows convex quadratic objectives and linear constraints."
        ],
        "formulas": [
            "jjwjj= 1=jjwjjis",
            "i= 1;:::;nWe've",
            "i= 1;:::;l:Some",
            "lXi=1ihi(w)1You"
        ],
        "explanations": []
    },
    "page_67": {
        "content_preview": "66Here, thei's are called the Lagrange multipliers . We would then ndand setL's partial derivatives to zero:@L@wi= 0;@L@i= 0;and solve for wand.In this section, we will generalize this to constrained optimization prob-lems in which we may have inequality as well as equality constraints. Due totime constraints, we won't really be able to do the theory of Lagrange dualityjustice in this class,2but we will give the main ideas and results, which wewill then apply to our optimal margin classier's opt...",
        "python_code": [
            "66Here, thei's are called the Lagrange multipliers . We would then ndand setL's partial derivatives to zero:@L@wi= 0;@L@i= 0;and solve for wand.In this section, we will generalize this to constrained optimization prob-lems in which we may have inequality as well as equality constraints. Due totime constraints, we won't really be able to do the theory of Lagrange dualityjustice in this class,2but we will give the main ideas and results, which wewill then apply to our optimal margin classier's optimization problem.Consider the following, which we'll call the primal optimization problem:minwf(w)s.t.gi(w)0; i= 1;:::;khi(w) = 0; i= 1;:::;l:To solve it, we start by dening the generalized LagrangianL(w;; ) =f(w) +kXi=1igi(w) +lXi=1ihi(w):Here, thei's andi's are the Lagrange multipliers. Consider the quantityP(w) = max;:i0L(w;; ):Here, the \\P\" subscript stands for \\primal.\" Let some wbe given. If wviolates any of the primal constraints (i.e., if either gi(w)>0 orhi(w)6= 0for somei), then you should be able to verify thatP(w) = max;:i0f(w) +kXi=1igi(w) +lXi=1ihi(w) (6.1)=1: (6.2)Conversely, if the constraints are indeed satised for a particular value of w,thenP(w) =f(w). Hence,P(w) =f(w) ifwsatises primal constraints1 otherwise:2Readers interested in learning more about this topic are encouraged to read, e.g., R.T. Rockarfeller (1970), Convex Analysis, Princeton University Press."
        ],
        "formulas": [
            "wi= 0;@L@i=",
            "i= 1;:::;khi(w)",
            "i= 1;:::;l:To",
            "kXi=1igi(w)",
            "lXi=1ihi(w):Here,",
            "6= 0for",
            "kXi=1igi(w)",
            "lXi=1ihi(w)"
        ],
        "explanations": []
    },
    "page_68": {
        "content_preview": "67Thus,Ptakes the same value as the objective in our problem for all val-ues ofwthat satises the primal constraints, and is positive innity if theconstraints are violated. Hence, if we consider the minimization problemminwP(w) = minwmax;:i0L(w;; );we see that it is the same problem (i.e., and has the same solutions as) ouroriginal, primal problem. For later use, we also dene the optimal value ofthe objective to be p= minwP(w); we call this the value of the primalproblem.Now, let's look at a slig...",
        "python_code": [
            "67Thus,Ptakes the same value as the objective in our problem for all val-ues ofwthat satises the primal constraints, and is positive innity if theconstraints are violated. Hence, if we consider the minimization problemminwP(w) = minwmax;:i0L(w;; );we see that it is the same problem (i.e., and has the same solutions as) ouroriginal, primal problem. For later use, we also dene the optimal value ofthe objective to be p= minwP(w); we call this the value of the primalproblem.Now, let's look at a slightly dierent problem. We deneD(;) = minwL(w;; ):Here, the \\D\" subscript stands for \\dual.\" Note also that whereas in thedenition of Pwe were optimizing (maximizing) with respect to ;, herewe are minimizing with respect to w.We can now pose the dual optimization problem:max;:i0D(;) = max;:i0minwL(w;; ):This is exactly the same as our primal problem shown above, except that theorder of the \\max\" and the \\min\" are now exchanged. We also dene theoptimal value of the dual problem's objective to be d= max;:i0D(w).How are the primal and the dual problems related? It can easily be shownthatd= max;:i0minwL(w;; )minwmax;:i0L(w;; ) =p:(You should convince yourself of this; this follows from the \\max min\" of afunction always being less than or equal to the \\min max.\") However, undercertain conditions, we will haved=p;so that we can solve the dual problem in lieu of the primal problem. Let'ssee what these conditions are.Supposefand thegi's are convex,3and thehi's are ane.4Supposefurther that the constraints giare (strictly) feasible; this means that thereexists some wso thatgi(w)<0 for alli.3Whenfhas a Hessian, then it is convex if and only if the Hessian is positive semi-denite. For instance, f(w) =wTwis convex; similarly, all linear (and ane) functionsare also convex. (A function fcan also be convex without being dierentiable, but wewon't need those more general denitions of convexity here.)4I.e., there exists ai,bi, so thathi(w) =aTiw+bi. \\Ane\" means the same thing aslinear, except that we also allow the extra intercept term bi."
        ],
        "formulas": [
            "p= minwP(w);",
            "d= max;:i0D(w).How",
            "shownthatd= max;:i0minwL(w;;",
            "haved=p;so"
        ],
        "explanations": []
    },
    "page_69": {
        "content_preview": "68Under our above assumptions, there must exist w;;so thatwis thesolution to the primal problem, ;are the solution to the dual problem,and moreover p=d=L(w;;). Moreover, w;andsatisfy theKarush-Kuhn-Tucker (KKT) conditions , which are as follows:@@wiL(w;;) = 0; i= 1;:::;d (6.3)@@iL(w;;) = 0; i= 1;:::;l (6.4)igi(w) = 0; i= 1;:::;k (6.5)gi(w)0; i= 1;:::;k (6.6)0; i= 1;:::;k (6.7)Moreover, if some w;;satisfy the KKT conditions, then it is also a solution to t he primal and dualproblems.We draw atten...",
        "python_code": [
            "68Under our above assumptions, there must exist w;;so thatwis thesolution to the primal problem, ;are the solution to the dual problem,and moreover p=d=L(w;;). Moreover, w;andsatisfy theKarush-Kuhn-Tucker (KKT) conditions , which are as follows:@@wiL(w;;) = 0; i= 1;:::;d (6.3)@@iL(w;;) = 0; i= 1;:::;l (6.4)igi(w) = 0; i= 1;:::;k (6.5)gi(w)0; i= 1;:::;k (6.6)0; i= 1;:::;k (6.7)Moreover, if some w;;satisfy the KKT conditions, then it is also a solution to t he primal and dualproblems.We draw attention to Equation (6.5), which is called the KKT dualcomplementarity condition. Specically, it implies that if i>0, thengi(w) = 0. (I.e., the \\ gi(w)0\" constraint is active , meaning it holds withequality rather than with inequality.) Later on, this will be key for showingthat the SVM has only a small number of \\support vectors\"; the KKT dualcomplementarity condition will also give us our convergence test when wetalk about the SMO algorithm.6.6 Optimal margin classiers: the dual form(option reading)Note: The equivalence of optimization problem (6.8) and the optimizationproblem (6.12) , and the relationship between the primary and dual variablesin equation (6.10) are the most important take home messages of this section.Previously, we posed the following (primal) optimization problem for nd-ing the optimal margin classier:minw;b12jjwjj2(6.8)s.t.y(i)(wTx(i)+b)1; i= 1;:::;nWe can write the constraints asgi(w) =y(i)(wTx(i)+b) + 10:"
        ],
        "formulas": [
            "p=d=L(w;;).",
            "i= 1;:::;d",
            "i= 1;:::;l",
            "i= 1;:::;k",
            "i= 1;:::;k",
            "i= 1;:::;k",
            "i= 1;:::;nWe"
        ],
        "explanations": []
    },
    "page_70": {
        "content_preview": "69We have one such constraint for each training example. Note that from theKKT dual complementarity condition, we will have i>0 only for the train-ing examples that have functional margin exactly equal to one (i.e., the onescorresponding to constraints that hold with equality, gi(w) = 0). Considerthe gure below, in which a maximum margin separating hyperplane is shownby the solid line.The points with the smallest margins are exactly the ones closest to thedecision boundary; here, these are the t...",
        "python_code": [
            "69We have one such constraint for each training example. Note that from theKKT dual complementarity condition, we will have i>0 only for the train-ing examples that have functional margin exactly equal to one (i.e., the onescorresponding to constraints that hold with equality, gi(w) = 0). Considerthe gure below, in which a maximum margin separating hyperplane is shownby the solid line.The points with the smallest margins are exactly the ones closest to thedecision boundary; here, these are the three points (one negative and two pos-itive examples) that lie on the dashed lines parallel to the decision boundary.Thus, only three of the i's|namely, the ones corresponding to these threetraining examples|will be non-zero at the optimal solution to our optimiza-tion problem. These three points are called the support vectors in thisproblem. The fact that the number of support vectors can be much smallerthan the size the training set will be useful later.Let's move on. Looking ahead, as we develop the dual form of the prob-lem, one key idea to watch out for is that we'll try to write our algorithmin terms of only the inner product hx(i);x(j)i(think of this as ( x(i))Tx(j))between points in the input feature space. The fact that we can express ouralgorithm in terms of these inner products will be key when we apply thekernel trick.When we construct the Lagrangian for our optimization problem we have:L(w;b; ) =12jjwjj2nXi=1iy(i)(wTx(i)+b)1: (6.9)Note that there're only \\ i\" but no \\ i\" Lagrange multipliers, since theproblem has only inequality constraints."
        ],
        "formulas": [
            "12jjwjj2nXi=1iy(i)(wTx(i)+b)1:"
        ],
        "explanations": []
    },
    "page_71": {
        "content_preview": "70Let's nd the dual form of the problem. To do so, we need to rstminimizeL(w;b; ) with respect to wandb(for xed), to getD, whichwe'll do by setting the derivatives of Lwith respect to wandbto zero. Wehave:rwL(w;b; ) =wnXi=1iy(i)x(i)= 0This implies thatw=nXi=1iy(i)x(i): (6.10)As for the derivative with respect to b, we obtain@@bL(w;b; ) =nXi=1iy(i)= 0: (6.11)If we take the denition of win Equation (6.10) and plug that back intothe Lagrangian (Equation 6.9), and simplify, we getL(w;b; ) =nXi=1i12n...",
        "python_code": [
            "70Let's nd the dual form of the problem. To do so, we need to rstminimizeL(w;b; ) with respect to wandb(for xed), to getD, whichwe'll do by setting the derivatives of Lwith respect to wandbto zero. Wehave:rwL(w;b; ) =wnXi=1iy(i)x(i)= 0This implies thatw=nXi=1iy(i)x(i): (6.10)As for the derivative with respect to b, we obtain@@bL(w;b; ) =nXi=1iy(i)= 0: (6.11)If we take the denition of win Equation (6.10) and plug that back intothe Lagrangian (Equation 6.9), and simplify, we getL(w;b; ) =nXi=1i12nXi;j=1y(i)y(j)ij(x(i))Tx(j)bnXi=1iy(i):But from Equation (6.11), the last term must be zero, so we obtainL(w;b; ) =nXi=1i12nXi;j=1y(i)y(j)ij(x(i))Tx(j):Recall that we got to the equation above by minimizing Lwith respect towandb. Putting this together with the constraints i0 (that we alwayshad) and the constraint (6.11), we obtain the following dual optimizationproblem:maxW() =nXi=1i12nXi;j=1y(i)y(j)ijhx(i);x(j)i: (6.12)s.t.i0; i= 1;:::;nnXi=1iy(i)= 0;You should also be able to verify that the conditions required for p=dand the KKT conditions (Equations 6.3{6.7) to hold are indeed satised in"
        ],
        "formulas": [
            "wnXi=1iy(i)x(i)=",
            "thatw=nXi=1iy(i)x(i):",
            "nXi=1iy(i)=",
            "nXi=1i12nXi;j=1y(i)y(j)ij(x(i))Tx(j)bnXi=1iy(i):But",
            "nXi=1i12nXi;j=1y(i)y(j)ij(x(i))Tx(j):Recall",
            "nXi=1i12nXi;j=1y(i)y(j)ijhx(i);x(j)i:",
            "i= 1;:::;nnXi=1iy(i)=",
            "p=dand"
        ],
        "explanations": []
    },
    "page_72": {
        "content_preview": "71our optimization problem. Hence, we can solve the dual in lieu of solvingthe primal problem. Specically, in the dual problem above, we have amaximization problem in which the parameters are the i's. We'll talk laterabout the specic algorithm that we're going to use to solve the dual problem,but if we are indeed able to solve it (i.e., nd the 's that maximize W()subject to the constraints), then we can use Equation (6.10) to go back andnd the optimal w's as a function of the 's. Having found w,...",
        "python_code": [
            "71our optimization problem. Hence, we can solve the dual in lieu of solvingthe primal problem. Specically, in the dual problem above, we have amaximization problem in which the parameters are the i's. We'll talk laterabout the specic algorithm that we're going to use to solve the dual problem,but if we are indeed able to solve it (i.e., nd the 's that maximize W()subject to the constraints), then we can use Equation (6.10) to go back andnd the optimal w's as a function of the 's. Having found w, by consideringthe primal problem, it is also straightforward to nd the optimal value forthe intercept term basb=maxi:y(i)=1wTx(i)+ mini:y(i)=1wTx(i)2: (6.13)(Check for yourself that this is correct.)Before moving on, let's also take a more careful look at Equation (6.10),which gives the optimal value of win terms of (the optimal value of) .Suppose we've t our model's parameters to a training set, and now wish tomake a prediction at a new point input x. We would then calculate wTx+b,and predict y= 1 if and only if this quantity is bigger than zero. Butusing (6.10), this quantity can also be written:wTx+b= nXi=1iy(i)x(i)!Tx+b (6.14)=nXi=1iy(i)hx(i);xi+b: (6.15)Hence, if we've found the i's, in order to make a prediction, we have tocalculate a quantity that depends only on the inner product between xandthe points in the training set. Moreover, we saw earlier that the i's will allbe zero except for the support vectors. Thus, many of the terms in the sumabove will be zero, and we really need to nd only the inner products betweenxand the support vectors (of which there is often only a small number) inorder calculate (6.15) and make our prediction.By examining the dual form of the optimization problem, we gained sig-nicant insight into the structure of the problem, and were also able to writethe entire algorithm in terms of only inner products between input featurevectors. In the next section, we will exploit this property to apply the ker-nels to our classication problem. The resulting algorithm, support vectormachines , will be able to eciently learn in very high dimensional spaces."
        ],
        "formulas": [
            "basb=maxi:y(i)=1wTx(i)+",
            "y= 1",
            "b= nXi=1iy(i)x(i)!Tx+b",
            "nXi=1iy(i)hx(i);xi+b:"
        ],
        "explanations": []
    },
    "page_73": {
        "content_preview": "726.7 Regularization and the non-separable case(optional reading)The derivation of the SVM as presented so far assumed that the data islinearly separable. While mapping data to a high dimensional feature spaceviadoes generally increase the likelihood that the data is separable, wecan't guarantee that it always will be so. Also, in some cases it is not clearthat nding a separating hyperplane is exactly what we'd want to do, sincethat might be susceptible to outliers. For instance, the left gure b...",
        "python_code": [
            "726.7 Regularization and the non-separable case(optional reading)The derivation of the SVM as presented so far assumed that the data islinearly separable. While mapping data to a high dimensional feature spaceviadoes generally increase the likelihood that the data is separable, wecan't guarantee that it always will be so. Also, in some cases it is not clearthat nding a separating hyperplane is exactly what we'd want to do, sincethat might be susceptible to outliers. For instance, the left gure belowshows an optimal margin classier, and when a single outlier is added in theupper-left region (right gure), it causes the decision boundary to make adramatic swing, and the resulting classier has a much smaller margin.To make the algorithm work for non-linearly separable datasets as wellas be less sensitive to outliers, we reformulate our optimization (using `1regularization ) as follows:min;w;b12jjwjj2+CnXi=1is.t.y(i)(wTx(i)+b)1i; i= 1;:::;ni0; i= 1;:::;n:Thus, examples are now permitted to have (functional) margin less than 1,and if an example has functional margin 1 i(with >0), we would paya cost of the objective function being increased by Ci. The parameter Ccontrols the relative weighting between the twin goals of making the jjwjj2small (which we saw earlier makes the margin large) and of ensuring thatmost examples have functional margin at least 1."
        ],
        "formulas": [
            "CnXi=1is.t.y(i)(wTx(i)+b)1i;",
            "i= 1;:::;ni0;",
            "i= 1;:::;n:Thus,"
        ],
        "explanations": []
    },
    "page_74": {
        "content_preview": "73As before, we can form the Lagrangian:L(w;b;;;r ) =12wTw+CnXi=1inXi=1iy(i)(xTw+b)1 +inXi=1rii:Here, thei's andri's are our Lagrange multipliers (constrained to be 0).We won't go through the derivation of the dual again in detail, but aftersetting the derivatives with respect to wandbto zero as before, substitutingthem back in, and simplifying, we obtain the following dual form of theproblem:maxW() =nXi=1i12nXi;j=1y(i)y(j)ijhx(i);x(j)is.t. 0iC; i = 1;:::;nnXi=1iy(i)= 0;As before, we also have t...",
        "python_code": [
            "73As before, we can form the Lagrangian:L(w;b;;;r ) =12wTw+CnXi=1inXi=1iy(i)(xTw+b)1 +inXi=1rii:Here, thei's andri's are our Lagrange multipliers (constrained to be 0).We won't go through the derivation of the dual again in detail, but aftersetting the derivatives with respect to wandbto zero as before, substitutingthem back in, and simplifying, we obtain the following dual form of theproblem:maxW() =nXi=1i12nXi;j=1y(i)y(j)ijhx(i);x(j)is.t. 0iC; i = 1;:::;nnXi=1iy(i)= 0;As before, we also have that wcan be expressed in terms of the i's asgiven in Equation (6.10), so that after solving the dual problem, we can con-tinue to use Equation (6.15) to make our predictions. Note that, somewhatsurprisingly, in adding `1regularization, the only change to the dual prob-lem is that what was originally a constraint that 0 ihas now become0iC. The calculation for balso has to be modied (Equation 6.13 isno longer valid); see the comments in the next section/Platt's paper.Also, the KKT dual-complementarity conditions (which in the next sec-tion will be useful for testing for the convergence of the SMO algorithm)are:i= 0)y(i)(wTx(i)+b)1 (6.16)i=C)y(i)(wTx(i)+b)1 (6.17)0<i<C)y(i)(wTx(i)+b) = 1: (6.18)Now, all that remains is to give an algorithm for actually solving the dualproblem, which we will do in the next section.6.8 The SMO algorithm (optional reading)The SMO (sequential minimal optimization) algorithm, due to John Platt,gives an ecient way of solving the dual problem arising from the derivation"
        ],
        "formulas": [
            "CnXi=1inXi=1iy(i)(xTw+b)1",
            "inXi=1rii:Here,",
            "nXi=1i12nXi;j=1y(i)y(j)ijhx(i);x(j)is.t.",
            "i = 1;:::;nnXi=1iy(i)=",
            "i= 0)y(i)(wTx(i)+b)1",
            "i=C)y(i)(wTx(i)+b)1"
        ],
        "explanations": []
    },
    "page_75": {
        "content_preview": "74of the SVM. Partly to motivate the SMO algorithm, and partly because it'sinteresting in its own right, let's rst take another digression to talk aboutthe coordinate ascent algorithm.6.8.1 Coordinate ascentConsider trying to solve the unconstrained optimization problemmaxW(1;2;:::;n):Here, we think of Was just some function of the parameters i's, and for nowignore any relationship between this problem and SVMs. We've already seentwo optimization algorithms, gradient ascent and Newton's method. ...",
        "python_code": [
            "74of the SVM. Partly to motivate the SMO algorithm, and partly because it'sinteresting in its own right, let's rst take another digression to talk aboutthe coordinate ascent algorithm.6.8.1 Coordinate ascentConsider trying to solve the unconstrained optimization problemmaxW(1;2;:::;n):Here, we think of Was just some function of the parameters i's, and for nowignore any relationship between this problem and SVMs. We've already seentwo optimization algorithms, gradient ascent and Newton's method. Thenew algorithm we're going to consider here is called coordinate ascent :Loop until convergence: fFori= 1;:::;n ,fi:= arg max ^iW(1;:::;i1;^i;i+1;:::;n).ggThus, in the innermost loop of this algorithm, we will hold all the variablesexcept for some ixed, and reoptimize Wwith respect to just the parameteri. In the version of this method presented here, the inner-loop reoptimizesthe variables in order 1;2;:::;n;1;2;:::. (A more sophisticated versionmight choose other orderings; for instance, we may choose the next variableto update according to which one we expect to allow us to make the largestincrease in W().)When the function Whappens to be of such a form that the \\arg max\"in the inner loop can be performed eciently, then coordinate ascent can bea fairly ecient algorithm. Here's a picture of coordinate ascent in action:"
        ],
        "formulas": [
            "fFori= 1;:::;n"
        ],
        "explanations": []
    },
    "page_76": {
        "content_preview": "75−2 −1.5 −1 −0.5 0 0.5 1 1.5 2 2.5−2−1.5−1−0.500.511.522.5The ellipses in the gure are the contours of a quadratic function thatwe want to optimize. Coordinate ascent was initialized at (2 ;2), and alsoplotted in the gure is the path that it took on its way to the global maximum.Notice that on each step, coordinate ascent takes a step that's parallel to oneof the axes, since only one variable is being optimized at a time.6.8.2 SMOWe close o the discussion of SVMs by sketching the derivation of ...",
        "python_code": [
            "75−2 −1.5 −1 −0.5 0 0.5 1 1.5 2 2.5−2−1.5−1−0.500.511.522.5The ellipses in the gure are the contours of a quadratic function thatwe want to optimize. Coordinate ascent was initialized at (2 ;2), and alsoplotted in the gure is the path that it took on its way to the global maximum.Notice that on each step, coordinate ascent takes a step that's parallel to oneof the axes, since only one variable is being optimized at a time.6.8.2 SMOWe close o the discussion of SVMs by sketching the derivation of the SMOalgorithm.Here's the (dual) optimization problem that we want to solve:maxW() =nXi=1i12nXi;j=1y(i)y(j)ijhx(i);x(j)i: (6.19)s.t. 0iC; i = 1;:::;n (6.20)nXi=1iy(i)= 0: (6.21)Let's say we have set of i's that satisfy the constraints (6.20-6.21). Now,suppose we want to hold 2;:::;nxed, and take a coordinate ascent stepand reoptimize the objective with respect to 1. Can we make any progress?The answer is no, because the constraint (6.21) ensures that1y(1)=nXi=2iy(i):"
        ],
        "formulas": [
            "nXi=1i12nXi;j=1y(i)y(j)ijhx(i);x(j)i:",
            "i = 1;:::;n",
            "nXi=1iy(i)=",
            "nXi=2iy(i):"
        ],
        "explanations": []
    },
    "page_77": {
        "content_preview": "76Or, by multiplying both sides by y(1), we equivalently have1=y(1)nXi=2iy(i):(This step used the fact that y(1)2f 1;1g, and hence ( y(1))2= 1.) Hence,1is exactly determined by the other i's, and if we were to hold 2;:::;nxed, then we can't make any change to 1without violating the con-straint (6.21) in the optimization problem.Thus, if we want to update some subject of the i's, we must update atleast two of them simultaneously in order to keep satisfying the constraints.This motivates the SMO a...",
        "python_code": [
            "76Or, by multiplying both sides by y(1), we equivalently have1=y(1)nXi=2iy(i):(This step used the fact that y(1)2f 1;1g, and hence ( y(1))2= 1.) Hence,1is exactly determined by the other i's, and if we were to hold 2;:::;nxed, then we can't make any change to 1without violating the con-straint (6.21) in the optimization problem.Thus, if we want to update some subject of the i's, we must update atleast two of them simultaneously in order to keep satisfying the constraints.This motivates the SMO algorithm, which simply does the following:Repeat till convergence f1. Select some pair iandjto update next (using a heuristic thattries to pick the two that will allow us to make the biggest progresstowards the global maximum).2. Reoptimize W() with respect to iandj, while holding all theotherk's (k6=i;j) xed.gTo test for convergence of this algorithm, we can check whether the KKTconditions (Equations 6.16-6.18) are satised to within some tol. Here, tolisthe convergence tolerance parameter, and is typically set to around 0.01 to0.001. (See the paper and pseudocode for details.)The key reason that SMO is an ecient algorithm is that the update toi,jcan be computed very eciently. Let's now briey sketch the mainideas for deriving the ecient update.Let's say we currently have some setting of the i's that satisfy the con-straints (6.20-6.21), and suppose we've decided to hold 3;:::;nxed, andwant to reoptimize W(1;2;:::;n) with respect to 1and2(subject tothe constraints). From (6.21), we require that1y(1)+2y(2)=nXi=3iy(i):Since the right hand side is xed (as we've xed 3;:::n), we can just letit be denoted by some constant :1y(1)+2y(2)=: (6.22)We can thus picture the constraints on 1and2as follows:"
        ],
        "formulas": [
            "have1=y(1)nXi=2iy(i):(This",
            "2= 1.)",
            "k6=i;j)",
            "nXi=3iy(i):Since"
        ],
        "explanations": []
    },
    "page_78": {
        "content_preview": "77α2α1α1 α2CC(1)+(2)y y=ζHLFrom the constraints (6.20), we know that 1and2must lie within the box[0;C][0;C] shown. Also plotted is the line 1y(1)+2y(2)=, on which weknow1and2must lie. Note also that, from these constraints, we knowL2H; otherwise, ( 1;2) can't simultaneously satisfy both the boxand the straight line constraint. In this example, L= 0. But depending onwhat the line 1y(1)+2y(2)=looks like, this won't always necessarily bethe case; but more generally, there will be some lower-bound L...",
        "python_code": [
            "77α2α1α1 α2CC(1)+(2)y y=ζHLFrom the constraints (6.20), we know that 1and2must lie within the box[0;C][0;C] shown. Also plotted is the line 1y(1)+2y(2)=, on which weknow1and2must lie. Note also that, from these constraints, we knowL2H; otherwise, ( 1;2) can't simultaneously satisfy both the boxand the straight line constraint. In this example, L= 0. But depending onwhat the line 1y(1)+2y(2)=looks like, this won't always necessarily bethe case; but more generally, there will be some lower-bound Land someupper-bound Hon the permissible values for 2that will ensure that 1,2lie within the box [0 ;C][0;C].Using Equation (6.22), we can also write 1as a function of 2:1= (2y(2))y(1):(Check this derivation yourself; we again used the fact that y(1)2f 1;1gsothat (y(1))2= 1.) Hence, the objective W() can be writtenW(1;2;:::;n) =W((2y(2))y(1);2;:::;n):Treating3;:::;nas constants, you should be able to verify that this isjust some quadratic function in 2. I.e., this can also be expressed in theforma22+b2+cfor some appropriate a,b, andc. If we ignore the \\box\"constraints (6.20) (or, equivalently, that L2H), then we can easilymaximize this quadratic function by setting its derivative to zero and solving.We'll letnew;unclipped2 denote the resulting value of 2. You should also beable to convince yourself that if we had instead wanted to maximize Wwithrespect to2but subject to the box constraint, then we can nd the resultingvalue optimal simply by taking new;unclipped2 and \\clipping\" it to lie in the"
        ],
        "formulas": [
            "y=ζHLFrom",
            "L= 0.",
            "1= (2y(2))y(1):(Check",
            "2= 1.)"
        ],
        "explanations": []
    },
    "page_79": {
        "content_preview": "78[L;H] interval, to getnew2 =8<:H ifnew;unclipped2 >Hnew;unclipped2 ifLnew;unclipped2HL ifnew;unclipped2 <LFinally, having found the new2, we can use Equation (6.22) to go back andnd the optimal value of new1.There're a couple more details that are quite easy but that we'll leave youto read about yourself in Platt's paper: One is the choice of the heuristicsused to select the next i,jto update; the other is how to update bas theSMO algorithm is run....",
        "python_code": [
            "78[L;H] interval, to getnew2 =8<:H ifnew;unclipped2 >Hnew;unclipped2 ifLnew;unclipped2HL ifnew;unclipped2 <LFinally, having found the new2, we can use Equation (6.22) to go back andnd the optimal value of new1.There're a couple more details that are quite easy but that we'll leave youto read about yourself in Platt's paper: One is the choice of the heuristicsused to select the next i,jto update; the other is how to update bas theSMO algorithm is run."
        ],
        "formulas": [
            "getnew2 =8<:H"
        ],
        "explanations": []
    },
    "page_80": {
        "content_preview": "Part IIDeep learning79...",
        "python_code": [],
        "formulas": [],
        "explanations": [
            "Part IIDeep learning79"
        ]
    },
    "page_81": {
        "content_preview": "Chapter 7Deep learningWe now begin our study of deep learning. In this set of notes, we give anoverview of neural networks, discuss vectorization and discuss training neuralnetworks with backpropagation.7.1 Supervised learning with non-linear mod-elsIn the supervised learning setting (predicting yfrom the input x), supposeour model/hypothesis is h(x). In the past lectures, we have considered thecases when h(x) =>x(in linear regression) or h(x) =>(x) (where(x)is the feature map). A commonality of...",
        "python_code": [
            "Chapter 7Deep learningWe now begin our study of deep learning. In this set of notes, we give anoverview of neural networks, discuss vectorization and discuss training neuralnetworks with backpropagation.7.1 Supervised learning with non-linear mod-elsIn the supervised learning setting (predicting yfrom the input x), supposeour model/hypothesis is h(x). In the past lectures, we have considered thecases when h(x) =>x(in linear regression) or h(x) =>(x) (where(x)is the feature map). A commonality of these two models is that they arelinear in the parameters . Next we will consider learning general family ofmodels that are non-linear in both the parameters and the inputs x. Themost common non-linear models are neural networks, which we will denestaring from the next section. For this section, it suces to think h(x) asan abstract non-linear model.1Supposef(x(i);y(i))gni=1are the training examples. We will dene thenonlinear model and the loss/cost function for learning it.Regression problems. For simplicity, we start with the case where theoutput is a real number, that is, y(i)2R, and thus the model halso outputsa real number h(x)2R. We dene the least square cost function for the1If a concrete example is helpful, perhaps think about the model h(x) =21x21+22x22++2dx2din this subsection, even though it's not a neural network.80"
        ],
        "formulas": [
            "gni=1are"
        ],
        "explanations": []
    },
    "page_82": {
        "content_preview": "81i-th example ( x(i);y(i)) asJ(i)() =12(h(x(i))y(i))2; (7.1)and dene the mean-square cost function for the dataset asJ() =1nnXi=1J(i)(); (7.2)which is same as in linear regression except that we introduce a constant1=nin front of the cost function to be consistent with the convention. Notethat multiplying the cost function with a scalar will not change the localminima or global minima of the cost function. Also note that the underlyingparameterization for h(x) is dierent from the case of linear...",
        "python_code": [
            "81i-th example ( x(i);y(i)) asJ(i)() =12(h(x(i))y(i))2; (7.1)and dene the mean-square cost function for the dataset asJ() =1nnXi=1J(i)(); (7.2)which is same as in linear regression except that we introduce a constant1=nin front of the cost function to be consistent with the convention. Notethat multiplying the cost function with a scalar will not change the localminima or global minima of the cost function. Also note that the underlyingparameterization for h(x) is dierent from the case of linear regression,even though the form of the cost function is the same mean-squared loss.Throughout the notes, we use the words \\loss\" and \\cost\" interchangeably.Binary classication. Next we dene the model and loss function forbinary classication. Suppose the inputs x2Rd. Let h:Rd!Rbe aparameterized model (the analog of >xin logistic linear regression). Wecall the output h(x)2Rthe logit. Analogous to Section 2.1, we use thelogistic function g() to turn the logit h(x) to a probability h(x)2[0;1]:h(x) =g(h(x)) = 1=(1 + exp(h(x)): (7.3)We model the conditional distribution of ygivenxandbyP(y= 1jx;) =h(x)P(y= 0jx;) = 1h(x)Following the same derivation in Section 2.1 and using the derivation inRemark 2.1.1, the negative likelihood loss function is equal to:J(i)() =logp(y(i)jx(i);) =`logistic (h(x(i));y(i)) (7.4)As done in equation (7.2), the total loss function is also dened as the averageof the loss function over individual training examples, J() =1nPni=1J(i)():"
        ],
        "formulas": [
            "1nnXi=1J(i)();",
            "constant1=nin",
            "1=(1",
            "y= 1jx;)",
            "y= 0jx;)",
            "1nPni=1J(i)():"
        ],
        "explanations": []
    },
    "page_83": {
        "content_preview": "82Multi-class classication. Following Section 2.3, we consider a classica-tion problem where the response variable ycan take on any one of kvalues,i.e.y2f1;2;:::;kg. Let h:Rd!Rkbe a parameterized model. Wecall the outputs h(x)2Rkthe logits. Each logit corresponds to the predic-tion for one of the kclasses. Analogous to Section 2.3, we use the softmaxfunction to turn the logits h(x) into a probability vector with non-negativeentries that sum up to 1:P(y=jjx;) =exp(h(x)j)Pks=1exp(h(x)s); (7.5)wher...",
        "python_code": [
            "82Multi-class classication. Following Section 2.3, we consider a classica-tion problem where the response variable ycan take on any one of kvalues,i.e.y2f1;2;:::;kg. Let h:Rd!Rkbe a parameterized model. Wecall the outputs h(x)2Rkthe logits. Each logit corresponds to the predic-tion for one of the kclasses. Analogous to Section 2.3, we use the softmaxfunction to turn the logits h(x) into a probability vector with non-negativeentries that sum up to 1:P(y=jjx;) =exp(h(x)j)Pks=1exp(h(x)s); (7.5)where h(x)sdenotes the s-th coordinate of h(x).Similarly to Section 2.3, the loss function for a single training example(x(i);y(i)) is its negative log-likelihood:J(i)() =logp(y(i)jx(i);) =log exp(h(x(i))y(i))Pks=1exp(h(x(i))s)!: (7.6)Using the notations of Section 2.3, we can simply write in an abstract way:J(i)() =`ce(h(x(i));y(i)): (7.7)The loss function is also dened as the average of the loss function of indi-vidual training examples, J() =1nPni=1J(i)():We also note that the approach above can also be generated to any con-ditional probabilistic model where we have an exponential distribution fory, Exponential-family( y;), where=h(x) is a parameterized nonlinearfunction of x. However, the most widely used situations are the three casesdiscussed above.Optimizers (SGD). Commonly, people use gradient descent (GD), stochas-tic gradient (SGD), or their variants to optimize the loss function J(). GD'supdate rule can be written as2:=rJ() (7.8)where > 0 is often referred to as the learning rate or step size. Next, weintroduce a version of the SGD (Algorithm 1), which is lightly dierent fromthat in the rst lecture notes.2Recall that, as dened in the previous lecture notes, we use the notation \\ a:=b\" todenote an operation (in a computer program) in which we setthe value of a variable atobe equal to the value of b. In other words, this operation overwrites awith the value ofb. In contrast, we will write \\ a=b\" when we are asserting a statement of fact, that thevalue ofais equal to the value of b."
        ],
        "formulas": [
            "y=jjx;)",
            "Pks=1exp(h(x)s);",
            "Pks=1exp(h(x(i))s)!:",
            "1nPni=1J(i)():We",
            "where=h(x)",
            "a=b\""
        ],
        "explanations": []
    },
    "page_84": {
        "content_preview": "83Algorithm 1 Stochastic Gradient Descent1:Hyperparameter: learning rate , number of total iteration niter.2:Initializerandomly.3:fori= 1 toniterdo4: Samplejuniformly fromf1;:::;ng, and update by:=rJ(j)() (7.9)Oftentimes computing the gradient of Bexamples simultaneously for theparameter can be faster than computing Bgradients separately due tohardware parallelization. Therefore, a mini-batch version of SGD is mostcommonly used in deep learning, as shown in Algorithm 2. There are alsoother varia...",
        "python_code": [
            "83Algorithm 1 Stochastic Gradient Descent1:Hyperparameter: learning rate , number of total iteration niter.2:Initializerandomly.3:fori= 1 toniterdo4: Samplejuniformly fromf1;:::;ng, and update by:=rJ(j)() (7.9)Oftentimes computing the gradient of Bexamples simultaneously for theparameter can be faster than computing Bgradients separately due tohardware parallelization. Therefore, a mini-batch version of SGD is mostcommonly used in deep learning, as shown in Algorithm 2. There are alsoother variants of the SGD or mini-batch SGD with slightly dierent samplingschemes.Algorithm 2 Mini-batch Stochastic Gradient Descent1:Hyperparameters: learning rate , batch size B, # iterations niter.2:Initializerandomly3:fori= 1 toniterdo4: SampleBexamplesj1;:::;jB(without replacement) uniformly fromf1;:::;ng, and update by:=BBXk=1rJ(jk)() (7.10)With these generic algorithms, a typical deep learning model is learnedwith the following steps. 1. Dene a neural network parametrization h(x),which we will introduce in Section 7.2, and 2. write the backpropagationalgorithm to compute the gradient of the loss function J(j)() eciently,which will be covered in Section 7.4, and 3. run SGD or mini-batch SGD (orother gradient-based optimizers) with the loss function J()."
        ],
        "formulas": [
            "fori= 1",
            "fori= 1",
            "BBXk=1rJ(jk)()"
        ],
        "explanations": []
    },
    "page_85": {
        "content_preview": "847.2 Neural networksNeural networks refer to a broad type of non-linear models/parametrizationsh(x) that involve combinations of matrix multiplications and other entry-wise non-linear operations. To have a unied treatment for regression prob-lem and classication problem, here we consider h(x) as the output of theneural network. For regression problem, the nal prediction h(x) =h(x),and for classication problem, h(x) is the logits and the predicted probabilitywill beh(x) = 1=(1+exp(h(x)) (see equ...",
        "python_code": [
            "847.2 Neural networksNeural networks refer to a broad type of non-linear models/parametrizationsh(x) that involve combinations of matrix multiplications and other entry-wise non-linear operations. To have a unied treatment for regression prob-lem and classication problem, here we consider h(x) as the output of theneural network. For regression problem, the nal prediction h(x) =h(x),and for classication problem, h(x) is the logits and the predicted probabilitywill beh(x) = 1=(1+exp(h(x)) (see equation 7.3) for binary classicationorh(x) = softmax( h(x)) for multi-class classication (see equation 7.5).We will start small and slowly build up a neural network, step by step.A Neural Network with a Single Neuron. Recall the housing priceprediction problem from before: given the size of the house, we want topredict the price. We will use it as a running example in this subsection.Previously, we t a straight line to the graph of size vs. housing price.Now, instead of tting a straight line, we wish to prevent negative housingprices by setting the absolute minimum price as zero. This produces a \\kink\"in the graph as shown in Figure 7.1. How do we represent such a functionwith a single kink as h(x) with unknown parameter? (After doing so, wecan invoke the machinery in Section 7.1.)We dene a parameterized function h(x) with input x, parameterized by, which outputs the price of the house y. Formally, h:x!y. Perhapsone of the simplest parametrization would beh(x) = max(wx+b;0);where= (w;b)2R2(7.11)Here h(x) returns a single value: ( wx+b) or zero, whichever is greater. Inthe context of neural networks, the function max ft;0gis called a ReLU (pro-nounced \\ray-lu\"), or rectied linear unit, and often denoted by ReLU( t),maxft;0g.Generally, a one-dimensional non-linear function that maps RtoRsuch asReLU is often referred to as an activation function . The model h(x) is saidto have a single neuron partly because it has a single non-linear activationfunction. (We will discuss more about why a non-linear activation is calledneuron.)When the input x2Rdhas multiple dimensions, a neural network witha single neuron can be written ash(x) = ReLU(w>x+b);wherew2Rd,b2R, and= (w;b) (7.12)"
        ],
        "formulas": [
            "1=(1+exp(h(x))",
            "where= (w;b)2R2(7.11)Here",
            "and= (w;b)"
        ],
        "explanations": []
    },
    "page_86": {
        "content_preview": "85500 1000 1500 2000 2500 3000 3500 4000 4500 500001002003004005006007008009001000housing prices square feet price (in $1000) Figure 7.1: Housing prices with a \\kink\" in the graph.The termbis often referred to as the \\bias\", and the vector wis referredto as the weight vector. Such a neural network has 1 layer. (We will denewhat multiple layers mean in the sequel.)Stacking Neurons. A more complex neural network may take the singleneuron described above and \\stack\" them together such that one neur...",
        "python_code": [],
        "formulas": [],
        "explanations": [
            "85500 1000 1500 2000 2500 3000 3500 4000 4500 500001002003004005006007008009001000housing prices square feet price (in $1000) Figure 7",
            "1: Housing prices with a \\kink\" in the graph",
            "The termbis often referred to as the \\bias\", and the vector wis referredto as the weight vector",
            "Such a neural network has 1 layer",
            "(We will denewhat multiple layers mean in the sequel",
            "A more complex neural network may take the singleneuron described above and \\stack\" them together such that one neuronpasses its output as input into the next neuron, resulting in a more complexfunction",
            "Let us now deepen the housing prediction example",
            "In addition to the sizeof the house, suppose that you know the number of bedrooms, the zip codeand the wealth of the neighborhood",
            "Building neural networks is analogousto Lego bricks: you take individual bricks and stack them together to buildcomplex structures",
            "The same applies to neural networks: we take individualneurons and stack them together to create complex neural networks",
            "Given these features (size, number of bedrooms, zip code, and wealth),we might then decide that the price of the house depends on the maximumfamily size it can accommodate",
            "Suppose the family size is a function of thesize of the house and number of bedrooms (see Figure 7",
            "The zip codemay provide additional information such as how walkable the neighborhoodis (i",
            ", can you walk to the grocery store or do you need to drive everywhere)",
            "Combining the zip code with the wealth of the neighborhood may predictthe quality of the local elementary school",
            "Given these three derived features(family size, walkable, school quality), we may conclude that the price of the"
        ]
    },
    "page_87": {
        "content_preview": "86home ultimately depends on these three features.Family Size School Quality Walkable Size # Bedrooms Zip Code Wealth Price yFigure 7.2: Diagram of a small neural network for predicting housing prices.Formally, the input to a neural network is a set of input featuresx1;x2;x3;x4. We denote the intermediate variables for \\family size\", \\walk-able\", and \\school quality\" by a1;a2;a3(theseai's are often referred to as\\hidden units\" or \\hidden neurons\"). We represent each of the ai's as a neu-ral netw...",
        "python_code": [
            "86home ultimately depends on these three features.Family Size School Quality Walkable Size # Bedrooms Zip Code Wealth Price yFigure 7.2: Diagram of a small neural network for predicting housing prices.Formally, the input to a neural network is a set of input featuresx1;x2;x3;x4. We denote the intermediate variables for \\family size\", \\walk-able\", and \\school quality\" by a1;a2;a3(theseai's are often referred to as\\hidden units\" or \\hidden neurons\"). We represent each of the ai's as a neu-ral network with a single neuron with a subset of x1;:::;x 4as inputs. Thenas in Figure 7.1, we will have the parameterization:a1= ReLU(1x1+2x2+3)a2= ReLU(4x3+5)a3= ReLU(6x3+7x4+8)where (1;;8) are parameters. Now we represent the nal output h(x)as another linear function with a1;a2;a3as inputs, and we get3h(x) =9a1+10a2+11a3+12 (7.13)wherecontains all the parameters ( 1;;12).Now we represent the output as a quite complex function of xwith pa-rameters. Then you can use this parametrization hwith the machinery ofSection 7.1 to learn the parameters .Inspiration from Biological Neural Networks. As the name suggests,articial neural networks were inspired by biological neural networks. Thehidden units a1;:::;amcorrespond to the neurons in a biological neural net-work, and the parameters i's correspond to the synapses. However, it'sunclear how similar the modern deep articial neural networks are to the bi-ological ones. For example, perhaps not many neuroscientists think biological3Typically, for multi-layer neural network, at the end, near the output, we don't applyReLU, especially when the output is not necessarily a positive number."
        ],
        "formulas": [
            "a1= ReLU(1x1+2x2+3)a2=",
            "a3= ReLU(6x3+7x4+8)where"
        ],
        "explanations": []
    },
    "page_88": {
        "content_preview": "87neural networks could have 1000 layers, while some modern articial neuralnetworks do (we will elaborate more on the notion of layers.) Moreover, it'san open question whether human brains update their neural networks in away similar to the way that computer scientists learn articial neural net-works (using backpropagation, which we will introduce in the next section.).Two-layer Fully-Connected Neural Networks. We constructed theneural network in equation (7.13) using a signicant amount of prior...",
        "python_code": [
            "87neural networks could have 1000 layers, while some modern articial neuralnetworks do (we will elaborate more on the notion of layers.) Moreover, it'san open question whether human brains update their neural networks in away similar to the way that computer scientists learn articial neural net-works (using backpropagation, which we will introduce in the next section.).Two-layer Fully-Connected Neural Networks. We constructed theneural network in equation (7.13) using a signicant amount of prior knowl-edge/belief about how the \\family size\", \\walkable\", and \\school quality\" aredetermined by the inputs. We implicitly assumed that we know the familysize is an important quantity to look at and that it can be determined byonly the \\size\" and \\# bedrooms\". Such a prior knowledge might not beavailable for other applications. It would be more exible and general to havea generic parameterization. A simple way would be to write the intermediatevariablea1as a function of all x1;:::;x 4:a1= ReLU(w>1x+b1);wherew12R4andb12R (7.14)a2= ReLU(w>2x+b2);wherew22R4andb22Ra3= ReLU(w>3x+b3);wherew32R4andb32RWe still dene h(x) using equation (7.13) with a1;a2;a3being dened asabove. Thus we have a so-called fully-connected neural network becauseall the intermediate variables ai's depend on all the inputs xi's.For full generality, a two-layer fully-connected neural network with mhidden units and ddimensional input x2Rdis dened as8j2[1;:::;m ]; zj=w[1]j>x+b[1]jwherew[1]j2Rd;b[1]j2R (7.15)aj= ReLU(zj);a= [a1;:::;am]>2Rmh(x) =w[2]>a+b[2]wherew[2]2Rm;b[2]2R; (7.16)Note that by default the vectors in Rdare viewed as column vectors, andin particular ais a column vector with components a1;a2;:::;am. The indices[1]and[2]are used to distinguish two sets of parameters: the w[1]j's (each ofwhich is a vector in Rd) andw[2](which is a vector in Rm). We will havemore of these later.Vectorization. Before we introduce neural networks with more layers andmore complex structures, we will simplify the expressions for neural networks"
        ],
        "formulas": [
            "a1= ReLU(w>1x+b1);wherew12R4andb12R",
            "a2= ReLU(w>2x+b2);wherew22R4andb22Ra3=",
            "zj=w[1]j>x+b[1]jwherew[1]j2Rd;b[1]j2R",
            "aj= ReLU(zj);a="
        ],
        "explanations": []
    },
    "page_89": {
        "content_preview": "88with more matrix and vector notations. Another important motivation ofvectorization is the speed perspective in the implementation. In order toimplement a neural network eciently, one must be careful when using forloops. The most natural way to implement equation (7.15) in code is perhapsto use a for loop. In practice, the dimensionalities of the inputs and hiddenunits are high. As a result, code will run very slowly if you use for loops.Leveraging the parallelism in GPUs is/was crucial for th...",
        "python_code": [
            "88with more matrix and vector notations. Another important motivation ofvectorization is the speed perspective in the implementation. In order toimplement a neural network eciently, one must be careful when using forloops. The most natural way to implement equation (7.15) in code is perhapsto use a for loop. In practice, the dimensionalities of the inputs and hiddenunits are high. As a result, code will run very slowly if you use for loops.Leveraging the parallelism in GPUs is/was crucial for the progress of deeplearning.This gave rise to vectorization . Instead of using for loops, vectorizationtakes advantage of matrix algebra and highly optimized numerical linearalgebra packages (e.g., BLAS) to make neural network computations runquickly. Before the deep learning era, a for loop may have been sucienton smaller datasets, but modern deep networks and state-of-the-art datasetswill be infeasible to run with for loops.We vectorize the two-layer fully-connected neural network as below. Wedene a weight matrix W[1]inRmdas the concatenation of all the vectorsw[1]j's in the following way:W[1]=266664|w[1]1>||w[1]2>|...|w[1]m>|3777752Rmd(7.17)Now by the denition of matrix vector multiplication, we can write z=[z1;:::;zm]>2Rmas26664z1......zm37775|{z}z2Rm1=266664|w[1]1>||w[1]2>|...|w[1]m>|377775|{z}W[1]2Rmd26664x1x2...xd37775|{z}x2Rd1+26664b[1]1b[1]2...b[1]m37775|{z}b[1]2Rm1(7.18)Or succinctly,z=W[1]x+b[1](7.19)We remark again that a vector in Rdin this notes, following the conventionspreviously established, is automatically viewed as a column vector, and can"
        ],
        "formulas": [
            "z=[z1;:::;zm]>2Rmas26664z1......zm37775|{z}z2Rm1=266664|w[1]1>||w[1]2>|...|w[1]m>|377775|{z}W[1]2Rmd26664x1x2...xd37775|{z}x2Rd1+26664b[1]1b[1]2...b[1]m37775|{z}b[1]2Rm1(7.18)Or",
            "z=W[1]x+b[1](7.19)We"
        ],
        "explanations": []
    },
    "page_90": {
        "content_preview": "89also be viewed as a d1 dimensional matrix. (Note that this is dierentfrom numpy where a vector is viewed as a row vector in broadcasting.)Computing the activations a2Rmfromz2Rminvolves an element-wise non-linear application of the ReLU function, which can be computed inparallel eciently. Overloading ReLU for element-wise application of ReLU(meaning, for a vector t2Rd, ReLU(t) is a vector such that ReLU( t)i=ReLU(ti)), we havea= ReLU(z) (7.20)DeneW[2]= [w[2]>]2R1msimilarly. Then, the model in e...",
        "python_code": [
            "89also be viewed as a d1 dimensional matrix. (Note that this is dierentfrom numpy where a vector is viewed as a row vector in broadcasting.)Computing the activations a2Rmfromz2Rminvolves an element-wise non-linear application of the ReLU function, which can be computed inparallel eciently. Overloading ReLU for element-wise application of ReLU(meaning, for a vector t2Rd, ReLU(t) is a vector such that ReLU( t)i=ReLU(ti)), we havea= ReLU(z) (7.20)DeneW[2]= [w[2]>]2R1msimilarly. Then, the model in equa-tion (7.16) can be summarized asa= ReLU(W[1]x+b[1])h(x) =W[2]a+b[2](7.21)Hereconsists of W[1];W[2](often referred to as the weight matrices) andb[1];b[2](referred to as the biases). The collection of W[1];b[1]is referred to asthe rst layer, and W[2];b[2]the second layer. The activation ais referred to asthe hidden layer. A two-layer neural network is also called one-hidden-layerneural network.Multi-layer fully-connected neural networks. With this succinct no-tations, we can stack more layers to get a deeper fully-connected neu-ral network. Let rbe the number of layers (weight matrices). LetW[1];:::;W[r];b[1];:::;b[r]be the weight matrices and biases of all the layers.Then a multi-layer neural network can be written asa[1]= ReLU(W[1]x+b[1])a[2]= ReLU(W[2]a[1]+b[2])a[r1]= ReLU(W[r1]a[r2]+b[r1])h(x) =W[r]a[r1]+b[r](7.22)We note that the weight matrices and biases need to have compatibledimensions for the equations above to make sense. If a[k]has dimension mk,then the weight matrix W[k]should be of dimension mkmk1, and the biasb[k]2Rmk. Moreover, W[1]2Rm1dandW[r]2R1mr1."
        ],
        "formulas": [
            "i=ReLU(ti)),",
            "havea= ReLU(z)",
            "asa= ReLU(W[1]x+b[1])h(x)"
        ],
        "explanations": []
    },
    "page_91": {
        "content_preview": "90The total number of neurons in the network is m1++mr, and thetotal number of parameters in this network is ( d+ 1)m1+ (m1+ 1)m2++(mr1+ 1)mr.Sometimes for notational consistency we also write a[0]=x, anda[r]=h(x). Then we have simple recursion thata[k]= ReLU(W[k]a[k1]+b[k]);8k= 1;:::;r1 (7.23)Note that this would have be true for k=rif there were an additionalReLU in equation (7.22), but often people like to make the last layer linear(aka without a ReLU) so that negative outputs are possible an...",
        "python_code": [
            "90The total number of neurons in the network is m1++mr, and thetotal number of parameters in this network is ( d+ 1)m1+ (m1+ 1)m2++(mr1+ 1)mr.Sometimes for notational consistency we also write a[0]=x, anda[r]=h(x). Then we have simple recursion thata[k]= ReLU(W[k]a[k1]+b[k]);8k= 1;:::;r1 (7.23)Note that this would have be true for k=rif there were an additionalReLU in equation (7.22), but often people like to make the last layer linear(aka without a ReLU) so that negative outputs are possible and it's easierto interpret the last layer as a linear model. (More on the interpretability atthe \\connection to kernel method\" paragraph of this section.)Other activation functions. The activation function ReLU can be re-placed by many other non-linear function () that maps RtoRsuch as(z) =11 +ez(sigmoid) (7.24)(z) =ezezez+ez(tanh) (7.25)(z) = maxfz;zg;2(0;1) (leaky ReLU) (7.26)(z) =z21 + erf(zp2)(GELU) (7.27)(z) =1log(1 + exp( z)); > 0 (Softplus) (7.28)The activation functions are plotted in Figure 7.3. Sigmoid and tanh areless and less used these days partly because their are bounded from both sidesand the gradient of them vanishes as zgoes to both positive and negativeinnity (whereas all the other activation functions still have gradients as theinput goes to positive innity.) Softplus is not used very often either inpractice and can be viewed as a smoothing of the ReLU so that it has aproper second order derivative. GELU and leaky ReLU are both variants ofReLU but they have some non-zero gradient even when the input is negative.GELU (or its slight variant) is used in NLP models such as BERT and GPT(which we will discuss in Chapter 14.)Why do we not use the identity function for (z)?That is, whynot use(z) =z? Assume for sake of argument that b[1]andb[2]are zeros."
        ],
        "formulas": [
            "8k= 1;:::;r1",
            "k=rif"
        ],
        "explanations": []
    },
    "page_92": {
        "content_preview": "91Figure 7.3: Activation functions in deep learning.Suppose(z) =z, then for two-layer neural network, we have thath(x) =W[2]a[1](7.29)=W[2](z[1]) by denition (7.30)=W[2]z[1]since(z) =z (7.31)=W[2]W[1]x from Equation (7.18) (7.32)=~Wx where ~W=W[2]W[1](7.33)Notice how W[2]W[1]collapsed into ~W.This is because applying a linear function to another linear function willresult in a linear function over the original input (i.e., you can construct a ~Wsuch that ~Wx=W[2]W[1]x). This loses much of the re...",
        "python_code": [
            "91Figure 7.3: Activation functions in deep learning.Suppose(z) =z, then for two-layer neural network, we have thath(x) =W[2]a[1](7.29)=W[2](z[1]) by denition (7.30)=W[2]z[1]since(z) =z (7.31)=W[2]W[1]x from Equation (7.18) (7.32)=~Wx where ~W=W[2]W[1](7.33)Notice how W[2]W[1]collapsed into ~W.This is because applying a linear function to another linear function willresult in a linear function over the original input (i.e., you can construct a ~Wsuch that ~Wx=W[2]W[1]x). This loses much of the representational powerof the neural network as often times the output we are trying to predicthas a non-linear relationship with the inputs. Without non-linear activationfunctions, the neural network will simply perform linear regression.Connection to the Kernel Method. In the previous lectures, we coveredthe concept of feature maps. Recall that the main motivation for featuremaps is to represent functions that are non-linear in the input xby>(x),whereare the parameters and (x), the feature map, is a handcraftedfunction non-linear in the raw input x. The performance of the learningalgorithms can signicantly depends on the choice of the feature map (x).Oftentimes people use domain knowledge to design the feature map (x) that"
        ],
        "formulas": [
            "W=W[2]W[1](7.33)Notice",
            "Wx=W[2]W[1]x)."
        ],
        "explanations": []
    },
    "page_93": {
        "content_preview": "92suits the particular applications. The process of choosing the feature mapsis often referred to as feature engineering .We can view deep learning as a way to automatically learn the rightfeature map (sometimes also referred to as \\the representation\") as follows.Suppose we denote by the collection of the parameters in a fully-connectedneural networks (equation (7.22)) except those in the last layer. Then wecan abstract right a[r1]as a function of the input xand the parameters in:a[r1]=(x). Now...",
        "python_code": [
            "92suits the particular applications. The process of choosing the feature mapsis often referred to as feature engineering .We can view deep learning as a way to automatically learn the rightfeature map (sometimes also referred to as \\the representation\") as follows.Suppose we denote by the collection of the parameters in a fully-connectedneural networks (equation (7.22)) except those in the last layer. Then wecan abstract right a[r1]as a function of the input xand the parameters in:a[r1]=(x). Now we can write the model ash(x) =W[r](x) +b[r](7.34)Whenis xed, then () can viewed as a feature map, and therefore h(x)is just a linear model over the features (x). However, we will train theneural networks, both the parameters in and the parameters W[r];b[r]areoptimized, and therefore we are not learning a linear model in the featurespace, but also learning a good feature map () itself so that it's possi-ble to predict accurately with a linear model on top of the feature map.Therefore, deep learning tends to depend less on the domain knowledge ofthe particular applications and requires often less feature engineering. Thepenultimate layer a[r]is often (informally) referred to as the learned featuresor representations in the context of deep learning.In the example of house price prediction, a fully-connected neural networkdoes not need us to specify the intermediate quantity such \\family size\", andmay automatically discover some useful features in the last penultimate layer(the activation a[r1]), and use them to linearly predict the housing price.Often the feature map / representation obtained from one datasets (that is,the function () can be also useful for other datasets, which indicates theycontain essential information about the data. However, oftentimes, the neuralnetwork will discover complex features which are very useful for predictingthe output but may be dicult for a human to understand or interpret. Thisis why some people refer to neural networks as a black box , as it can bedicult to understand the features it has discovered.7.3 Modules in Modern Neural NetworksThe multi-layer neural network introduced in equation (7.22) of Section 7.2is often called multi-layer perceptron (MLP) these days. Modern neural net-works used in practice are often much more complex and consist of multiplebuilding blocks or multiple layers of building blocks. In this section, we will"
        ],
        "formulas": [],
        "explanations": []
    },
    "page_94": {
        "content_preview": "93introduce some of the other building blocks and discuss possible ways tocombine them.First, each matrix multiplication can be viewed as a building block. Con-sider a matrix multiplication operation with parameters ( W;b) whereWisthe weight matrix and bis the bias vector, operating on an input z,MMW;b(z) =Wz+b: (7.35)Note that we implicitly assume all the dimensions are chosen to be compat-ible. We will also drop the subscripts under MM when they are clear in thecontext or just for convenience ...",
        "python_code": [
            "93introduce some of the other building blocks and discuss possible ways tocombine them.First, each matrix multiplication can be viewed as a building block. Con-sider a matrix multiplication operation with parameters ( W;b) whereWisthe weight matrix and bis the bias vector, operating on an input z,MMW;b(z) =Wz+b: (7.35)Note that we implicitly assume all the dimensions are chosen to be compat-ible. We will also drop the subscripts under MM when they are clear in thecontext or just for convenience when they are not essential to the discussion.Then, the MLP can be written as as a composition of multiple matrixmultiplication modules and nonlinear activation modules (which can also beviewed as a building block):MLP(x) = MMW[r];b[r]((MMW[r1];b[r1]((MMW[1];b[1](x)))):(7.36)Alternatively, when we drop the subscripts that indicate the parameters forconvenience, we can writeMLP(x) = MM((MM(MM(x)))): (7.37)Note that in this lecture notes, by default, all the modules have dierentsets of parameters, and the dimensions of the parameters are chosen suchthat the composition is meaningful.Larger modules can be dened via smaller modules as well, e.g., oneactivation layer and a matrix multiplication layer MM are often combinedand called a \\layer\" in many papers. People often draw the architecturewith the basic modules in a gure by indicating the dependency betweenthese modules. E.g., see an illustration of an MLP in Figure 7.4, Left.Residual connections. One of the very inuential neural network archi-tecture for vision application is ResNet, which uses the residual connectionsthat are essentially used in almost all large-scale deep learning architecturesthese days. Using our notation above, a very much simplied residual blockcan be dened asRes(z) =z+(MM((MM(z)))): (7.38)A much simplied ResNet is a composition of many residual blocks followedby a matrix multiplication,ResNet-S(x) = MM(Res(Res( Res(x)))): (7.39)"
        ],
        "formulas": [],
        "explanations": []
    },
    "page_95": {
        "content_preview": "94𝑥Layer 𝑟−1Layer 𝑖...Layer 1MLP(𝑥)...Layer𝑖MM![\"],#[\"]𝜎MM![$],#[$]𝑥ResRes...ResResNet-S(𝑥)...ResMM𝜎MM𝜎Figure 7.4: Illustrative Figures for Architecture. Left: An MLP with rlayers. Right : A residual network.We also draw the dependency of these modules in Figure 7.4, Right.We note that the ResNet-S is still not the same as the ResNet architec-ture introduced in the seminal paper [He et al., 2016] because ResNet usesconvolution layers instead of vanilla matrix multiplication, and adds batchnormal...",
        "python_code": [
            "94𝑥Layer 𝑟−1Layer 𝑖...Layer 1MLP(𝑥)...Layer𝑖MM![\"],#[\"]𝜎MM![$],#[$]𝑥ResRes...ResResNet-S(𝑥)...ResMM𝜎MM𝜎Figure 7.4: Illustrative Figures for Architecture. Left: An MLP with rlayers. Right : A residual network.We also draw the dependency of these modules in Figure 7.4, Right.We note that the ResNet-S is still not the same as the ResNet architec-ture introduced in the seminal paper [He et al., 2016] because ResNet usesconvolution layers instead of vanilla matrix multiplication, and adds batchnormalization between convolutions and activations. We will introduce con-volutional layers and some variants of batch normalization below. ResNet-Sand layer normalization are part of the Transformer architecture that arewidely used in modern large language models.Layer normalization. Layer normalization, denoted by LN in this text,is a module that maps a vector z2Rmto a more normalized vector LN( z)2Rm. It is oftentimes used after the nonlinear activations.We rst dene a sub-module of the layer normalization, denoted by LN-S.LN-S(z) =26664z1^^z2^^...zm^^37775; (7.40)where ^=Pmi=1zimis the empirical mean of the vector zand ^=qPmi=1(zi^2)mis the empirical standard deviation of the entries of z.4Intuitively, LN-S( z)is a vector that is normalized to having empirical mean zero and empiricalstandard deviation 1.4Note that we divide by minstead ofm1 in the empirical standard deviation herebecause we are interested in making the output of LN-S( z) have sum of squares equal to1 (as opposed to estimating the standard deviation in statistics.)"
        ],
        "formulas": [
            "Pmi=1zimis",
            "qPmi=1(zi^2)mis"
        ],
        "explanations": []
    },
    "page_96": {
        "content_preview": "95Oftentimes zero mean and standard deviation 1 is not the most desirednormalization scheme, and thus layernorm introduces to parameters learnablescalarsandas the desired mean and standard deviation, and use an anetransformation to turn the output of LN-S( z) into a vector with mean andstandard deviation .LN(z) =+LN-S(z) =26664+z1^^+z2^^...+zm^^37775: (7.41)Here the rst occurrence of should be technically interpreted as a vectorwith all the entries being . in We also note that ^ and ^are also fu...",
        "python_code": [
            "95Oftentimes zero mean and standard deviation 1 is not the most desirednormalization scheme, and thus layernorm introduces to parameters learnablescalarsandas the desired mean and standard deviation, and use an anetransformation to turn the output of LN-S( z) into a vector with mean andstandard deviation .LN(z) =+LN-S(z) =26664+z1^^+z2^^...+zm^^37775: (7.41)Here the rst occurrence of should be technically interpreted as a vectorwith all the entries being . in We also note that ^ and ^are also functionsofzand shouldn't be treated as constants when computing the derivatives oflayernorm. Moreover, andare learnable parameters and thus layernormis a parameterized module (as opposed to the activation layer which doesn'thave any parameters.)Scaling-invariant property. One important property of layer normalizationis that it will make the model invariant to scaling of the parameters in thefollowing sense. Suppose we consider composing LN with MM W;band geta subnetwork LN(MM W;b(z)). Then, we have that the output of this sub-network does not change when the parameter in MM W;bis scaled:LN(MM W;b (z)) = LN(MM W;b(z));8>0: (7.42)To see this, we rst know that LN-S( ) is scale-invariantLN-S(z) =26664z1^^z2^^...zm^^37775=26664z1^^z2^^...zm^^37775= LN-S(z): (7.43)Then we haveLN(MM W;b (z)) =+LN-S(MM W;b (z)) (7.44)=+LN-S(MMW;b(z)) (7.45)=+LN-S(MM W;b(z)) (7.46)= LN(MM W;b(z)): (7.47)Due to this property, most of the modern DL architectures for large-scalecomputer vision and language applications have the following scale-invariant"
        ],
        "formulas": [
            "37775=26664z1^^z2^^...zm^^37775="
        ],
        "explanations": []
    },
    "page_97": {
        "content_preview": "96property w.r.t all the weights that are not at the last layer. Suppose thenetworkfhas last layer' weights Wlast, and all the rest of the weights aredenote byW. Then, we have fWlast;W(x) =fWlast;W(x) for all>0. Here,the last layers weights are special because there are typically no layernormor batchnorm after the last layer's weights.Other normalization layers. There are several other normalization layers thataim to normalize the intermediate layers of the neural networks to a morexed and contr...",
        "python_code": [
            "96property w.r.t all the weights that are not at the last layer. Suppose thenetworkfhas last layer' weights Wlast, and all the rest of the weights aredenote byW. Then, we have fWlast;W(x) =fWlast;W(x) for all>0. Here,the last layers weights are special because there are typically no layernormor batchnorm after the last layer's weights.Other normalization layers. There are several other normalization layers thataim to normalize the intermediate layers of the neural networks to a morexed and controllable scaling, such as batch-normalization [ ?], and groupnormalization [ ?]. Batch normalization and group normalization are moreoften used in computer vision applications whereas layer norm is used moreoften in language applications.Convolutional Layers. Convolutional Neural Networks are neural net-works that consist of convolution layers (and many other modules), and areparticularly useful for computer vision applications. For the simplicity ofexposition, we focus on 1-D convolution in this text and only briey mention2-D convolution informally at the end of this subsection. (2-D convolutionis more suitable for images which have two dimensions. 1-D convolution isalso used in natural language processing.)We start by introducing a simplied version of the 1-D convolution layer,denoted by Conv1D-S( ) which is a type of matrix multiplication layer witha special structure. The parameters of Conv1D-S are a lter vector w2Rkwherekis called the lter size (oftentimes km), and a bias scalar b.Oftentimes the lter is also called a kernel (but it does not have much to dowith the kernel in kernel method.) For simplicity, we assume k= 2`+ 1 isan odd number. We rst pad zeros to the input vector zin the sense that weletz1`=z1`+1=::=z0= 0 andzm+1=zm+2=::=zm+`= 0, and treatzas an (m+ 2`)-dimension vector. Conv1D-S outputs a vector of dimensionRmwhere each output dimension is a linear combination of subsets of zj'swith coecients from w,Conv1D-S(z)i=w1zi`+w2zi`+1++w2`+1zi+`=2`+1Xj=1wjzi`+(j1):(7.48)Therefore, one can view Conv1D-S as a matrix multiplication with shared"
        ],
        "formulas": [
            "k= 2`+",
            "1=::=z0=",
            "1=zm+2=::=zm+`=",
            "i=w1zi`+w2zi`+1++w2`+1zi+`=2`+1Xj=1wjzi`+(j1):(7.48)Therefore,"
        ],
        "explanations": []
    },
    "page_98": {
        "content_preview": "97parameters: Conv1D-S( z) =Qz, whereQ=2666666666666666666664w`+1w2`+1 0 0 0w`w2`w2`+1 0 0...w1w`+1 w2`+1 0 00w1 w2`w2`+1 0 0......0 0w1 w2`+1...0 0w1w`+13777777777777777777775:(7.49)Note thatQi;j=Qi1;j1for alli;j2f2;:::;mg, and thus convoluation is amatrix multiplication with parameter sharing. We also note that computingthe convolution only takes O(km) times but computing a generic matrixmultiplication takes O(m2) time. Convolution has kparameters but genericmatrix multiplication will have m2p...",
        "python_code": [
            "97parameters: Conv1D-S( z) =Qz, whereQ=2666666666666666666664w`+1w2`+1 0 0 0w`w2`w2`+1 0 0...w1w`+1 w2`+1 0 00w1 w2`w2`+1 0 0......0 0w1 w2`+1...0 0w1w`+13777777777777777777775:(7.49)Note thatQi;j=Qi1;j1for alli;j2f2;:::;mg, and thus convoluation is amatrix multiplication with parameter sharing. We also note that computingthe convolution only takes O(km) times but computing a generic matrixmultiplication takes O(m2) time. Convolution has kparameters but genericmatrix multiplication will have m2parameters. Thus convolution is supposedto be much more ecient than a generic matrix multiplication (as long asthe additional structure imposed does not hurt the exibility of the modelto t the data).We also note that in practice there are many variants of the convolutionallayers that we dene here, e.g., there are other ways to pad zeros or sometimesthe dimension of the output of the convolutional layers could be dierent fromthe input. We omit some of this subtleties here for simplicity.The convolutional layers used in practice have also many \\channels\" andthe simplied version above corresponds to the 1-channel version. Formally,Conv1D takes in Cvectorsz1;:::;zC2Rmas inputs, where Cis referredto as the number of channels. In other words, the more general version,denoted by Conv1D, takes in a matrix as input, which is the concatenationofz1;:::;zCand has dimension mC. It can output C0vectors of dimensionm, denoted by Conv1D( z)1;:::; Conv1D(z)C0, whereC0is referred to as theoutput channel, or equivalently a matrix of dimension mC0. Each of theoutput is a sum of the simplied convolutions applied on various channels.8i2[C0];Conv1D(z)i=CXj=1Conv1D-S i;j(zj): (7.50)Note that each Conv1D-S i;jare modules with dierent parameters, andthus the total number of parameters is k(the number of parameters in aConv1D-S)CC0(the number of Conv1D-S i:j's) =kCC0. In contrast, ageneric linear mapping from RmCandRmC0hasm2CC0parameters. The"
        ],
        "formulas": [
            "whereQ=2666666666666666666664w`+1w2`+1",
            "j=Qi1;j1for",
            "i=CXj=1Conv1D-S"
        ],
        "explanations": []
    },
    "page_99": {
        "content_preview": "98parameters can also be represented as a three-dimensional tensor of dimen-sionkCC0.2-D convolution (brief). A 2-D convolution with one channel, denoted byConv2D-S, is analogous to the Conv1D-S, but takes a 2-dimensional inputz2Rmmand applies a lter of size kk, and outputs Conv2D-S( z)2Rmm. The full 2-D convolutional layer, denoted by Conv2D, takes ina sequence of matrices z1;:::;zC2Rmm, or equivalently a 3-D ten-sorz= (z1;:::;zC)2RmmCand outputs a sequence of matrices,Conv2D(z)1;:::; Conv2D(z)...",
        "python_code": [
            "98parameters can also be represented as a three-dimensional tensor of dimen-sionkCC0.2-D convolution (brief). A 2-D convolution with one channel, denoted byConv2D-S, is analogous to the Conv1D-S, but takes a 2-dimensional inputz2Rmmand applies a lter of size kk, and outputs Conv2D-S( z)2Rmm. The full 2-D convolutional layer, denoted by Conv2D, takes ina sequence of matrices z1;:::;zC2Rmm, or equivalently a 3-D ten-sorz= (z1;:::;zC)2RmmCand outputs a sequence of matrices,Conv2D(z)1;:::; Conv2D(z)C02Rmm, which can also be viewed as a 3Dtensor in RmmC0. Each channel of the output is sum of the outcomes ofapplying Conv2D-S layers on all the input channels.8i2[C0];Conv2D(z)i=CXj=1Conv2D-S i;j(zj): (7.51)Because there are CC0number of Conv2D-S modules and each of theConv2D-S module has k2parameters, the total number of parameters isCC0k2. The parameters can also be viewed as a 4D tensor of dimensionCC0kk.7.4 BackpropagationIn this section, we introduce backpropgation or auto-dierentiation, whichcomputes the gradient of the loss rJ() eciently. We will start with aninformal theorem that states that as long as a real-valued function fcan beeciently computed/evaluated by a dierentiable network or circuit, then itsgradient can be eciently computed in a similar time. We will then showhow to do this concretely for neural networks.Because the formality of the general theorem is not the main focus here,we will introduce the terms with informal denitions. By a dierentiablecircuit or a dierentiable network, we mean a composition of a sequence ofdierentiable arithmetic operations (additions, subtraction, multiplication,divisions, etc) and elementary dierentiable functions (ReLU, exp, log, sin,cos, etc.). Let the size of the circuit be the total number of such operationsand elementary functions. We assume that each of the operations and func-tions, and their derivatives or partial derivatives ecan be computed in O(1)time.Theorem 7.4.1: [backpropagation or auto-dierentiation, informally stated]Suppose a dierentiable circuit of size Ncomputes a real-valued function"
        ],
        "formulas": [
            "sorz= (z1;:::;zC)2RmmCand",
            "i=CXj=1Conv2D-S"
        ],
        "explanations": []
    },
    "page_100": {
        "content_preview": "99f:R`!R. Then, the gradient rfcan be computed in time O(N), by acircuit of size O(N).5We note that the loss function J(j)() forj-th example can be indeedcomputed by a sequence of operations and functions involving additions,subtraction, multiplications, and non-linear activations. Thus the theoremsuggests that we should be able to compute the rJ(j)() in a similar timeto that for computing J(j)() itself. This does not only apply to the fully-connected neural network introduced in the Section 7.2...",
        "python_code": [],
        "formulas": [],
        "explanations": [
            "Then, the gradient rfcan be computed in time O(N), by acircuit of size O(N)",
            "5We note that the loss function J(j)() forj-th example can be indeedcomputed by a sequence of operations and functions involving additions,subtraction, multiplications, and non-linear activations",
            "Thus the theoremsuggests that we should be able to compute the rJ(j)() in a similar timeto that for computing J(j)() itself",
            "This does not only apply to the fully-connected neural network introduced in the Section 7",
            "2, but also many othertypes of neural networks that uses more advance modules",
            "We remark that auto-dierentiation or backpropagation is already imple-mented in all the deep learning packages such as tensorow and pytorch, andthus in practice, in most of cases a researcher does not need to write theirbackpropagation algorithms",
            "However, understanding it is very helpful forgaining insights into the working of deep learning",
            "Organization of the rest of the section",
            "1, we will start review-ing the basic Chain rule with a new perspective that is particularly usefulfor understanding backpropgation",
            "2 will introduce the generalstrategy for backpropagation",
            "2 will discuss how to compute theso-called backward function for basic modules used in neural networks, andSection 7",
            "4 will put everything together to get a concrete backprop algo-rithm for MLPs",
            "1 Preliminaries on partial derivativesSuppose a scalar variable Jdepend on some variables z(which could be ascalar, matrix, or high-order tensor), we write@J@zas the partial derivativesofJw",
            "We stress that the convention here is that@J@zhas exactly the same dimension as zitself",
            "For example, if z2Rmn, then@J@z2Rmn, and the (i;j)-entry of@J@zis equal to@J@zij",
            "2: When both Jandzare not scalars, the partial derivatives ofJw",
            "tzbecomes either a matrix or tensor and the notation becomes some-what tricky",
            "Besides the mathematical or notational challenges in dealing5We note if the output of the function fdoes not depend on some of the input co-ordinates, then we set by default the gradient w",
            "t that coordinate to zero",
            "Setting tozero does not count towards the total runtime here in our accounting scheme",
            "This is whywhenN`, we can compute the gradient in O(N) time, which might be potentially evenless than`"
        ]
    },
    "page_101": {
        "content_preview": "100with these partial derivatives of multi-variate functions, they are also expen-sive to compute and store, and thus rarely explicitly constructed empirically.The experience of authors of this note is that it's generally more productiveto think only about derivatives of scalar function w.r.t to vector, matrices,or tensors. For example, in this note, we will not deal with derivatives ofmulti-variate functions.Chain rule. We review the chain rule in calculus but with a perspectiveand notions that...",
        "python_code": [
            "100with these partial derivatives of multi-variate functions, they are also expen-sive to compute and store, and thus rarely explicitly constructed empirically.The experience of authors of this note is that it's generally more productiveto think only about derivatives of scalar function w.r.t to vector, matrices,or tensors. For example, in this note, we will not deal with derivatives ofmulti-variate functions.Chain rule. We review the chain rule in calculus but with a perspectiveand notions that are more relevant for auto-dierentiation.Consider a scalar variable Jwhich is obtained by the composition of fandgon some variable z,z2Rmu=g(z)2RnJ=f(u)2R: (7.52)The same derivations below can be easily extend to the cases when zanduare matrices or tensors; but we insist that the nal variable Jis a scalar. (Seealso Remark 7.4.2.) Let u= (u1;:::;un) and letg(z) = (g1(z);;gn(z)):Then, the standard chain rule gives us that8i2f1;:::;mg;@J@zi=nXj=1@J@uj@gj@zi: (7.53)Alternatively, when zanduare both vectors, in a vectorized notation:@J@z=264@g1@z1@gn@z1.........@g1@zm@gn@zm375@J@u: (7.54)In other words, the backward function is always a linear map from@J@uto@J@z, though note that the mapping itself can depend on zin complex ways.The matrix on the RHS of (7.54) is actually the transpose of the Jacobianmatrix of the function g. However, we do not discuss in-depth about Jacobianmatrices to avoid complications. Part of the reason is that when zis a matrix(or tensor), to write an analog of equation (7.54), one has to either atten zinto a vector or introduce additional notations on tensor-matrix product. Inthis sense, equation (7.53) is more convenient and eective to use in all cases.For example, when z2Rrsis a matrix, we can easily rewrite equation (7.53)"
        ],
        "formulas": [
            "z2Rmu=g(z)2RnJ=f(u)2R:",
            "u= (u1;:::;un)",
            "zi=nXj=1@J@uj@gj@zi:",
            "z=264@g1@z1@gn@z1.........@g1@zm@gn@zm375@J@u:"
        ],
        "explanations": []
    },
    "page_102": {
        "content_preview": "101to8i;k;@J@zik=nXj=1@J@uj@gj@zik: (7.55)which will indeed be used in some of the derivations in Section 7.4.3.Key interpretation of the chain rule. We can view the formula above (equa-tion (7.53) or (7.54)) as a way to compute@J@zfrom@J@u. Consider the followingabstract problem. Suppose Jdepends on zviauas dened in equation (7.52).However, suppose the function fis not given or the function fis complex,but we are given the value of@J@u. Then, the formula in equation (7.54) givesus a way to comp...",
        "python_code": [
            "101to8i;k;@J@zik=nXj=1@J@uj@gj@zik: (7.55)which will indeed be used in some of the derivations in Section 7.4.3.Key interpretation of the chain rule. We can view the formula above (equa-tion (7.53) or (7.54)) as a way to compute@J@zfrom@J@u. Consider the followingabstract problem. Suppose Jdepends on zviauas dened in equation (7.52).However, suppose the function fis not given or the function fis complex,but we are given the value of@J@u. Then, the formula in equation (7.54) givesus a way to compute@J@zfrom@J@u.@J@uchain rule, formula (7.54)= = = = = = = = = = = = = = = = = = = = )only requires info about g() andz@J@z: (7.56)Moreover, this formula only involves knowledge about g(more precisely@gj@zi).We will repeatedly use this fact in situations where gis a building blocks ofa complex network f.Empirically, it's often useful to modularized the mapping in (7.53) or(7.54) into a black-box, and mathematically it's also convenient to dene anotation for it.6We useB[g;z] to dene the function that maps@J@uto@J@z,and write@J@z=B[g;z]@J@u: (7.57)We callB[g;z] thebackward function for the module g. Note that when zis xed,B[g;z] is merely a linear map from RntoRm. Using equation (7.53),we have(B[g;z](v))i=mXj=1@gj@zivj: (7.58)Or in vectorized notation, using (7.54), we haveB[g;z](v) =264@g1@z1@gn@z1.........@g1@zm@gn@zm375v: (7.59)6e.g., the function is the .backward() method of the module in pytorch."
        ],
        "formulas": [
            "zik=nXj=1@J@uj@gj@zik:",
            "z=B[g;z]@J@u:",
            "i=mXj=1@gj@zivj:"
        ],
        "explanations": []
    },
    "page_103": {
        "content_preview": "102and thereforeB[g;z] can be viewed as a matrix. However, in reality, zwill bechanging and thus the backward mapping has to be recomputed for dierentz's whilegis often xed. Thus, empirically, the backward function B[g;z](v)is often viewed as a function which takes in z(=the input to g) andv(=avector that is supposed to be the gradient of some variable Jw.r.t to theoutput ofg) as the inputs, and outputs a vector that is supposed to be thegradient of Jw.r.t toz.7.4.2 General strategy of backpropa...",
        "python_code": [
            "102and thereforeB[g;z] can be viewed as a matrix. However, in reality, zwill bechanging and thus the backward mapping has to be recomputed for dierentz's whilegis often xed. Thus, empirically, the backward function B[g;z](v)is often viewed as a function which takes in z(=the input to g) andv(=avector that is supposed to be the gradient of some variable Jw.r.t to theoutput ofg) as the inputs, and outputs a vector that is supposed to be thegradient of Jw.r.t toz.7.4.2 General strategy of backpropagationWe discuss the general strategy of auto-dierentiation in this section to builda high-level understanding. Then, we will instantiate the approach to con-crete neural networks. We take the viewpoint that neural networks are com-plex compositions of small building blocks such as MM, , Conv2D, LN,etc., dened in Section 7.3. Note that the losses (e.g., mean-squared loss, orthe cross-entropy loss) can also be abstractly viewed as additional modules.Thus, we can abstractly write the loss function J(on a single example ( x;y))as a composition of many modules:7J=Mk(Mk1(M1(x))): (7.60)For example, for a binary classication problem with a MLP h(x) (de-ned in equation (7.36) and (7.37)), the loss function has ber written in theform of equation (7.60) with M1= MMW[1];b[1],M2=,M3= MMW[2];b[2],:::, andMk1= MMW[r];b[r]andMk=`logistic .We can see from this example that some modules involve parameters, andother modules might only involve a xed set of operations. For generality,we assume that eachj Miinvolves a set of parameters [i], though[i]couldpossibly be an empty set when Miis a xed operation such as the nonlinearactivations. We will discuss more on the granularity of the modularization,but so far we assume all the modules Mi's are simple enough.We introduce the intermediate variables for the computation in (7.60).7Technically, we should write J=Mk(Mk1(M1(x));y). However, yis treated as aconstant for the purpose of computing the derivatives w.r.t to the parameters, and thuswe can view it as part of Mkfor the sake of simplicity of notations."
        ],
        "formulas": [
            "7J=Mk(Mk1(M1(x))):",
            "M1= MMW[1];b[1],M2=,M3=",
            "andMk1= MMW[r];b[r]andMk=`logistic",
            "J=Mk(Mk1(M1(x));y)."
        ],
        "explanations": []
    },
    "page_104": {
        "content_preview": "103Letu[0]=xu[1]=M1(u[0])u[2]=M2(u[1])...J=u[k]=Mk(u[k1]): (F)Backpropgation consists of two passes, the forward pass and backwardpass. In the forward pass, the algorithm simply computes u[1];:::;u[k]fromi= 1;:::;k , sequentially using the denition in (F), and save all the in-termediate variables u[i]'s in the memory.In the backward pass , we rst compute the derivatives w.r.t to theintermediate variables, that is,@J@u[k];:::;@J@u[1], sequentially in this backwardorder, and then compute the deriv...",
        "python_code": [
            "103Letu[0]=xu[1]=M1(u[0])u[2]=M2(u[1])...J=u[k]=Mk(u[k1]): (F)Backpropgation consists of two passes, the forward pass and backwardpass. In the forward pass, the algorithm simply computes u[1];:::;u[k]fromi= 1;:::;k , sequentially using the denition in (F), and save all the in-termediate variables u[i]'s in the memory.In the backward pass , we rst compute the derivatives w.r.t to theintermediate variables, that is,@J@u[k];:::;@J@u[1], sequentially in this backwardorder, and then compute the derivatives of the parameters@J@[i]from@J@u[i]andu[i1]. These two type of computations can be also interleaved with eachother because@J@[i]only depends on@J@u[i]andu[i1]but not any@J@u[k]withk<i .We rst see why@J@u[i1]can be computed eciently from@J@u[i]andu[i1]by invoking the discussion in Section 7.4.1 on the chain rule. We in-stantiate the discussion by setting u=u[i]andz=u[i1], andf(u) =Mk(Mk1(Mi+1(u[i]))), andg() =Mi(). Note that fis very complexbut we don't need any concrete information about f. Then, the conclusiveequation (7.56) corresponds to@J@u[i]chain rule= = = = = = = = = = = = = = = = = = = = = = = = = = )only requires info about Mi()andu[i1]@J@u[i1]: (7.61)More precisely, we can write, following equation (7.57)@J@u[i1]=B[Mi;u[i1]]@J@u[i]: (B1)Instantiating the chain rule with z=[i]andu=u[i], we also have@J@[i]=B[Mi;[i]]@J@u[i]: (B2)See Figure 7.5 for an illustration of the algorithm."
        ],
        "formulas": [
            "J=u[k]=Mk(u[k1]):",
            "fromi= 1;:::;k",
            "u=u[i]andz=u[i1],",
            "rule= =",
            "z=[i]andu=u[i],"
        ],
        "explanations": []
    },
    "page_105": {
        "content_preview": "104𝑥...𝑀!𝐽...𝑀\"𝑢[!]𝑢[\"%!]𝜕𝐽𝜕𝐽ℬ[𝑀\",𝑢[\"%!]]𝜕𝐽𝜕𝑢!\"#𝑢[&]𝑀&𝑢[&%!]ℬ[𝑀&,𝑢[&%!]]𝜕𝐽𝜕𝑢$\"#𝜕𝐽𝜕𝑢$...𝜕𝐽𝜕𝑢#...Forward passBackward passℬ[𝑀!,𝜃!]𝜕𝐽𝜕𝜃#ℬ[𝑀&,𝜃&]𝜕𝐽𝜕𝜃$ℬ[𝑀\",𝜃\"]𝜕𝐽𝜕𝜃!Figure 7.5: Back-propagation.Remark 7.4.3: [Computational eciency and granularity of the modules]The main underlying purpose of treating a complex network as compositionsof small modules is that small modules tend to have eciently implementablebackward function. In fact, the backward functions of all the atomic modulessuch as addition, mul...",
        "python_code": [],
        "formulas": [],
        "explanations": [
            "𝑀\"𝑢[!]𝑢[\"%!]𝜕𝐽𝜕𝐽ℬ[𝑀\",𝑢[\"%!]]𝜕𝐽𝜕𝑢!\"#𝑢[&]𝑀&𝑢[&%!]ℬ[𝑀&,𝑢[&%!]]𝜕𝐽𝜕𝑢$\"#𝜕𝐽𝜕𝑢$",
            "Forward passBackward passℬ[𝑀!,𝜃!]𝜕𝐽𝜕𝜃#ℬ[𝑀&,𝜃&]𝜕𝐽𝜕𝜃$ℬ[𝑀\",𝜃\"]𝜕𝐽𝜕𝜃!Figure 7",
            "3: [Computational eciency and granularity of the modules]The main underlying purpose of treating a complex network as compositionsof small modules is that small modules tend to have eciently implementablebackward function",
            "In fact, the backward functions of all the atomic modulessuch as addition, multiplication and ReLU can be computed as eciently asthe the evaluation of these modules (up to multiplicative constant factor)",
            "Using this fact, we can prove Theorem 7",
            "1 by viewing neural networks ascompositions of many atomic operations, and invoking the backpropagationdiscussed above",
            "However, in practice, it's oftentimes more convenient tomodularize the networks using modules on the level of matrix multiplication,layernorm, etc",
            "As we will see, naive implementation of these operations'backward functions also have the same runtime as the evaluation of thesefunctions"
        ]
    },
    "page_106": {
        "content_preview": "1057.4.3 Backward functions for basic modulesUsing the general strategy in Section 7.4.2, it suces to compute the back-ward function for all modules Mi's used in the networks. We compute thebackward function for the basic module MM, activations , and loss functionsin this section.Backward function for MM.Suppose MM W;b(z) =Wz+bis a matrix multi-plication module where z2RmandW2Rnm. Then, using equation (7.59),we have for v2RnB[MM;z](v) =264@(Wz+b)1@z1@(Wz+b)n@z1.........@(Wz+b)1@zm@(Wz+b)n@zm375v...",
        "python_code": [
            "1057.4.3 Backward functions for basic modulesUsing the general strategy in Section 7.4.2, it suces to compute the back-ward function for all modules Mi's used in the networks. We compute thebackward function for the basic module MM, activations , and loss functionsin this section.Backward function for MM.Suppose MM W;b(z) =Wz+bis a matrix multi-plication module where z2RmandW2Rnm. Then, using equation (7.59),we have for v2RnB[MM;z](v) =264@(Wz+b)1@z1@(Wz+b)n@z1.........@(Wz+b)1@zm@(Wz+b)n@zm375v: (7.62)Using the fact that 8i2[m];j2[n],@(Wz+b)j@zi=@bj+Pmk=1Wjkzk@zi=Wji, wehaveB[MM;z](v) =W>v2Rm: (7.63)In the derivation above, we have treated MM as a function of z. If we treatMM as a function of Wandb, then we can also compute the backwardfunction for the parameter variables Wandb. It's less convenient to useequation (7.59) because the variable Wis a matrix and the matrix in (7.59)will be a 4-th order tensor that is challenging for us to mathematically writedown. We use (7.58) instead:(B[MM;W](v))ij=mXk=1@(Wz+b)k@Wijvk=mXk=1@Pms=1Wkszs@Wijvk=vizj:(7.64)In vectorized notation, we haveB[MM;W](v) =vz>2Rnm: (7.65)Using equation (7.59) for the variable b, we have,B[MM;b](v) =264@(Wz+b)1@b1@(Wz+b)n@b1.........@(Wz+b)1@bn@(Wz+b)n@bn375v=v: (7.66)"
        ],
        "formulas": [
            "zi=@bj+Pmk=1Wjkzk@zi=Wji,",
            "ij=mXk=1@(Wz+b)k@Wijvk=mXk=1@Pms=1Wkszs@Wijvk=vizj:(7.64)In",
            "bn375v=v:"
        ],
        "explanations": []
    },
    "page_107": {
        "content_preview": "106Here we used that@(Wz+b)j@bi= 0 ifi6=jand@(Wz+b)j@bi= 1 ifi=j.The computational eciency for computing the backward function isO(mn), the same as evaluating the result of matrix multiplication up toconstant factor.Backward function for the activations. SupposeM(z) =(z) whereis anelement-wise activation function and z2Rm. Then, using equation (7.59),we haveB[;z](v) =264@(z1)@z1@(zm)@z1.........@(z1)@zm@(zm)@zm375v (7.67)= diag(0(z1);;0(zm))v (7.68)=0(z)v2Rm: (7.69)Here, we used the fact that@(z...",
        "python_code": [
            "106Here we used that@(Wz+b)j@bi= 0 ifi6=jand@(Wz+b)j@bi= 1 ifi=j.The computational eciency for computing the backward function isO(mn), the same as evaluating the result of matrix multiplication up toconstant factor.Backward function for the activations. SupposeM(z) =(z) whereis anelement-wise activation function and z2Rm. Then, using equation (7.59),we haveB[;z](v) =264@(z1)@z1@(zm)@z1.........@(z1)@zm@(zm)@zm375v (7.67)= diag(0(z1);;0(zm))v (7.68)=0(z)v2Rm: (7.69)Here, we used the fact that@(zj)@zi= 0 whenj6=i, diag(1;:::;m) denotesthe diagonal matrix with 1;:::;mon the diagonal, and denotes theelement-wise product of two vectors with the same dimension, and 0() isthe element-wise application of the derivative of the activation function .Regarding computation eciency, we note that at the rst sight, equa-tion (7.67) appears to indicate the backward function takes O(m2) time, butequation (7.69) shows that it's implementable in O(m) time (which is thesame as the time for evaluating of the function.) We are not supposed to besurprised by that the possibility of simplifying equation (7.67) to (7.69)|ifwe use smaller modules, that is, treating the vector-to-vector nonlinear ac-tivation as mscalar-to-scalar non-linear activation, then it's more obviousthat the backward pass should have similar time to the forward pass.Backward function for loss functions. When a module Mtakes in a vectorzand outputs a scalar, by equation (7.59), the backward function takes in ascalarvand outputs a vector with entries ( B[M;z](v))i=@M@ziv. Therefore,in vectorized notation, B[M;z](v) =@M@zv.Recall that squared loss `MSE(z;y) =12(zy)2:Thus,B[`MSE;z](v) =@12(zy)2@zv= (zy)v.For logistics loss, by equation (2.6), we haveB[`logistic;t](v) =@`logistic (t;y)@tv= (1=(1 + exp(t))y)v: (7.70)"
        ],
        "formulas": [
            "bi= 0",
            "ifi6=jand@(Wz+b)j@bi=",
            "ifi=j.The",
            "zi= 0",
            "whenj6=i,",
            "i=@M@ziv.",
            "zv= (zy)v.For",
            "tv= (1=(1"
        ],
        "explanations": []
    },
    "page_108": {
        "content_preview": "107For cross-entropy loss, by equation (2.17), we haveB[`ce;t](v) =@`ce(t;y)@tv= (ey)v; (7.71)where= softmax( t).7.4.4 Back-propagation for MLPsGiven the backward functions for every module needed in evaluating the lossof an MLP, we follow the strategy in Section 7.4.2 to compute the gradientof the loss w.r.t to the hidden activations and the parameters.We consider the an r-layer MLP with a logistic loss. The loss functioncan be computed via a sequence of operations (that is, the forward pass),z...",
        "python_code": [
            "107For cross-entropy loss, by equation (2.17), we haveB[`ce;t](v) =@`ce(t;y)@tv= (ey)v; (7.71)where= softmax( t).7.4.4 Back-propagation for MLPsGiven the backward functions for every module needed in evaluating the lossof an MLP, we follow the strategy in Section 7.4.2 to compute the gradientof the loss w.r.t to the hidden activations and the parameters.We consider the an r-layer MLP with a logistic loss. The loss functioncan be computed via a sequence of operations (that is, the forward pass),z[1]= MMW[1];b[1](x);a[1]=(z[1])z[2]= MMW[2];b[2](a[1])a[2]=(z[2])...z[r]= MMW[r];b[r](a[r1])J=`logistic (z[r];y): (7.72)We apply the backward function sequentially in a backward order. First, wehave that@J@z[r]=B[`logistic;z[r]]@J@J=B[`logistic;z[r]](1): (7.73)Then, we iteratively compute@J@a[i]and@J@z[i]'s by repeatedly invoking the chainrule (equation (7.58)),@J@a[r1]=B[MM;a[r1]]@J@z[r]@J@z[r1]=B[;z[r1]]@J@a[r1]...@J@z[1]=B[;z[1]]@J@a[1]: (7.74)"
        ],
        "formulas": [
            "tv= (ey)v;",
            "where= softmax(",
            "J=`logistic",
            "J=B[`logistic;z[r]](1):"
        ],
        "explanations": []
    },
    "page_109": {
        "content_preview": "108Numerically, we compute these quantities by repeatedly invoking equa-tions (7.69) and (7.63) with dierent choices of variables.We note that the intermediate values of a[i]andz[i]are used in the back-propagation (equation (7.74)), and therefore these values need to be storedin the memory after the forward pass.Next, we compute the gradient of the parameters by invoking equa-tions (7.65) and (7.66),@J@W[r]=B[MM;W[r]]@J@z[r]@J@b[r]=B[MM;b[r]]@J@z[r]...@J@W[1]=B[MM;W[1]]@J@z[1]@J@b[1]=B[MM;b[1]]@...",
        "python_code": [
            "108Numerically, we compute these quantities by repeatedly invoking equa-tions (7.69) and (7.63) with dierent choices of variables.We note that the intermediate values of a[i]andz[i]are used in the back-propagation (equation (7.74)), and therefore these values need to be storedin the memory after the forward pass.Next, we compute the gradient of the parameters by invoking equa-tions (7.65) and (7.66),@J@W[r]=B[MM;W[r]]@J@z[r]@J@b[r]=B[MM;b[r]]@J@z[r]...@J@W[1]=B[MM;W[1]]@J@z[1]@J@b[1]=B[MM;b[1]]@J@z[1]: (7.75)We also note that the block of computations in equations (7.75) can beinterleaved with the block of computation in equations (7.74) because the@J@W[i]and@J@b[i]can be computed as soon as@J@z[i]is computed.Putting all of these together, and explicitly invoking the equa-tions (7.72), (7.74) and (7.75), we have the following algorithm (Algorithm 3)."
        ],
        "formulas": [],
        "explanations": []
    },
    "page_110": {
        "content_preview": "109Algorithm 3 Back-propagation for multi-layer neural networks.1:Forward pass. Compute and store the values of a[k]'s,z[k]'s, andJusing the equations (7.72).2:Backward pass. Compute the gradient of loss Jwith respect to z[r]:@J@z[r]=B[`logistic;z[r]](1) =1=(1 + exp(z[r]))y: (7.76)3:fork=r1 to 0 do4: Compute the gradient with respect to parameters W[k+1]andb[k+1].@J@W[k+1]=B[MM;W[k+1]]@J@z[k+1]=@J@z[k+1]a[k]>: (7.77)@J@b[k+1]=B[MM;b[k+1]]@J@z[k+1]=@J@z[k+1]: (7.78)5: Whenk1, compute the gradient...",
        "python_code": [
            "109Algorithm 3 Back-propagation for multi-layer neural networks.1:Forward pass. Compute and store the values of a[k]'s,z[k]'s, andJusing the equations (7.72).2:Backward pass. Compute the gradient of loss Jwith respect to z[r]:@J@z[r]=B[`logistic;z[r]](1) =1=(1 + exp(z[r]))y: (7.76)3:fork=r1 to 0 do4: Compute the gradient with respect to parameters W[k+1]andb[k+1].@J@W[k+1]=B[MM;W[k+1]]@J@z[k+1]=@J@z[k+1]a[k]>: (7.77)@J@b[k+1]=B[MM;b[k+1]]@J@z[k+1]=@J@z[k+1]: (7.78)5: Whenk1, compute the gradient with respect to z[k]anda[k].@J@a[k]=B[;a[k]]@J@z[k+1]=W[k+1]>@J@z[k+1]: (7.79)@J@z[k]=B[;z[k]]@J@a[k]=0(z[k])@J@a[k]: (7.80)7.5 Vectorization over training examplesAs we discussed in Section 7.1, in the implementation of neural networks,we will leverage the parallelism across the multiple examples. This meansthat we will need to write the forward pass (the evaluation of the outputs)of the neural network and the backward pass (backpropagation) for multiple"
        ],
        "formulas": [
            "1=(1",
            "fork=r1"
        ],
        "explanations": []
    },
    "page_111": {
        "content_preview": "110training examples in matrix notation.The basic idea. The basic idea is simple. Suppose you have a trainingset with three examples x(1);x(2);x(3). The rst-layer activations for eachexample are as follows:z[1](1)=W[1]x(1)+b[1]z[1](2)=W[1]x(2)+b[1]z[1](3)=W[1]x(3)+b[1]Note the dierence between square brackets [ ], which refer to the layer num-ber, and parenthesis ( ), which refer to the training example number. In-tuitively, one would implement this using a for loop. It turns out, we canvectoriz...",
        "python_code": [
            "110training examples in matrix notation.The basic idea. The basic idea is simple. Suppose you have a trainingset with three examples x(1);x(2);x(3). The rst-layer activations for eachexample are as follows:z[1](1)=W[1]x(1)+b[1]z[1](2)=W[1]x(2)+b[1]z[1](3)=W[1]x(3)+b[1]Note the dierence between square brackets [ ], which refer to the layer num-ber, and parenthesis ( ), which refer to the training example number. In-tuitively, one would implement this using a for loop. It turns out, we canvectorize these operations as well. First, dene:X=24j j jx(1)x(2)x(3)j j j352Rd3(7.81)Note that we are stacking training examples in columns and notrows. Wecan then combine this into a single unied formulation:Z[1]=24j j jz[1](1)z[1](2)z[1](3)j j j35=W[1]X+b[1](7.82)You may notice that we are attempting to add b[1]2R41toW[1]X2R43. Strictly following the rules of linear algebra, this is not allowed. Inpractice however, this addition is performed using broadcasting . We createan intermediate ~b[1]2R43:~b[1]=24j j jb[1]b[1]b[1]j j j35 (7.83)We can then perform the computation: Z[1]=W[1]X+~b[1]. Often times, itis not necessary to explicitly construct ~b[1]. By inspecting the dimensions in(7.82), you can assume b[1]2R41is correctly broadcast to W[1]X2R43.The matricization approach as above can easily generalize to multiplelayers, with one subtlety though, as discussed below."
        ],
        "formulas": [
            "X=24j",
            "j35=W[1]X+b[1](7.82)You"
        ],
        "explanations": []
    },
    "page_112": {
        "content_preview": "111Complications/Subtlety in the Implementation. All the deep learn-ing packages or implementations put the data points in the rows of a datamatrix. (If the data point itself is a matrix or tensor, then the data are con-centrated along the zero-th dimension.) However, most of the deep learningpapers use a similar notation to these notes where the data points are treatedas column vectors.8There is a simple conversion to deal with the mismatch:in the implementation, all the columns become row vect...",
        "python_code": [
            "111Complications/Subtlety in the Implementation. All the deep learn-ing packages or implementations put the data points in the rows of a datamatrix. (If the data point itself is a matrix or tensor, then the data are con-centrated along the zero-th dimension.) However, most of the deep learningpapers use a similar notation to these notes where the data points are treatedas column vectors.8There is a simple conversion to deal with the mismatch:in the implementation, all the columns become row vectors, row vectors be-come column vectors, all the matrices are transposed, and the orders of thematrix multiplications are ipped. In the example above, using the row ma-jor convention, the data matrix is X2R3d, the rst layer weight matrixhas dimensionality dm(instead of mdas in the two layer neural netsection), and the bias vector b[1]2R1m. The computation for the hiddenactivation becomesZ[1]=XW[1]+b[1]2R3m(7.84)8The instructor suspects that this is mostly because in mathematics we naturally mul-tiply a matrix to a vector on the left hand side."
        ],
        "formulas": [],
        "explanations": []
    },
    "page_113": {
        "content_preview": "Part IIIGeneralization andregularization112...",
        "python_code": [],
        "formulas": [],
        "explanations": [
            "Part IIIGeneralization andregularization112"
        ]
    },
    "page_114": {
        "content_preview": "Chapter 8GeneralizationThis chapter discusses tools to analyze and understand the generaliza-tion of machine learning models, i.e, their performances on unseen testexamples. Recall that for supervised learning problems, given a train-ing datasetf(x(i);y(i))gni=1, we typically learn a model hby minimizing aloss/cost function J(), which encourages hto t the data. E.g., whenthe loss function is the least square loss (aka mean squared error), we haveJ() =1nPni=1(y(i)h(x(i)))2. This loss function for...",
        "python_code": [
            "Chapter 8GeneralizationThis chapter discusses tools to analyze and understand the generaliza-tion of machine learning models, i.e, their performances on unseen testexamples. Recall that for supervised learning problems, given a train-ing datasetf(x(i);y(i))gni=1, we typically learn a model hby minimizing aloss/cost function J(), which encourages hto t the data. E.g., whenthe loss function is the least square loss (aka mean squared error), we haveJ() =1nPni=1(y(i)h(x(i)))2. This loss function for training purposes isoftentimes referred to as the training loss/error/cost.However, minimizing the training loss is not our ultimate goal|it ismerely our approach towards the goal of learning a predictive model. Themost important evaluation metric of a model is the loss on unseen test exam-ples, which is oftentimes referred to as the test error. Formally, we sample atest example ( x;y) from the so-called test distribution D, and measure themodel's error on it, by, e.g., the mean squared error, ( h(x)y)2. The ex-pected loss/error over the randomness of the test example is called the testloss/error,1L() =E(x;y)D[(yh(x))2] (8.1)Note that the measurement of the error involves computing the expectation,and in practice, it can be approximated by the average error on many sampledtest examples, which are referred to as the test dataset. Note that the keydierence here between training and test datasets is that the test examples1In theoretical and statistical literature, we oftentimes call the uniform distributionover the training set f(x(i);y(i))gni=1, denoted by bD, an empirical distribution, and callDthe population distribution. Partly because of this, the training loss is also referredto as the empirical loss/risk/error, and the test loss is also referred to as the populationloss/risk/error.113"
        ],
        "formulas": [
            "gni=1,",
            "1nPni=1(y(i)h(x(i)))2.",
            "gni=1,"
        ],
        "explanations": []
    },
    "page_115": {
        "content_preview": "114areunseen , in the sense that the training procedure has not used the testexamples. In classical statistical learning settings, the training examples arealso drawn from the same distribution as the test distribution D, but stillthe test examples are unseen by the learning procedure whereas the trainingexamples are seen.2Because of this key dierence between training and test datasets, evenif they are both drawn from the same distribution D, the test error is notnecessarily always close to the ...",
        "python_code": [],
        "formulas": [],
        "explanations": [
            "114areunseen , in the sense that the training procedure has not used the testexamples",
            "In classical statistical learning settings, the training examples arealso drawn from the same distribution as the test distribution D, but stillthe test examples are unseen by the learning procedure whereas the trainingexamples are seen",
            "2Because of this key dierence between training and test datasets, evenif they are both drawn from the same distribution D, the test error is notnecessarily always close to the training error",
            "3As a result, successfully min-imizing the training error may not always lead to a small test error",
            "Wetypically say the model overts the data if the model predicts accurately onthe training dataset but doesn't generalize well to other test examples, thatis, if the training error is small but the test error is large",
            "We say the modelunderts the data if the training error is relatively large4(and in this case,typically the test error is also relatively large",
            ")This chapter studies how the test error is inuenced by the learning pro-cedure, especially the choice of model parameterizations",
            "We will decomposethe test error into \\bias\" and \\variance\" terms and study how each of them isaected by the choice of model parameterizations and their tradeos",
            "Usingthe bias-variance tradeo, we will discuss when overtting and underttingwill occur and be avoided",
            "We will also discuss the double descent phe-nomenon in Section 8",
            "2 and some classical theoretical results in Section 8",
            "2These days, researchers have increasingly been more interested in the setting with\\domain shift\", that is, the training distribution and test distribution are dierent",
            "3the dierence between test error and training error is often referred to as the gener-alization gap",
            "The term generalization error in some literature means the test error, andin some other literature means the generalization gap",
            ", larger than the intrinsic noise level of the data in regression problems"
        ]
    },
    "page_116": {
        "content_preview": "1158.1 Bias-variance tradeo0.0 0.2 0.4 0.6 0.8 1.0x0.00.51.01.5ytraining datasettraining dataground truth h*0.0 0.2 0.4 0.6 0.8 1.0x0.00.51.01.5ytest datasettest dataground truth h*Figure 8.1: A running example of training and test dataset for this section.As an illustrating example, we consider the following training dataset andtest dataset, which are also shown in Figure 8.1. The training inputs x(i)'s arerandomly chosen and the outputs y(i)are generated by y(i)=h?(x(i)) +(i)where the function...",
        "python_code": [
            "1158.1 Bias-variance tradeo0.0 0.2 0.4 0.6 0.8 1.0x0.00.51.01.5ytraining datasettraining dataground truth h*0.0 0.2 0.4 0.6 0.8 1.0x0.00.51.01.5ytest datasettest dataground truth h*Figure 8.1: A running example of training and test dataset for this section.As an illustrating example, we consider the following training dataset andtest dataset, which are also shown in Figure 8.1. The training inputs x(i)'s arerandomly chosen and the outputs y(i)are generated by y(i)=h?(x(i)) +(i)where the function h?() is a quadratic function and is shown in Figure 8.1as the solid line, and (i)is the a observation noise assumed to be generatedfromN(0;2). A test example ( x;y) also has the same input-outputrelationship y=h?(x) +whereN(0;2). It's impossible to predict thenoise, and therefore essentially our goal is to recover the function h?().We will consider the test error of learning various types of models. Whentalking about linear regression, we discussed the problem of whether to ta \\simple\" model such as the linear \\ y=0+1x,\" or a more \\complex\"model such as the polynomial \\ y=0+1x+5x5.\"We start with tting a linear model, as shown in Figure 8.2. The besttted linear model cannot predict yfromxaccurately even on the trainingdataset, let alone on the test dataset. This is because the true relationshipbetweenyandxis not linear|any linear model is far away from the truefunctionh?(). As a result, the training error is large and this is a typicalsituation of undertting ."
        ],
        "formulas": [
            "y=h?(x)",
            "y=0+1x,\"",
            "y=0+1x+5x5.\"We"
        ],
        "explanations": []
    },
    "page_117": {
        "content_preview": "1160.0 0.2 0.4 0.6 0.8 1.0x0.00.51.01.5ytraining databest fit linear model0.0 0.2 0.4 0.6 0.8 1.0x0.00.51.01.5ytest databest fit linear modelFigure 8.2: The best t linear model has large training and test errors.The issue cannot be mitigated with more training examples|even witha very large amount of, or even innite training examples, the best ttedlinear model is still inaccurate and fails to capture the structure of the data(Figure 8.3). Even if the noise is not present in the training data, th...",
        "python_code": [],
        "formulas": [],
        "explanations": [
            "5ytraining databest fit linear model0",
            "5ytest databest fit linear modelFigure 8",
            "2: The best t linear model has large training and test errors",
            "The issue cannot be mitigated with more training examples|even witha very large amount of, or even innite training examples, the best ttedlinear model is still inaccurate and fails to capture the structure of the data(Figure 8",
            "Even if the noise is not present in the training data, the issuestill occurs (Figure 8",
            "Therefore, the fundamental bottleneck here is thelinear model family's inability to capture the structure in the data|linearmodels cannot represent the true quadratic function h?|, but not the lack ofthe data",
            "Informally, we dene the bias of a model to be the test error evenif we were to t it to a very (say, innitely) large training dataset",
            "Thus, inthis case, the linear model suers from large bias, and underts (i",
            ", fails tocapture structure exhibited by) the data",
            "5yfitting linear models on a large datasettraining dataground truth h*best fit linear modelFigure 8",
            "3: The best t linearmodel on a much larger datasetstill has a large training error",
            "5yfitting linear models on a noiseless datasettraining dataground truth h*best fit linear modelFigure 8",
            "4: The best t linearmodel on a noiseless dataset alsohas a large training/test error",
            "Next, we t a 5th-degree polynomial to the data",
            "5 shows thatit fails to learn a good model either",
            "However, the failure pattern is dierentfrom the linear model case",
            "Specically, even though the learnt 5th-degree"
        ]
    },
    "page_118": {
        "content_preview": "117polynomial did a very good job predicting y(i)'s fromx(i)'s for training ex-amples, it does not work well on test examples (Figure 8.5). In other words,the model learnt from the training set does not generalize well to other testexamples|the test error is high. Contrary to the behavior of linear models,the bias of the 5-th degree polynomials is small|if we were to t a 5-th de-gree polynomial to an extremely large dataset, the resulting model would beclose to a quadratic function and be accura...",
        "python_code": [
            "117polynomial did a very good job predicting y(i)'s fromx(i)'s for training ex-amples, it does not work well on test examples (Figure 8.5). In other words,the model learnt from the training set does not generalize well to other testexamples|the test error is high. Contrary to the behavior of linear models,the bias of the 5-th degree polynomials is small|if we were to t a 5-th de-gree polynomial to an extremely large dataset, the resulting model would beclose to a quadratic function and be accurate (Figure 8.6). This is becausethe family of 5-th degree polynomials contains all the quadratic functions(setting5=4=3= 0 results in a quadratic function), and, therefore,5-th degree polynomials are in principle capable of capturing the structureof the data.0.0 0.2 0.4 0.6 0.8 1.0x0.00.51.01.5ytraining databest fit 5-th degree model0.0 0.2 0.4 0.6 0.8 1.0x0.00.51.01.5ytest dataground truth h*best fit 5-th degree modelFigure 8.5: Best t 5-th degree polynomial has zero training error, but stillhas a large test error and does not recover the the ground truth. This is aclassic situation of overtting.0.0 0.2 0.4 0.6 0.8 1.0x0.00.51.01.5ytraining databest fit 5-th degree modelground truth h*fitting 5-th degree model on large datasetFigure 8.6: The best t 5-th degree polynomial on a huge dataset nearlyrecovers the ground-truth|suggesting that the culprit in Figure 8.5 is thevariance (or lack of data) but not bias.The failure of tting 5-th degree polynomials can be captured by another"
        ],
        "formulas": [
            "setting5=4=3="
        ],
        "explanations": []
    },
    "page_119": {
        "content_preview": "118component of the test error, called variance of a model tting procedure.Specically, when tting a 5-th degree polynomial as in Figure 8.7, there is alarge risk that we're tting patterns in the data that happened to be presentin our small, nite training set, but that do not reect the wider pattern ofthe relationship between xandy. These \\spurious\" patterns in the trainingset are (mostly) due to the observation noise (i), and tting these spuriouspatters results in a model with large test error. ...",
        "python_code": [],
        "formulas": [],
        "explanations": [
            "118component of the test error, called variance of a model tting procedure",
            "Specically, when tting a 5-th degree polynomial as in Figure 8",
            "7, there is alarge risk that we're tting patterns in the data that happened to be presentin our small, nite training set, but that do not reect the wider pattern ofthe relationship between xandy",
            "These \\spurious\" patterns in the trainingset are (mostly) due to the observation noise (i), and tting these spuriouspatters results in a model with large test error",
            "In this case, we say the modelhas a large variance",
            "5ytraining databest fit 5-th degree model0",
            "5ytraining databest fit 5-th degree model0",
            "5ytraining databest fit 5-th degree modelfitting 5-th degree model on different datasetsFigure 8",
            "7: The best t 5-th degree models on three dierent datasets gen-erated from the same distribution behave quite dierently, suggesting theexistence of a large variance",
            "The variance can be intuitively (and mathematically, as shown in Sec-tion 8",
            "1) characterized by the amount of variations across models learnton multiple dierent training datasets (drawn from the same underlying dis-tribution)",
            "The \\spurious patterns\" are specic to the randomness of thenoise (and inputs) in a particular dataset, and thus are dierent across mul-tiple training datasets",
            "Therefore, overtting to the \\spurious patterns\" ofmultiple datasets should result in very dierent models",
            "Indeed, as shownin Figure 8",
            "7, the models learned on the three dierent training datasets arequite dierent, overtting to the \\spurious patterns\" of each datasets",
            "Often, there is a tradeo between bias and variance",
            "If our model is too\\simple\" and has very few parameters, then it may have large bias (but smallvariance), and it typically may suer from underttng",
            "If it is too \\complex\"and has very many parameters, then it may suer from large variance (buthave smaller bias), and thus overtting",
            "8 for a typical tradeobetween bias and variance"
        ]
    },
    "page_120": {
        "content_preview": "119Model ComplexityErrorBias2VarianceTest Error (= Bias2+Variance) Optimal TradeoffFigure 8.8: An illustration of the typical bias-variance tradeo.As we will see formally in Section 8.1.1, the test error can be decomposedas a summation of bias and variance. This means that the test error willhave a convex curve as the model complexity increases, and in practice weshould tune the model complexity to achieve the best tradeo. For instance,in the example above, tting a quadratic function does better...",
        "python_code": [
            "119Model ComplexityErrorBias2VarianceTest Error (= Bias2+Variance) Optimal TradeoffFigure 8.8: An illustration of the typical bias-variance tradeo.As we will see formally in Section 8.1.1, the test error can be decomposedas a summation of bias and variance. This means that the test error willhave a convex curve as the model complexity increases, and in practice weshould tune the model complexity to achieve the best tradeo. For instance,in the example above, tting a quadratic function does better than either ofthe extremes of a rst or a 5-th degree polynomial, as shown in Figure 8.9.0.0 0.2 0.4 0.6 0.8 1.0x0.00.51.01.5ytraining databest fit quadratic model0.0 0.2 0.4 0.6 0.8 1.0x0.00.51.01.5ytest databest fit quadratic modelground truth h*Figure 8.9: Best t quadratic model has small training and test error becausequadratic model achieves a better tradeo.Interestingly, the bias-variance tradeo curves or the test error curvesdo not universally follow the shape in Figure 8.8, at least not universallywhen the model complexity is simply measured by the number of parameters.(We will discuss the so-called double descent phenomenon in Section 8.2.)Nevertheless, the principle of bias-variance tradeo is perhaps still the rstresort when analyzing and predicting the behavior of test errors."
        ],
        "formulas": [],
        "explanations": []
    },
    "page_121": {
        "content_preview": "1208.1.1 A mathematical decomposition (for regression)To formally state the bias-variance tradeo for regression problems, we con-sider the following setup (which is an extension of the beginning paragraphof Section 8.1).•Draw a training dataset S=fx(i);y(i)gni=1such thaty(i)=h?(x(i)) +(i)where(i)2N(0;2).•Train a model on the dataset S, denoted by ^hS.•Take a test example ( x;y) such that y=h?(x) +whereN(0;2),and measure the expected test error (averaged over the random draw ofthe training set Sa...",
        "python_code": [
            "1208.1.1 A mathematical decomposition (for regression)To formally state the bias-variance tradeo for regression problems, we con-sider the following setup (which is an extension of the beginning paragraphof Section 8.1).•Draw a training dataset S=fx(i);y(i)gni=1such thaty(i)=h?(x(i)) +(i)where(i)2N(0;2).•Train a model on the dataset S, denoted by ^hS.•Take a test example ( x;y) such that y=h?(x) +whereN(0;2),and measure the expected test error (averaged over the random draw ofthe training set Sand the randomness of )56MSE(x) =ES;[(yhS(x))2] (8.2)We will decompose the MSE into a bias and variance term. We start bystating a following simple mathematical tool that will be used twice below.Claim 8.1.1: SupposeAandBare two independent real random variablesandE[A] = 0. Then, E[(A+B)2] =E[A2] +E[B2].As a corollary, because a random variable Ais independent with a con-stantc, when E[A] = 0, we have E[(A+c)2] =E[A2] +c2.The proof of the claim follows from expanding the square: E[(A+B)2] =E[A2] +E[B2] + 2E[AB] =E[A2] +E[B2]. Here we used the independence toshow that E[AB] =E[A]E[B] = 0.Using Claim 8.1.1 with A=andB=h?(x)^hS(x), we haveMSE(x) =E[(yhS(x))2] =E[(+ (h?(x)hS(x)))2] (8.3)=E[2] +E[(h?(x)hS(x))2] (by Claim 8.1.1)=2+E[(h?(x)hS(x))2] (8.4)Then, let's dene havg(x) =ES[hS(x)] as the \\average model\"|the modelobtained by drawing an innite number of datasets, training on them, andaveraging their predictions on x. Note that havgis a hypothetical model foranalytical purposes that can not be obtained in reality (because we don't5For simplicity, the test input xis considered to be xed here, but the same conceptualmessage holds when we average over the choice of x's.6The subscript under the expectation symbol is to emphasize the variables that areconsidered as random by the expectation operation."
        ],
        "formulas": [
            "S=fx(i);y(i)gni=1such",
            "y=h?(x)",
            "A=andB=h?(x)^hS(x),"
        ],
        "explanations": []
    },
    "page_122": {
        "content_preview": "121have innite number of datasets). It turns out that for many cases, havgis (approximately) equal to the the model obtained by training on a singledataset with innite samples. Thus, we can also intuitively interpret havgthisway, which is consistent with our intuitive denition of bias in the previoussubsection.We can further decompose MSE( x) by letting c=h?(x)havg(x) (which isa constant that does not depend on the choice of S!) andA=havg(x)hS(x)in the corollary part of Claim 8.1.1:MSE(x) =2+E[(...",
        "python_code": [
            "121have innite number of datasets). It turns out that for many cases, havgis (approximately) equal to the the model obtained by training on a singledataset with innite samples. Thus, we can also intuitively interpret havgthisway, which is consistent with our intuitive denition of bias in the previoussubsection.We can further decompose MSE( x) by letting c=h?(x)havg(x) (which isa constant that does not depend on the choice of S!) andA=havg(x)hS(x)in the corollary part of Claim 8.1.1:MSE(x) =2+E[(h?(x)hS(x))2] (8.5)=2+ (h?(x)havg(x))2+E[(havghS(x))2] (8.6)=2|{z}unavoidable+ (h?(x)havg(x))2|{z},bias2+ var(hS(x))|{z},variance(8.7)We call the second term the bias (square) and the third term the variance. Asdiscussed before, the bias captures the part of the error that are introduceddue to the lack of expressivity of the model. Recall that havgcan be thoughtof as the best possible model learned even with innite data. Thus, the bias isnot due to the lack of data, but is rather caused by that the family of modelsfundamentally cannot approximate the h?. For example, in the illustratingexample in Figure 8.2, because any linear model cannot approximate thetrue quadratic function h?, neither can havg, and thus the bias term has tobe large.The variance term captures how the random nature of the nite datasetintroduces errors in the learned model. It measures the sensitivity of thelearned model to the randomness in the dataset. It often decreases as thesize of the dataset increases.There is nothing we can do about the rst term 2as we can not predictthe noiseby denition.Finally, we note that the bias-variance decomposition for classicationis much less clear than for regression problems. There have been severalproposals, but there is as yet no agreement on what is the \\right\" and/orthe most useful formalism.8.2 The double descent phenomenonModel-wise double descent. Recent works have demonstrated that thetest error can present a \\double descent\" phenomenon in a range of machine"
        ],
        "formulas": [
            "c=h?(x)havg(x)",
            "andA=havg(x)hS(x)in"
        ],
        "explanations": []
    },
    "page_123": {
        "content_preview": "122learning models including linear models and deep neural networks.7Theconventional wisdom, as discussed in Section 8.1, is that as we increase themodel complexity, the test error rst decreases and then increases, as illus-trated in Figure 8.8. However, in many cases, we empirically observe thatthe test error can have a second descent|it rst decreases, then increasesto a peak around when the model size is large enough to t all the trainingdata very well, and then decreases again in the so-calle...",
        "python_code": [],
        "formulas": [],
        "explanations": [
            "122learning models including linear models and deep neural networks",
            "7Theconventional wisdom, as discussed in Section 8",
            "1, is that as we increase themodel complexity, the test error rst decreases and then increases, as illus-trated in Figure 8",
            "However, in many cases, we empirically observe thatthe test error can have a second descent|it rst decreases, then increasesto a peak around when the model size is large enough to t all the trainingdata very well, and then decreases again in the so-called overparameterizedregime, where the number of parameters is larger than the number of datapoints",
            "10 for an illustration of the typical curves of test errorsagainst model complexity (measured by the number of parameters)",
            "To someextent, the overparameterized regime with the second descent is considered asnew to the machine learning community|partly because lightly-regularized,overparameterized models are only extensively used in the deep learning era",
            "A practical implication of the phenomenon is that one should not hold backfrom scaling into and experimenting with over-parametrized models becausethe test error may well decrease again to a level even smaller than the previ-ous lowest point",
            "Actually, in many cases, larger overparameterized modelsalways lead to a better test performance (meaning there won't be a secondascent after the second descent)",
            "# parameterstest errortypically when # parametersis sufficient to fit the dataclassical regime:bias-variance tradeoffmodern regime:over-parameterizationFigure 8",
            "10: A typical model-wise double descent phenomenon",
            "As the num-ber of parameters increases, the test error rst decreases when the number ofparameters is smaller than the training data",
            "Then in the overparameterizedregime, the test error decreases again",
            "7The discovery of the phenomenon perhaps dates back to Opper [1995, 2001], and hasbeen recently popularized by Belkin et al"
        ]
    },
    "page_124": {
        "content_preview": "123Sample-wise double descent. A priori, we would expect that moretraining examples always lead to smaller test errors|more samples givestrictly more information for the algorithm to learn from. However, recentwork [Nakkiran, 2019] observes that the test error is not monotonically de-creasing as we increase the sample size. Instead, as shown in Figure 8.11, thetest error decreases, and then increases and peaks around when the numberof examples (denoted by n) is similar to the number of parameter...",
        "python_code": [],
        "formulas": [],
        "explanations": [
            "123Sample-wise double descent",
            "A priori, we would expect that moretraining examples always lead to smaller test errors|more samples givestrictly more information for the algorithm to learn from",
            "However, recentwork [Nakkiran, 2019] observes that the test error is not monotonically de-creasing as we increase the sample size",
            "Instead, as shown in Figure 8",
            "11, thetest error decreases, and then increases and peaks around when the numberof examples (denoted by n) is similar to the number of parameters (denotedbyd), and then decreases again",
            "We refer to this as the sample-wise dou-ble descent phenomenon",
            "To some extent, sample-wise double descent andmodel-wise double descent are essentially describing similar phenomena|thetest error is peaked when nd",
            "Explanation and mitigation strategy",
            "The sample-wise double descent,or, in particular, the peak of test error at nd, suggests that the existingtraining algorithms evaluated in these experiments are far from optimal whennd",
            "We will be better o by tossing away some examples and run thealgorithms with a smaller sample size to steer clear of the peak",
            "In otherwords, in principle, there are other algorithms that can achieve smaller testerror when nd, but the algorithms evaluated in these experiments fail todo so",
            "The sub-optimality of the learning procedure appears to be the culpritof the peak in both sample-wise and model-wise double descent",
            "Indeed, with an optimally-tuned regularization (which will be discussedmore in Section 9), the test error in the ndregime can be dramaticallyimproved, and the model-wise and sample-wise double descent are both mit-igated",
            "The intuition above only explains the peak in the model-wise and sample-wise double descent, but does not explain the second descent in the model-wise double descent|why overparameterized models are able to generalizeso well",
            "The theoretical understanding of overparameterized models is an ac-tive research area with many recent advances",
            "A typical explanation is thatthe commonly-used optimizers such as gradient descent provide an implicitregularization eect (which will be discussed in more detail in Section 9",
            "In other words, even in the overparameterized regime and with an unregular-ized loss function, the model is still implicitly regularized, and thus exhibitsa better test performance than an arbitrary solution that ts the data",
            "Forexample, for linear models, when nd, the gradient descent optimizer withzero initialization nds the minimum norm solution that ts the data (in-stead of an arbitrary solution that ts the data), and the minimum norm reg-ularizer turns out to be a suciently good for the overparameterized regime(but it's not a good regularizer when nd, resulting in the peak of test"
        ]
    },
    "page_125": {
        "content_preview": "124error).0 200 400 600 800 1000Num Samples0.000.250.500.751.001.251.501.752.00T est ErrorT est Error vs. # SamplesT est ErrorFigure 8.11: Left: The sample-wise double descent phenomenon for linearmodels. Right: The sample-wise double descent with dierent regularizationstrength for linear models. Using the optimal regularization parameter (optimally tuned for each n, shown in green solid curve) mitigates doubledescent. Setup: The data distribution of ( x;y) isxN (0;Id) andyx>+N(0;2) whered= 500;...",
        "python_code": [
            "124error).0 200 400 600 800 1000Num Samples0.000.250.500.751.001.251.501.752.00T est ErrorT est Error vs. # SamplesT est ErrorFigure 8.11: Left: The sample-wise double descent phenomenon for linearmodels. Right: The sample-wise double descent with dierent regularizationstrength for linear models. Using the optimal regularization parameter (optimally tuned for each n, shown in green solid curve) mitigates doubledescent. Setup: The data distribution of ( x;y) isxN (0;Id) andyx>+N(0;2) whered= 500;= 0:5 andkk2= 1.8Finally, we also remark that the double descent phenomenon has beenmostly observed when the model complexity is measured by the number ofparameters. It is unclear if and when the number of parameters is the bestcomplexity measure of a model. For example, in many situations, the normof the models is used as a complexity measure. As shown in Figure 8.12right, for a particular linear case, if we plot the test error against the normof the learnt model, the double descent phenomenon no longer occurs. Thisis partly because the norm of the learned model is also peaked around nd(See Figure 8.12 (middle) or Belkin et al. [2019], Mei and Montanari [2022],and discussions in Section 10.8 of James et al. [2021]). For deep neuralnetworks, the correct complexity measure is even more elusive. The study ofdouble descent phenomenon is an active research topic.8The gure is reproduced from Figure 1 of Nakkiran et al. [2020]. Similar phenomenonare also observed in Hastie et al. [2022], Mei and Montanari [2022]"
        ],
        "formulas": [
            "whered= 500;=",
            "andkk2= 1.8Finally,"
        ],
        "explanations": []
    },
    "page_126": {
        "content_preview": "1250 250 500 750 1000# parameters0.00.20.40.60.81.0test errortest error vs. # params0 200 400 600 800 1000# parameters010203040normnorm vs. # params0 10 20 30 40norm0.00.20.40.60.81.0test errord=n# parameterstest error vs. norm02004006008001000Figure 8.12: Left: The double descent phenomenon, where the number of pa-rameters is used as the model complexity. Middle: The norm of the learnedmodel is peaked around nd.Right: The test error against the norm ofthe learnt model. The color bar indicate th...",
        "python_code": [
            "1250 250 500 750 1000# parameters0.00.20.40.60.81.0test errortest error vs. # params0 200 400 600 800 1000# parameters010203040normnorm vs. # params0 10 20 30 40norm0.00.20.40.60.81.0test errord=n# parameterstest error vs. norm02004006008001000Figure 8.12: Left: The double descent phenomenon, where the number of pa-rameters is used as the model complexity. Middle: The norm of the learnedmodel is peaked around nd.Right: The test error against the norm ofthe learnt model. The color bar indicate the number of parameters and thearrows indicates the direction of increasing model size. Their relationshipare closer to the convention wisdom than to a double descent. Setup: Weconsider a linear regression with a xed dataset of size n= 500:The inputxis a random ReLU feature on Fashion-MNIST, and output y2R10is theone-hot label. This is the same setting as in Section 5.2 of Nakkiran et al.[2020]."
        ],
        "formulas": [
            "errord=n#",
            "n= 500:The"
        ],
        "explanations": []
    },
    "page_127": {
        "content_preview": "1268.3 Sample complexity bounds (optionalreadings)8.3.1 PreliminariesIn this set of notes, we begin our foray into learning theory. Apart frombeing interesting and enlightening in its own right, this discussion will alsohelp us hone our intuitions and derive rules of thumb about how to bestapply learning algorithms in dierent settings. We will also seek to answera few questions: First, can we make formal the bias/variance tradeo thatwas just discussed? This will also eventually lead us to talk a...",
        "python_code": [
            "1268.3 Sample complexity bounds (optionalreadings)8.3.1 PreliminariesIn this set of notes, we begin our foray into learning theory. Apart frombeing interesting and enlightening in its own right, this discussion will alsohelp us hone our intuitions and derive rules of thumb about how to bestapply learning algorithms in dierent settings. We will also seek to answera few questions: First, can we make formal the bias/variance tradeo thatwas just discussed? This will also eventually lead us to talk about modelselection methods, which can, for instance, automatically decide what orderpolynomial to t to a training set. Second, in machine learning it's reallygeneralization error that we care about, but most learning algorithms t theirmodels to the training set. Why should doing well on the training set tell usanything about generalization error? Specically, can we relate error on thetraining set to generalization error? Third and nally, are there conditionsunder which we can actually prove that learning algorithms will work well?We start with two simple but very useful lemmas.Lemma. (The union bound). Let A1;A2;:::;Akbekdierent events (thatmay not be independent). ThenP(A1[[Ak)P(A1) +:::+P(Ak):In probability theory, the union bound is usually stated as an axiom(and thus we won't try to prove it), but it also makes intuitive sense: Theprobability of any one of kevents happening is at most the sum of theprobabilities of the kdierent events.Lemma. (Hoeding inequality) Let Z1;:::;Znbenindependent and iden-tically distributed (iid) random variables drawn from a Bernoulli( ) distri-bution. I.e., P(Zi= 1) =, andP(Zi= 0) = 1. Let ^= (1=n)Pni=1Zibe the mean of these random variables, and let any >0 be xed. ThenP(j^j>)2 exp(22n)This lemma (which in learning theory is also called the Cherno bound )says that if we take ^|the average of nBernoulli() random variables|tobe our estimate of , then the probability of our being far from the true valueis small, so long as nis large. Another way of saying this is that if you havea biased coin whose chance of landing on heads is , then if you toss it n"
        ],
        "formulas": [
            "Zi= 1)",
            "Zi= 0)",
            "1=n)Pni=1Zibe"
        ],
        "explanations": []
    },
    "page_128": {
        "content_preview": "127times and calculate the fraction of times that it came up heads, that will bea good estimate of with high probability (if nis large).Using just these two lemmas, we will be able to prove some of the deepestand most important results in learning theory.To simplify our exposition, let's restrict our attention to binary classica-tion in which the labels are y2f0;1g. Everything we'll say here generalizesto other problems, including regression and multi-class classication.We assume we are given a ...",
        "python_code": [
            "127times and calculate the fraction of times that it came up heads, that will bea good estimate of with high probability (if nis large).Using just these two lemmas, we will be able to prove some of the deepestand most important results in learning theory.To simplify our exposition, let's restrict our attention to binary classica-tion in which the labels are y2f0;1g. Everything we'll say here generalizesto other problems, including regression and multi-class classication.We assume we are given a training set S=f(x(i);y(i));i= 1;:::;ngof sizen, where the training examples ( x(i);y(i)) are drawn iid from some probabilitydistributionD. For a hypothesis h, we dene the training error (also calledtheempirical risk orempirical error in learning theory) to be^\"(h) =1nnXi=11fh(x(i))6=y(i)g:This is just the fraction of training examples that hmisclassies. When wewant to make explicit the dependence of ^ \"(h) on the training set S, we mayalso write this a ^ \"S(h). We also dene the generalization error to be\"(h) =P(x;y)D(h(x)6=y):I.e. this is the probability that, if we now draw a new example ( x;y) fromthe distributionD,hwill misclassify it.Note that we have assumed that the training data was drawn from thesame distributionDwith which we're going to evaluate our hypotheses (inthe denition of generalization error). This is sometimes also referred to asone of the PAC assumptions.9Consider the setting of linear classication, and let h(x) = 1fTx0g.What's a reasonable way of tting the parameters ? One approach is to tryto minimize the training error, and pick^= arg min^\"(h):We call this process empirical risk minimization (ERM), and the resultinghypothesis output by the learning algorithm is ^h=h^. We think of ERMas the most \\basic\" learning algorithm, and it will be this algorithm that we9PAC stands for \\probably approximately correct,\" which is a framework and set ofassumptions under which numerous results on learning theory were proved. Of these, theassumption of training and testing on the same distribution, and the assumption of theindependently drawn training examples, were the most important."
        ],
        "formulas": [
            "S=f(x(i);y(i));i=",
            "1nnXi=11fh(x(i))6=y(i)g:This",
            "6=y):I.e.",
            "h=h^."
        ],
        "explanations": []
    },
    "page_129": {
        "content_preview": "128focus on in these notes. (Algorithms such as logistic regression can also beviewed as approximations to empirical risk minimization.)In our study of learning theory, it will be useful to abstract away fromthe specic parameterization of hypotheses and from issues such as whetherwe're using a linear classier. We dene the hypothesis class Hused by alearning algorithm to be the set of all classiers considered by it. For linearclassication,H=fh:h(x) = 1fTx0g;2Rd+1gis thus the set ofall classiers o...",
        "python_code": [
            "128focus on in these notes. (Algorithms such as logistic regression can also beviewed as approximations to empirical risk minimization.)In our study of learning theory, it will be useful to abstract away fromthe specic parameterization of hypotheses and from issues such as whetherwe're using a linear classier. We dene the hypothesis class Hused by alearning algorithm to be the set of all classiers considered by it. For linearclassication,H=fh:h(x) = 1fTx0g;2Rd+1gis thus the set ofall classiers over X(the domain of the inputs) where the decision boundaryis linear. More broadly, if we were studying, say, neural networks, then wecould letHbe the set of all classiers representable by some neural networkarchitecture.Empirical risk minimization can now be thought of as a minimization overthe class of functions H, in which the learning algorithm picks the hypothesis:^h= arg minh2H^\"(h)8.3.2 The case of nite HLet's start by considering a learning problem in which we have a nite hy-pothesis classH=fh1;:::;hkgconsisting of khypotheses. Thus, His just aset ofkfunctions mapping from Xtof0;1g, and empirical risk minimizationselects ^hto be whichever of these kfunctions has the smallest training error.We would like to give guarantees on the generalization error of ^h. Ourstrategy for doing so will be in two parts: First, we will show that ^ \"(h) is areliable estimate of \"(h) for allh. Second, we will show that this implies anupper-bound on the generalization error of ^h.Take any one, xed, hi2H. Consider a Bernoulli random variable Zwhose distribution is dened as follows. We're going to sample ( x;y)D.Then, we set Z= 1fhi(x)6=yg. I.e., we're going to draw one example,and letZindicate whether himisclassies it. Similarly, we also dene Zj=1fhi(x(j))6=y(j)g. Since our training set was drawn iid from D,Zand theZj's have the same distribution.We see that the misclassication probability on a randomly drawnexample|that is, \"(h)|is exactly the expected value of Z(andZj). More-over, the training error can be written^\"(hi) =1nnXj=1Zj:Thus, ^\"(hi) is exactly the mean of the nrandom variables Zjthat are drawniid from a Bernoulli distribution with mean \"(hi). Hence, we can apply the"
        ],
        "formulas": [
            "H=fh:h(x)",
            "h= arg",
            "classH=fh1;:::;hkgconsisting",
            "Z= 1fhi(x)6=yg.",
            "Zj=1fhi(x(j))6=y(j)g.",
            "1nnXj=1Zj:Thus,"
        ],
        "explanations": []
    },
    "page_130": {
        "content_preview": "129Hoeding inequality, and obtainP(j\"(hi)^\"(hi)j>)2 exp(22n):This shows that, for our particular hi, training error will be close togeneralization error with high probability, assuming nis large. But we don'tjust want to guarantee that \"(hi) will be close to ^ \"(hi) (with high probability)for just only one particular hi. We want to prove that this will be truesimultaneously for allh2H. To do so, let Aidenote the event that j\"(hi)^\"(hi)j> . We've already shown that, for any particular Ai, it hold...",
        "python_code": [
            "129Hoeding inequality, and obtainP(j\"(hi)^\"(hi)j>)2 exp(22n):This shows that, for our particular hi, training error will be close togeneralization error with high probability, assuming nis large. But we don'tjust want to guarantee that \"(hi) will be close to ^ \"(hi) (with high probability)for just only one particular hi. We want to prove that this will be truesimultaneously for allh2H. To do so, let Aidenote the event that j\"(hi)^\"(hi)j> . We've already shown that, for any particular Ai, it holds truethatP(Ai)2 exp(22n). Thus, using the union bound, we have thatP(9h2H:j\"(hi)^\"(hi)j>) =P(A1[[Ak)kXi=1P(Ai)kXi=12 exp(22n)= 2kexp(22n)If we subtract both sides from 1, we nd thatP(:9h2H:j\"(hi)^\"(hi)j>) =P(8h2H:j\"(hi)^\"(hi)j)12kexp(22n)(The \\:\" symbol means \\not.\") So, with probability at least 1 2kexp(22n), we have that \"(h) will be within of ^\"(h) for allh2H.This is called a uniform convergence result, because this is a bound thatholds simultaneously for all (as opposed to just one) h2H.In the discussion above, what we did was, for particular values of nand, give a bound on the probability that for some h2H,j\"(h)^\"(h)j> .There are three quantities of interest here: n,, and the probability of error;we can bound either one in terms of the other two.For instance, we can ask the following question: Given and some>0,how large must nbe before we can guarantee that with probability at least1, training error will be within of generalization error? By setting= 2kexp(22n) and solving for n, [you should convince yourself this isthe right thing to do!], we nd that ifn122log2k;"
        ],
        "formulas": [
            "kXi=1P(Ai)kXi=12",
            "setting= 2kexp(22n)"
        ],
        "explanations": []
    },
    "page_131": {
        "content_preview": "130then with probability at least 1 , we have thatj\"(h)^\"(h)jfor allh2H. (Equivalently, this shows that the probability that j\"(h)^\"(h)j>for someh2 H is at most .) This bound tells us how many trainingexamples we need in order make a guarantee. The training set size nthata certain method or algorithm requires in order to achieve a certain level ofperformance is also called the algorithm's sample complexity .The key property of the bound above is that the number of trainingexamples needed to make...",
        "python_code": [
            "130then with probability at least 1 , we have thatj\"(h)^\"(h)jfor allh2H. (Equivalently, this shows that the probability that j\"(h)^\"(h)j>for someh2 H is at most .) This bound tells us how many trainingexamples we need in order make a guarantee. The training set size nthata certain method or algorithm requires in order to achieve a certain level ofperformance is also called the algorithm's sample complexity .The key property of the bound above is that the number of trainingexamples needed to make this guarantee is only logarithmic ink, the numberof hypotheses inH. This will be important later.Similarly, we can also hold nandxed and solve for in the previousequation, and show [again, convince yourself that this is right!] that withprobability 1, we have that for all h2H,j^\"(h)\"(h)jr12nlog2k:Now, let's assume that uniform convergence holds, i.e., that j\"(h)^\"(h)jfor allh2H. What can we prove about the generalization of our learningalgorithm that picked ^h= arg min h2H^\"(h)?Deneh= arg min h2H\"(h) to be the best possible hypothesis in H. Notethathis the best that we could possibly do given that we are using H, soit makes sense to compare our performance to that of h. We have:\"(^h)^\"(^h) +^\"(h) +\"(h) + 2The rst line used the fact that j\"(^h)^\"(^h)j(by our uniform convergenceassumption). The second used the fact that ^hwas chosen to minimize ^ \"(h),and hence ^\"(^h)^\"(h) for allh, and in particular ^ \"(^h)^\"(h). The thirdline used the uniform convergence assumption again, to show that ^ \"(h)\"(h) +. So, what we've shown is the following: If uniform convergenceoccurs, then the generalization error of ^his at most 2 worse than the bestpossible hypothesis in H!Let's put all this together into a theorem.Theorem. LetjHj=k, and let any n;be xed. Then with probability atleast 1, we have that\"(^h)minh2H\"(h)+ 2r12nlog2k:"
        ],
        "formulas": [
            "h= arg",
            "Deneh= arg",
            "LetjHj=k,"
        ],
        "explanations": []
    },
    "page_132": {
        "content_preview": "131This is proved by letting equal thepterm, using our previous argu-ment that uniform convergence occurs with probability at least 1 , andthen noting that uniform convergence implies \"(h) is at most 2 higher than\"(h) = minh2H\"(h) (as we showed previously).This also quanties what we were saying previously saying about thebias/variance tradeo in model selection. Specically, suppose we have somehypothesis classH, and are considering switching to some much larger hy-pothesis classH0H . If we switch...",
        "python_code": [
            "131This is proved by letting equal thepterm, using our previous argu-ment that uniform convergence occurs with probability at least 1 , andthen noting that uniform convergence implies \"(h) is at most 2 higher than\"(h) = minh2H\"(h) (as we showed previously).This also quanties what we were saying previously saying about thebias/variance tradeo in model selection. Specically, suppose we have somehypothesis classH, and are considering switching to some much larger hy-pothesis classH0H . If we switch to H0, then the rst term min h\"(h)can only decrease (since we'd then be taking a min over a larger set of func-tions). Hence, by learning using a larger hypothesis class, our \\bias\" canonly decrease. However, if k increases, then the second 2pterm would alsoincrease. This increase corresponds to our \\variance\" increasing when we usea larger hypothesis class.By holding andxed and solving for nlike we did before, we can alsoobtain the following sample complexity bound:Corollary. LetjHj=k, and let any ;be xed. Then for \"(^h)minh2H\"(h) + 2to hold with probability at least 1 , it suces thatn122log2k=O12logk;8.3.3 The case of innite HWe have proved some useful theorems for the case of nite hypothesis classes.But many hypothesis classes, including any parameterized by real numbers(as in linear classication) actually contain an innite number of functions.Can we prove similar results for this setting?Let's start by going through something that is notthe \\right\" argument.Better and more general arguments exist , but this will be useful for honingour intuitions about the domain.Suppose we have an Hthat is parameterized by dreal numbers. Since weare using a computer to represent real numbers, and IEEE double-precisionoating point ( double 's in C) uses 64 bits to represent a oating point num-ber, this means that our learning algorithm, assuming we're using double-precision oating point, is parameterized by 64 dbits. Thus, our hypothesisclass really consists of at most k= 264ddierent hypotheses. From the Corol-lary at the end of the previous section, we therefore nd that, to guarantee"
        ],
        "formulas": [
            "LetjHj=k,",
            "thatn122log2k=O12logk;8.3.3",
            "k= 264ddierent"
        ],
        "explanations": []
    },
    "page_133": {
        "content_preview": "132\"(^h)\"(h)+2, with to hold with probability at least 1 , it suces thatnO12log264d=Od2log1=O;(d). (The;subscripts indicatethat the last big- Ois hiding constants that may depend on and.) Thus,the number of training examples needed is at most linear in the parametersof the model.The fact that we relied on 64-bit oating point makes this argument notentirely satisfying, but the conclusion is nonetheless roughly correct: If whatwe try to do is minimize training error, then in order to learn \\well\" ...",
        "python_code": [
            "132\"(^h)\"(h)+2, with to hold with probability at least 1 , it suces thatnO12log264d=Od2log1=O;(d). (The;subscripts indicatethat the last big- Ois hiding constants that may depend on and.) Thus,the number of training examples needed is at most linear in the parametersof the model.The fact that we relied on 64-bit oating point makes this argument notentirely satisfying, but the conclusion is nonetheless roughly correct: If whatwe try to do is minimize training error, then in order to learn \\well\" using ahypothesis class that has dparameters, generally we're going to need on theorder of a linear number of training examples in d.(At this point, it's worth noting that these results were proved for an al-gorithm that uses empirical risk minimization. Thus, while the linear depen-dence of sample complexity on ddoes generally hold for most discriminativelearning algorithms that try to minimize training error or some approxima-tion to training error, these conclusions do not always apply as readily todiscriminative learning algorithms. Giving good theoretical guarantees onmany non-ERM learning algorithms is still an area of active research.)The other part of our previous argument that's slightly unsatisfying isthat it relies on the parameterization of H. Intuitively, this doesn't seem likeit should matter: We had written the class of linear classiers as h(x) =1f0+1x1+dxd0g, withn+ 1 parameters 0;:::;d. But it couldalso be written hu;v(x) = 1f(u20v20) + (u21v21)x1+(u2dv2d)xd0gwith 2d+ 2 parameters ui;vi. Yet, both of these are just dening the sameH: The set of linear classiers in ddimensions.To derive a more satisfying argument, let's dene a few more things.Given a set S=fx(i);:::;x(D)g(no relation to the training set) of pointsx(i)2X, we say thatHshattersSifHcan realize any labeling on S.I.e., if for any set of labels fy(1);:::;y(D)g, there exists some h2H so thath(x(i)) =y(i)for alli= 1;:::D.Given a hypothesis class H, we then dene its Vapnik-Chervonenkisdimension , written VC(H), to be the size of the largest set that is shatteredbyH. (IfHcan shatter arbitrarily large sets, then VC( H) =1.)For instance, consider the following set of three points:"
        ],
        "formulas": [
            "thatnO12log264d=Od2log1=O;(d).",
            "S=fx(i);:::;x(D)g(no",
            "alli= 1;:::D.Given"
        ],
        "explanations": []
    },
    "page_134": {
        "content_preview": "133/0 /1/0 /1/0 /1xx12Can the setHof linear classiers in two dimensions ( h(x) = 1f0+1x1+2x20g) can shatter the set above? The answer is yes. Specically, wesee that, for any of the eight possible labelings of these points, we can nd alinear classier that obtains \\zero training error\" on them:xx12 xx12 xx12 xx12xx12 xx12 xx12 xx12Moreover, it is possible to show that there is no set of 4 points that thishypothesis class can shatter. Thus, the largest set that Hcan shatter is ofsize 3, and hence V...",
        "python_code": [
            "133/0 /1/0 /1/0 /1xx12Can the setHof linear classiers in two dimensions ( h(x) = 1f0+1x1+2x20g) can shatter the set above? The answer is yes. Specically, wesee that, for any of the eight possible labelings of these points, we can nd alinear classier that obtains \\zero training error\" on them:xx12 xx12 xx12 xx12xx12 xx12 xx12 xx12Moreover, it is possible to show that there is no set of 4 points that thishypothesis class can shatter. Thus, the largest set that Hcan shatter is ofsize 3, and hence VC( H) = 3.Note that the VC dimension of Hhere is 3 even though there may besets of size 3 that it cannot shatter. For instance, if we had a set of threepoints lying in a straight line (left gure), then there is no way to nd a linearseparator for the labeling of the three points shown below (right gure):"
        ],
        "formulas": [],
        "explanations": []
    },
    "page_135": {
        "content_preview": "134xx12/0 /1/0 /1/0 /1xx12In order words, under the denition of the VC dimension, in order toprove that VC(H) is at least D, we need to show only that there's at leastoneset of size DthatHcan shatter.The following theorem, due to Vapnik, can then be shown. (This is, manywould argue, the most important theorem in all of learning theory.)Theorem. LetHbe given, and let D= VC(H). Then with probability atleast 1, we have that for all h2H,j\"(h)^\"(h)jO rDnlognD+1nlog1!:Thus, with probability at least 1...",
        "python_code": [
            "134xx12/0 /1/0 /1/0 /1xx12In order words, under the denition of the VC dimension, in order toprove that VC(H) is at least D, we need to show only that there's at leastoneset of size DthatHcan shatter.The following theorem, due to Vapnik, can then be shown. (This is, manywould argue, the most important theorem in all of learning theory.)Theorem. LetHbe given, and let D= VC(H). Then with probability atleast 1, we have that for all h2H,j\"(h)^\"(h)jO rDnlognD+1nlog1!:Thus, with probability at least 1 , we also have that:\"(^h)\"(h) +O rDnlognD+1nlog1!:In other words, if a hypothesis class has nite VC dimension, then uniformconvergence occurs as nbecomes large. As before, this allows us to give abound on\"(h) in terms of \"(h). We also have the following corollary:Corollary. Forj\"(h)^\"(h)jto hold for all h2H (and hence \"(^h)\"(h) + 2) with probability at least 1 , it suces that n=O;(D).In other words, the number of training examples needed to learn \\well\"usingHis linear in the VC dimension of H. It turns out that, for \\most\"hypothesis classes, the VC dimension (assuming a \\reasonable\" parameter-ization) is also roughly linear in the number of parameters. Putting thesetogether, we conclude that for a given hypothesis class H(and for an algo-rithm that tries to minimize training error), the number of training examplesneeded to achieve generalization error close to that of the optimal classieris usually roughly linear in the number of parameters of H."
        ],
        "formulas": [
            "D= VC(H).",
            "n=O;(D).In"
        ],
        "explanations": []
    },
    "page_136": {
        "content_preview": "Chapter 9Regularization and modelselection9.1 RegularizationRecall that as discussed in Section 8.1, overftting is typically a result of usingtoo complex models, and we need to choose a proper model complexity toachieve the optimal bias-variance tradeo. When the model complexity ismeasured by the number of parameters, we can vary the size of the model(e.g., the width of a neural net). However, the correct, informative complex-ity measure of the models can be a function of the parameters (e.g., `...",
        "python_code": [
            "Chapter 9Regularization and modelselection9.1 RegularizationRecall that as discussed in Section 8.1, overftting is typically a result of usingtoo complex models, and we need to choose a proper model complexity toachieve the optimal bias-variance tradeo. When the model complexity ismeasured by the number of parameters, we can vary the size of the model(e.g., the width of a neural net). However, the correct, informative complex-ity measure of the models can be a function of the parameters (e.g., `2normof the parameters), which may not necessarily depend on the number of pa-rameters. In such cases, we will use regularization, an important techniquein machine learning, control the model complexity and prevent overtting.Regularization typically involves adding an additional term, called a reg-ularizer and denoted by R() here, to the training loss/cost function:J() =J() +R() (9.1)HereJis often called the regularized loss, and 0 is called the regular-ization parameter. The regularizer R() is a nonnegative function (in almostall cases). In classical methods, R() is purely a function of the parameter ,but some modern approach allows R() to depend on the training dataset.1The regularizer R() is typically chosen to be some measure of the com-plexity of the model . Thus, when using the regularized loss, we aim tond a model that both t the data (a small loss J()) and have a small1Here our notations generally omit the dependency on the training dataset forsimplicity|we write J() even though it obviously needs to depend on the training dataset.135"
        ],
        "formulas": [],
        "explanations": []
    },
    "page_137": {
        "content_preview": "136model complexity (a small R()). The balance between the two objectives iscontrolled by the regularization parameter . When= 0, the regularizedloss is equivalent to the original loss. When is a suciently small positivenumber, minimizing the regularized loss is eectively minimizing the originalloss with the regularizer as the tie-breaker. When the regularizer is extremelylarge, then the original loss is not eective (and likely the model will have alarge bias.)The most commonly used regularizati...",
        "python_code": [
            "136model complexity (a small R()). The balance between the two objectives iscontrolled by the regularization parameter . When= 0, the regularizedloss is equivalent to the original loss. When is a suciently small positivenumber, minimizing the regularized loss is eectively minimizing the originalloss with the regularizer as the tie-breaker. When the regularizer is extremelylarge, then the original loss is not eective (and likely the model will have alarge bias.)The most commonly used regularization is perhaps `2regularization,whereR() =12kk22. It encourages the optimizer to nd a model withsmall`2norm. In deep learning, it's oftentimes referred to as weight de-cay, because gradient descent with learning rate on the regularized lossR() is equivalent to shrinking/decaying by a scalar factor of 1 andthen applying the standard gradient rJ() =rJ()= (1)|{z}decaying weightsrJ() (9.2)Besides encouraging simpler models, regularization can also impose in-ductive biases or structures on the model parameters. For example, supposewe had a prior belief that the number of non-zeros in the ground-truth modelparameters is small,2|which is oftentimes called sparsity of the model|, wecan impose a regularization on the number of non-zeros in , denoted bykk0, to leverage such a prior belief. Imposing additional structure of theparameters narrows our search space and makes the complexity of the modelfamily smaller,|e.g., the family of sparse models can be thought of as havinglower complexity than the family of all models|, and thus tends to lead to abetter generalization. On the other hand, imposing additional structure mayrisk increasing the bias. For example, if we regularize the sparsity stronglybut no sparse models can predict the label accurately, we will suer fromlarge bias (analogously to the situation when we use linear models to learndata than can only be represented by quadratic functions in Section 8.1.)The sparsity of the parameters is not a continuous function of the param-eters, and thus we cannot optimize it with (stochastic) gradient descent. Acommon relaxation is to use R() =kk1as a continuous surrogate.32For linear models, this means the model just uses a few coordinates of the inputs tomake an accurate prediction.3There has been a rich line of theoretical work that explains why kk1is a good sur-rogate for encouraging sparsity, but it's beyond the scope of this course. An intuition is:assuming the parameter is on the unit sphere, the parameter with smallest `1norm also"
        ],
        "formulas": [
            "When= 0,"
        ],
        "explanations": []
    },
    "page_138": {
        "content_preview": "137TheR() =kk1(also called LASSO) and R() =12kk22are perhapsamong the most commonly used regularizers for linear models. Other normand powers of norms are sometimes also used. The `2norm regularization ismuch more commonly used with kernel methods because `1regularization istypically not compatible with the kernel trick (the optimal solution cannotbe written as functions of inner products of features.)In deep learning, the most commonly used regularizer is `2regularizationor weight decay. Other ...",
        "python_code": [
            "137TheR() =kk1(also called LASSO) and R() =12kk22are perhapsamong the most commonly used regularizers for linear models. Other normand powers of norms are sometimes also used. The `2norm regularization ismuch more commonly used with kernel methods because `1regularization istypically not compatible with the kernel trick (the optimal solution cannotbe written as functions of inner products of features.)In deep learning, the most commonly used regularizer is `2regularizationor weight decay. Other common ones include dropout, data augmentation,regularizing the spectral norm of the weight matrices, and regularizing theLipschitzness of the model, etc. Regularization in deep learning is an ac-tive research area, and it's known that there is another implicit source ofregularization, as discussed in the next section.9.2 Implicit regularization eect (optionalreading)The implicit regularization eect of optimizers, or implicit bias or algorithmicregularization, is a new concept/phenomenon observed in the deep learningera. It largely refers to that the optimizers can implicitly impose structureson parameters beyond what has been imposed by the regularized loss.In most classical settings, the loss or regularized loss has a unique globalminimum, and thus any reasonable optimizer should converge to that globalminimum and cannot impose any additional preferences. However, in deeplearning, oftentimes the loss or regularized loss has more than one (approx-imate) global minima, and dierence optimizers may converge to dierentglobal minima. Though these global minima have the same or similar train-ing losses, they may be of dierent nature and have dramatically dierentgeneralization performance. See Figures 9.1 and 9.2 and its caption for anillustration and some experiment results. For example, it's possible that oneglobal minimum gives a much more Lipschitz or sparse model than othersand thus has a better test error. It turns out that many commonly-used op-timizers (or their components) prefer or bias towards nding global minimaof certain properties, leading to a better test performance.happen to be the sparsest parameter with only 1 non-zero coordinate. Thus, sparsity and`1norm gives the same extremal points to some extent."
        ],
        "formulas": [],
        "explanations": []
    },
    "page_139": {
        "content_preview": "138θlossFigure 9.1: An Illustration that dierent global minima of the training losscan have dierent test performance.Figure 9.2: Left: Performance of neural networks trained by two dierentlearning rates schedules on the CIFAR-10 dataset. Although both exper-iments used exactly the same regularized losses and the optimizers t thetraining data perfectly, the models' generalization performance dier much.Right: On a dierent synthetic dataset, optimizers with dierent initializa-tions have the same tr...",
        "python_code": [],
        "formulas": [],
        "explanations": [
            "1: An Illustration that dierent global minima of the training losscan have dierent test performance",
            "2: Left: Performance of neural networks trained by two dierentlearning rates schedules on the CIFAR-10 dataset",
            "Although both exper-iments used exactly the same regularized losses and the optimizers t thetraining data perfectly, the models' generalization performance dier much",
            "Right: On a dierent synthetic dataset, optimizers with dierent initializa-tions have the same training error but dierent generalization performance",
            "4In summary, the takehome message here is that the choice of optimizerdoes not only aect minimizing the training loss, but also imposes implicitregularization and aects the generalization of the model",
            "Even if your cur-rent optimizer already converges to a small training error perfectly, you maystill need to tune your optimizer for a better generalization,",
            "4The setting is the same as in Woodworth et al",
            "[2020], HaoChen et al"
        ]
    },
    "page_140": {
        "content_preview": "139One may wonder which components of the optimizers bias towards whattype of global minima and what type of global minima may generalize bet-ter. These are open questions that researchers are actively investigating.Empirical and theoretical research have oered some clues and heuristics.In many (but denitely far from all) situations, among those setting whereoptimization can succeed in minimizing the training loss, the use of largerinitial learning rate, smaller initialization, smaller batch siz...",
        "python_code": [
            "139One may wonder which components of the optimizers bias towards whattype of global minima and what type of global minima may generalize bet-ter. These are open questions that researchers are actively investigating.Empirical and theoretical research have oered some clues and heuristics.In many (but denitely far from all) situations, among those setting whereoptimization can succeed in minimizing the training loss, the use of largerinitial learning rate, smaller initialization, smaller batch size, and momen-tum appears to help with biasing towards more generalizable solutions. Aconjecture (that can be proven in certain simplied case) is that stochas-ticity in the optimization process help the optimizer to nd atter globalminima (global minima where the curvature of the loss is small), and atglobal minima tend to give more Lipschitz models and better generalization.Characterizing the implicit regularization eect formally is still a challengingopen research question.9.3 Model selection via cross validationSuppose we are trying select among several dierent models for a learningproblem. For instance, we might be using a polynomial regression modelh(x) =g(0+1x+2x2++kxk), and wish to decide if kshould be0, 1, . . . , or 10. How can we automatically select a model that representsa good tradeo between the twin evils of bias and variance5? Alternatively,suppose we want to automatically choose the bandwidth parameter forlocally weighted regression, or the parameter Cfor our`1-regularized SVM.How can we do that?For the sake of concreteness, in these notes we assume we have somenite set of models M=fM1;:::;Mdgthat we're trying to select among.For instance, in our rst example above, the model Miwould be an i-thdegree polynomial regression model. (The generalization to innite Misnot hard.6) Alternatively, if we are trying to decide between using an SVM,a neural network or logistic regression, then Mmay contain these models.5Given that we said in the previous set of notes that bias and variance are two verydierent beasts, some readers may be wondering if we should be calling them \\twin\" evilshere. Perhaps it'd be better to think of them as non-identical twins. The phrase \\thefraternal twin evils of bias and variance\" doesn't have the same ring to it, though.6If we are trying to choose from an innite set of models, say corresponding to thepossible values of the bandwidth 2R+, we may discretize and consider only a nitenumber of possible values for it. More generally, most of the algorithms described herecan all be viewed as performing optimization search in the space of models, and we canperform this search over innite model classes as well."
        ],
        "formulas": [
            "M=fM1;:::;Mdgthat"
        ],
        "explanations": []
    },
    "page_141": {
        "content_preview": "140Cross validation. Lets suppose we are, as usual, given a training set S.Given what we know about empirical risk minimization, here's what mightinitially seem like a algorithm, resulting from using empirical risk minimiza-tion for model selection:1. Train each model MionS, to get some hypothesis hi.2. Pick the hypotheses with the smallest training error.This algorithm does notwork. Consider choosing the degree of a poly-nomial. The higher the degree of the polynomial, the better it will t thet...",
        "python_code": [
            "140Cross validation. Lets suppose we are, as usual, given a training set S.Given what we know about empirical risk minimization, here's what mightinitially seem like a algorithm, resulting from using empirical risk minimiza-tion for model selection:1. Train each model MionS, to get some hypothesis hi.2. Pick the hypotheses with the smallest training error.This algorithm does notwork. Consider choosing the degree of a poly-nomial. The higher the degree of the polynomial, the better it will t thetraining set S, and thus the lower the training error. Hence, this method willalways select a high-variance, high-degree polynomial model, which we sawpreviously is often poor choice.Here's an algorithm that works better. In hold-out cross validation(also called simple cross validation ), we do the following:1. Randomly split SintoStrain(say, 70% of the data) and Scv(the remain-ing 30%). Here, Scvis called the hold-out cross validation set.2. Train each model MionStrainonly, to get some hypothesis hi.3. Select and output the hypothesis hithat had the smallest error ^ \"Scv(hi)on the hold out cross validation set. (Here ^ \"Scv(h) denotes the averageerror ofhon the set of examples in Scv.) The error on the hold outvalidation set is also referred to as the validation error.By testing/validating on a set of examples Scvthat the models were nottrained on, we obtain a better estimate of each hypothesis hi's true general-ization/test error. Thus, this approach is essentially picking the model withthe smallest estimated generalization/test error. The size of the validationset depends on the total number of available examples. Usually, somewherebetween 1=41=3 of the data is used in the hold out cross validation set, and30% is a typical choice. However, when the total dataset is huge, validationset can be a smaller fraction of the total examples as long as the absolutenumber of validation examples is decent. For example, for the ImageNetdataset that has about 1M training images, the validation set is sometimesset to be 50K images, which is only about 5% of the total examples.Optionally, step 3 in the algorithm may also be replaced with selectingthe modelMiaccording to arg min i^\"Scv(hi), and then retraining Mion theentire training set S. (This is often a good idea, with one exception beinglearning algorithms that are be very sensitive to perturbations of the initial"
        ],
        "formulas": [
            "1=41=3"
        ],
        "explanations": []
    },
    "page_142": {
        "content_preview": "141conditions and/or data. For these methods, Midoing well on Straindoes notnecessarily mean it will also do well on Scv, and it might be better to forgothis retraining step.)The disadvantage of using hold out cross validation is that it \\wastes\"about 30% of the data. Even if we were to take the optional step of retrainingthe model on the entire training set, it's still as if we're trying to nd a goodmodel for a learning problem in which we had 0 :7ntraining examples, ratherthanntraining example...",
        "python_code": [
            "141conditions and/or data. For these methods, Midoing well on Straindoes notnecessarily mean it will also do well on Scv, and it might be better to forgothis retraining step.)The disadvantage of using hold out cross validation is that it \\wastes\"about 30% of the data. Even if we were to take the optional step of retrainingthe model on the entire training set, it's still as if we're trying to nd a goodmodel for a learning problem in which we had 0 :7ntraining examples, ratherthanntraining examples, since we're testing models that were trained ononly 0:7nexamples each time. While this is ne if data is abundant and/orcheap, in learning problems in which data is scarce (consider a problem withn= 20, say), we'd like to do something better.Here is a method, called k-fold cross validation , that holds out lessdata each time:1. Randomly split Sintokdisjoint subsets of m=k training examples each.Lets call these subsets S1;:::;Sk.2. For each model Mi, we evaluate it as follows:Forj= 1;:::;kTrain the model MionS1[[Sj1[Sj+1[Sk(i.e., trainon all the data except Sj) to get some hypothesis hij.Test the hypothesis hijonSj, to get ^\"Sj(hij).The estimated generalization error of model Miis then calculatedas the average of the ^ \"Sj(hij)'s (averaged over j).3. Pick the model Miwith the lowest estimated generalization error, andretrain that model on the entire training set S. The resulting hypothesisis then output as our nal answer.A typical choice for the number of folds to use here would be k= 10.While the fraction of data held out each time is now 1 =k|much smallerthan before|this procedure may also be more computationally expensivethan hold-out cross validation, since we now need train to each model ktimes.Whilek= 10 is a commonly used choice, in problems in which data isreally scarce, sometimes we will use the extreme choice of k=min orderto leave out as little data as possible each time. In this setting, we wouldrepeatedly train on all but one of the training examples in S, and test on thatheld-out example. The resulting m=kerrors are then averaged together toobtain our estimate of the generalization error of a model. This method has"
        ],
        "formulas": [
            "withn= 20,",
            "m=k",
            "Forj= 1;:::;kTrain",
            "k= 10.While",
            "1 =k|much",
            "Whilek= 10",
            "k=min",
            "m=kerrors"
        ],
        "explanations": []
    },
    "page_143": {
        "content_preview": "142its own name; since we're holding out one training example at a time, thismethod is called leave-one-out cross validation.Finally, even though we have described the dierent versions of cross vali-dation as methods for selecting a model, they can also be used more simply toevaluate a single model or algorithm. For example, if you have implementedsome learning algorithm and want to estimate how well it performs for yourapplication (or if you have invented a novel learning algorithm and want tor...",
        "python_code": [
            "142its own name; since we're holding out one training example at a time, thismethod is called leave-one-out cross validation.Finally, even though we have described the dierent versions of cross vali-dation as methods for selecting a model, they can also be used more simply toevaluate a single model or algorithm. For example, if you have implementedsome learning algorithm and want to estimate how well it performs for yourapplication (or if you have invented a novel learning algorithm and want toreport in a technical paper how well it performs on various test sets), crossvalidation would give a reasonable way of doing so.9.4 Bayesian statistics and regularizationIn this section, we will talk about one more tool in our arsenal for our battleagainst overtting.At the beginning of the quarter, we talked about parameter tting usingmaximum likelihood estimation (MLE), and chose our parameters accordingtoMLE= arg maxnYi=1p(y(i)jx(i);):Throughout our subsequent discussions, we viewed as an unknown param-eter of the world. This view of the as being constant-valued but unknownis taken in frequentist statistics. In the frequentist this view of the world, is not random|it just happens to be unknown|and it's our job to come upwith statistical procedures (such as maximum likelihood) to try to estimatethis parameter.An alternative way to approach our parameter estimation problems is totake the Bayesian view of the world, and think of as being a randomvariable whose value is unknown. In this approach, we would specify aprior distribution p() onthat expresses our \\prior beliefs\" about theparameters. Given a training set S=f(x(i);y(i))gni=1, when we are asked tomake a prediction on a new value of x, we can then compute the posteriordistribution on the parametersp(jS) =p(Sj)p()p(S)=Qni=1p(y(i)jx(i);)p()R(Qni=1p(y(i)jx(i);)p())d(9.3)In the equation above, p(y(i)jx(i);) comes from whatever model you're using"
        ],
        "formulas": [
            "accordingtoMLE= arg",
            "maxnYi=1p(y(i)jx(i);):Throughout",
            "S=f(x(i);y(i))gni=1,",
            "Qni=1p(y(i)jx(i);)p()R(Qni=1p(y(i)jx(i);)p())d(9.3)In"
        ],
        "explanations": []
    },
    "page_144": {
        "content_preview": "143for your learning problem. For example, if you are using Bayesian logistic re-gression, then you might choose p(y(i)jx(i);) =h(x(i))y(i)(1h(x(i)))(1y(i)),whereh(x(i)) = 1=(1 + exp(Tx(i))).7When we are given a new test example xand asked to make it predictionon it, we can compute our posterior distribution on the class label using theposterior distribution on :p(yjx;S) =Zp(yjx;)p(jS)d (9.4)In the equation above, p(jS) comes from Equation (9.3). Thus, for example,if the goal is to the predict t...",
        "python_code": [
            "143for your learning problem. For example, if you are using Bayesian logistic re-gression, then you might choose p(y(i)jx(i);) =h(x(i))y(i)(1h(x(i)))(1y(i)),whereh(x(i)) = 1=(1 + exp(Tx(i))).7When we are given a new test example xand asked to make it predictionon it, we can compute our posterior distribution on the class label using theposterior distribution on :p(yjx;S) =Zp(yjx;)p(jS)d (9.4)In the equation above, p(jS) comes from Equation (9.3). Thus, for example,if the goal is to the predict the expected value of ygivenx, then we wouldoutput8E[yjx;S] =Zyyp(yjx;S)dyThe procedure that we've outlined here can be thought of as doing \\fullyBayesian\" prediction, where our prediction is computed by taking an averagewith respect to the posterior p(jS) over. Unfortunately, in general it iscomputationally very dicult to compute this posterior distribution. This isbecause it requires taking integrals over the (usually high-dimensional) asin Equation (9.3), and this typically cannot be done in closed-form.Thus, in practice we will instead approximate the posterior distributionfor. One common approximation is to replace our posterior distribution for(as in Equation 9.4) with a single point estimate. The MAP (maximuma posteriori) estimate for is given byMAP= arg maxnYi=1p(y(i)jx(i);)p(): (9.5)Note that this is the same formulas as for the MLE (maximum likelihood)estimate for , except for the prior p() term at the end.In practical applications, a common choice for the prior p() is to assumethatN(0;2I). Using this choice of prior, the tted parameters MAPwillhave smaller norm than that selected by maximum likelihood. In practice,this causes the Bayesian MAP estimate to be less susceptible to overttingthan the ML estimate of the parameters. For example, Bayesian logisticregression turns out to be an eective algorithm for text classication, eventhough in text classication we usually have dn.7Since we are now viewing as a random variable, it is okay to condition on it value,and write \\ p(yjx;)\" instead of \\ p(yjx;).\"8The integral below would be replaced by a summation if yis discrete-valued."
        ],
        "formulas": [
            "1=(1",
            "byMAP= arg",
            "maxnYi=1p(y(i)jx(i);)p():"
        ],
        "explanations": []
    },
    "page_145": {
        "content_preview": "Part IVUnsupervised learning144...",
        "python_code": [],
        "formulas": [],
        "explanations": [
            "Part IVUnsupervised learning144"
        ]
    },
    "page_146": {
        "content_preview": "Chapter 10Clustering and the k-meansalgorithmIn the clustering problem, we are given a training set fx(1);:::;x(n)g, andwant to group the data into a few cohesive \\clusters.\" Here, x(i)2Rdas usual; but no labels y(i)are given. So, this is an unsupervised learningproblem.Thek-means clustering algorithm is as follows:1. Initialize cluster centroids 1;2;:::;k2Rdrandomly.2. Repeat until convergence: fFor everyi, setc(i):= arg minjjjx(i)jjj2:For eachj, setj:=Pni=11fc(i)=jgx(i)Pni=11fc(i)=jg:gIn the a...",
        "python_code": [
            "Chapter 10Clustering and the k-meansalgorithmIn the clustering problem, we are given a training set fx(1);:::;x(n)g, andwant to group the data into a few cohesive \\clusters.\" Here, x(i)2Rdas usual; but no labels y(i)are given. So, this is an unsupervised learningproblem.Thek-means clustering algorithm is as follows:1. Initialize cluster centroids 1;2;:::;k2Rdrandomly.2. Repeat until convergence: fFor everyi, setc(i):= arg minjjjx(i)jjj2:For eachj, setj:=Pni=11fc(i)=jgx(i)Pni=11fc(i)=jg:gIn the algorithm above, k(a parameter of the algorithm) is the numberof clusters we want to nd; and the cluster centroids jrepresent our currentguesses for the positions of the centers of the clusters. To initialize the clustercentroids (in step 1 of the algorithm above), we could choose ktrainingexamples randomly, and set the cluster centroids to be equal to the values ofthesekexamples. (Other initialization methods are also possible.)The inner-loop of the algorithm repeatedly carries out two steps: (i)\\Assigning\" each training example x(i)to the closest cluster centroid j, and145"
        ],
        "formulas": [
            "Pni=11fc(i)=jgx(i)Pni=11fc(i)=jg:gIn"
        ],
        "explanations": []
    },
    "page_147": {
        "content_preview": "146Figure 10.1: K-means algorithm. Training examples are shown as dots, andcluster centroids are shown as crosses. (a) Original dataset. (b) Random ini-tial cluster centroids (in this instance, not chosen to be equal to two trainingexamples). (c-f) Illustration of running two iterations of k-means. In eachiteration, we assign each training example to the closest cluster centroid(shown by \\painting\" the training examples the same color as the clustercentroid to which is assigned); then we move ea...",
        "python_code": [
            "146Figure 10.1: K-means algorithm. Training examples are shown as dots, andcluster centroids are shown as crosses. (a) Original dataset. (b) Random ini-tial cluster centroids (in this instance, not chosen to be equal to two trainingexamples). (c-f) Illustration of running two iterations of k-means. In eachiteration, we assign each training example to the closest cluster centroid(shown by \\painting\" the training examples the same color as the clustercentroid to which is assigned); then we move each cluster centroid to themean of the points assigned to it. (Best viewed in color.) Images courtesyMichael Jordan.(ii) Moving each cluster centroid jto the mean of the points assigned to it.Figure 10.1 shows an illustration of running k-means.Is thek-means algorithm guaranteed to converge? Yes it is, in a certainsense. In particular, let us dene the distortion function to be:J(c;) =nXi=1jjx(i)c(i)jj2Thus,Jmeasures the sum of squared distances between each training exam-plex(i)and the cluster centroid c(i)to which it has been assigned. It canbe shown that k-means is exactly coordinate descent on J. Specically, theinner-loop of k-means repeatedly minimizes Jwith respect to cwhile holdingxed, and then minimizes Jwith respect to while holding cxed. Thus,"
        ],
        "formulas": [
            "nXi=1jjx(i)c(i)jj2Thus,Jmeasures"
        ],
        "explanations": []
    },
    "page_148": {
        "content_preview": "147Jmust monotonically decrease, and the value of Jmust converge. (Usu-ally, this implies that candwill converge too. In theory, it is possible fork-means to oscillate between a few dierent clusterings|i.e., a few dierentvalues forcand/or|that have exactly the same value of J, but this almostnever happens in practice.)The distortion function Jis a non-convex function, and so coordinatedescent on Jis not guaranteed to converge to the global minimum. In otherwords,k-means can be susceptible to loc...",
        "python_code": [],
        "formulas": [],
        "explanations": [
            "147Jmust monotonically decrease, and the value of Jmust converge",
            "(Usu-ally, this implies that candwill converge too",
            "In theory, it is possible fork-means to oscillate between a few dierent clusterings|i",
            ", a few dierentvalues forcand/or|that have exactly the same value of J, but this almostnever happens in practice",
            ")The distortion function Jis a non-convex function, and so coordinatedescent on Jis not guaranteed to converge to the global minimum",
            "In otherwords,k-means can be susceptible to local optima",
            "Very often k-means willwork ne and come up with very good clusterings despite this",
            "But if youare worried about getting stuck in bad local minima, one common thing todo is runk-means many times (using dierent random initial values for thecluster centroids j)",
            "Then, out of all the dierent clusterings found, pickthe one that gives the lowest distortion J(c;)"
        ]
    },
    "page_149": {
        "content_preview": "Chapter 11EM algorithmsIn this set of notes, we discuss the EM (Expectation-Maximization) algorithmfor density estimation.11.1 EM for mixture of GaussiansSuppose that we are given a training set fx(1);:::;x(n)gas usual. Since weare in the unsupervised learning setting, these points do not come with anylabels.We wish to model the data by specifying a joint distribution p(x(i);z(i)) =p(x(i)jz(i))p(z(i)). Here,z(i)Multinomial( ) (wherej0,Pkj=1j= 1,and the parameter jgivesp(z(i)=j)), andx(i)jz(i)=jN...",
        "python_code": [
            "Chapter 11EM algorithmsIn this set of notes, we discuss the EM (Expectation-Maximization) algorithmfor density estimation.11.1 EM for mixture of GaussiansSuppose that we are given a training set fx(1);:::;x(n)gas usual. Since weare in the unsupervised learning setting, these points do not come with anylabels.We wish to model the data by specifying a joint distribution p(x(i);z(i)) =p(x(i)jz(i))p(z(i)). Here,z(i)Multinomial( ) (wherej0,Pkj=1j= 1,and the parameter jgivesp(z(i)=j)), andx(i)jz(i)=jN (j;j). Weletkdenote the number of values that the z(i)'s can take on. Thus, ourmodel posits that each x(i)was generated by randomly choosing z(i)fromf1;:::;kg, and then x(i)was drawn from one of kGaussians depending onz(i). This is called the mixture of Gaussians model. Also, note that thez(i)'s are latent random variables, meaning that they're hidden/unobserved.This is what will make our estimation problem dicult.The parameters of our model are thus ,and . To estimate them, wecan write down the likelihood of our data:`(;; ) =nXi=1logp(x(i);;; )=nXi=1logkXz(i)=1p(x(i)jz(i);;)p(z(i);):However, if we set to zero the derivatives of this formula with respect to148"
        ],
        "formulas": [
            "Pkj=1j=",
            "nXi=1logp(x(i);;;",
            "nXi=1logkXz(i)=1p(x(i)jz(i);;)p(z(i);):However,"
        ],
        "explanations": []
    },
    "page_150": {
        "content_preview": "149the parameters and try to solve, we'll nd that it is not possible to nd themaximum likelihood estimates of the parameters in closed form. (Try thisyourself at home.)The random variables z(i)indicate which of the kGaussians each x(i)had come from. Note that if we knew what the z(i)'s were, the maximumlikelihood problem would have been easy. Specically, we could then writedown the likelihood as`(;; ) =nXi=1logp(x(i)jz(i);;) + logp(z(i);):Maximizing this with respect to ,and gives the parameters...",
        "python_code": [
            "149the parameters and try to solve, we'll nd that it is not possible to nd themaximum likelihood estimates of the parameters in closed form. (Try thisyourself at home.)The random variables z(i)indicate which of the kGaussians each x(i)had come from. Note that if we knew what the z(i)'s were, the maximumlikelihood problem would have been easy. Specically, we could then writedown the likelihood as`(;; ) =nXi=1logp(x(i)jz(i);;) + logp(z(i);):Maximizing this with respect to ,and gives the parameters:j=1nnXi=11fz(i)=jg;j=Pni=11fz(i)=jgx(i)Pni=11fz(i)=jg;j=Pni=11fz(i)=jg(x(i)j)(x(i)j)TPni=11fz(i)=jg:Indeed, we see that if the z(i)'s were known, then maximum likelihoodestimation becomes nearly identical to what we had when estimating theparameters of the Gaussian discriminant analysis model, except that herethez(i)'s playing the role of the class labels.1However, in our density estimation problem, the z(i)'s are notknown.What can we do?The EM algorithm is an iterative algorithm that has two main steps.Applied to our problem, in the E-step, it tries to \\guess\" the values of thez(i)'s. In the M-step, it updates the parameters of our model based on ourguesses. Since in the M-step we are pretending that the guesses in the rstpart were correct, the maximization becomes easy. Here's the algorithm:Repeat until convergence: f(E-step) For each i;j, setw(i)j:=p(z(i)=jjx(i);;; )1There are other minor dierences in the formulas here from what we'd obtained inPS1 with Gaussian discriminant analysis, rst because we've generalized the z(i)'s to bemultinomial rather than Bernoulli, and second because here we are using a dierent jfor each Gaussian."
        ],
        "formulas": [
            "nXi=1logp(x(i)jz(i);;)",
            "j=1nnXi=11fz(i)=jg;j=Pni=11fz(i)=jgx(i)Pni=11fz(i)=jg;j=Pni=11fz(i)=jg(x(i)j)(x(i)j)TPni=11fz(i)=jg:Indeed,"
        ],
        "explanations": []
    },
    "page_151": {
        "content_preview": "150(M-step) Update the parameters:j:=1nnXi=1w(i)j;j:=Pni=1w(i)jx(i)Pni=1w(i)j;j:=Pni=1w(i)j(x(i)j)(x(i)j)TPni=1w(i)jgIn the E-step, we calculate the posterior probability of our parametersthez(i)'s, given the x(i)and using the current setting of our parameters. I.e.,using Bayes rule, we obtain:p(z(i)=jjx(i);;; ) =p(x(i)jz(i)=j;;)p(z(i)=j;)Pkl=1p(x(i)jz(i)=l;;)p(z(i)=l;)Here,p(x(i)jz(i)=j;;) is given by evaluating the density of a Gaussianwith mean jand covariance jatx(i);p(z(i)=j;) is given by j...",
        "python_code": [
            "150(M-step) Update the parameters:j:=1nnXi=1w(i)j;j:=Pni=1w(i)jx(i)Pni=1w(i)j;j:=Pni=1w(i)j(x(i)j)(x(i)j)TPni=1w(i)jgIn the E-step, we calculate the posterior probability of our parametersthez(i)'s, given the x(i)and using the current setting of our parameters. I.e.,using Bayes rule, we obtain:p(z(i)=jjx(i);;; ) =p(x(i)jz(i)=j;;)p(z(i)=j;)Pkl=1p(x(i)jz(i)=l;;)p(z(i)=l;)Here,p(x(i)jz(i)=j;;) is given by evaluating the density of a Gaussianwith mean jand covariance jatx(i);p(z(i)=j;) is given by j, and soon. The values w(i)jcalculated in the E-step represent our \\soft\" guesses2forthe values of z(i).Also, you should contrast the updates in the M-step with the formulas wehad when the z(i)'s were known exactly. They are identical, except that in-stead of the indicator functions \\1 fz(i)=jg\" indicating from which Gaussianeach datapoint had come, we now instead have the w(i)j's.The EM-algorithm is also reminiscent of the K-means clustering algo-rithm, except that instead of the \\hard\" cluster assignments c(i), we insteadhave the \\soft\" assignments w(i)j. Similar to K-means, it is also susceptibleto local optima, so reinitializing at several dierent initial parameters maybe a good idea.It's clear that the EM algorithm has a very natural interpretation ofrepeatedly trying to guess the unknown z(i)'s; but how did it come about,and can we make any guarantees about it, such as regarding its convergence?In the next set of notes, we will describe a more general view of EM, one2The term \\soft\" refers to our guesses being probabilities and taking values in [0 ;1]; incontrast, a \\hard\" guess is one that represents a single best guess (such as taking valuesinf0;1gorf1;:::;kg)."
        ],
        "formulas": [
            "1nnXi=1w(i)j;j:=Pni=1w(i)jx(i)Pni=1w(i)j;j:=Pni=1w(i)j(x(i)j)(x(i)j)TPni=1w(i)jgIn",
            "Pkl=1p(x(i)jz(i)=l;;)p(z(i)=l;)Here,p(x(i)jz(i)=j;;)"
        ],
        "explanations": []
    },
    "page_152": {
        "content_preview": "151that will allow us to easily apply it to other estimation problems in whichthere are also latent variables, and which will allow us to give a convergenceguarantee.11.2 Jensen's inequalityWe begin our discussion with a very useful result called Jensen's inequalityLetfbe a function whose domain is the set of real numbers. Recall thatfis a convex function if f00(x)0 (for allx2R). In the case of ftakingvector-valued inputs, this is generalized to the condition that its hessian His positive semi-d...",
        "python_code": [
            "151that will allow us to easily apply it to other estimation problems in whichthere are also latent variables, and which will allow us to give a convergenceguarantee.11.2 Jensen's inequalityWe begin our discussion with a very useful result called Jensen's inequalityLetfbe a function whose domain is the set of real numbers. Recall thatfis a convex function if f00(x)0 (for allx2R). In the case of ftakingvector-valued inputs, this is generalized to the condition that its hessian His positive semi-denite ( H0). Iff00(x)>0 for allx, then we say fisstrictly convex (in the vector-valued case, the corresponding statement isthatHmust be positive denite, written H > 0). Jensen's inequality canthen be stated as follows:Theorem. Letfbe a convex function, and let Xbe a random variable.Then:E[f(X)]f(EX):Moreover, if fis strictly convex, then E[ f(X)] =f(EX) holds true if andonly ifX= E[X] with probability 1 (i.e., if Xis a constant).Recall our convention of occasionally dropping the parentheses when writ-ing expectations, so in the theorem above, f(EX) =f(E[X]).For an interpretation of the theorem, consider the gure below.a E[X] bf(a)f(b)f(EX)E[f(X)]fHere,fis a convex function shown by the solid line. Also, Xis a randomvariable that has a 0.5 chance of taking the value a, and a 0.5 chance of"
        ],
        "formulas": [
            "ifX= E[X]"
        ],
        "explanations": []
    },
    "page_153": {
        "content_preview": "152taking the value b(indicated on the x-axis). Thus, the expected value of Xis given by the midpoint between aandb.We also see the values f(a),f(b) andf(E[X]) indicated on the y-axis.Moreover, the value E[ f(X)] is now the midpoint on the y-axis between f(a)andf(b). From our example, we see that because fis convex, it must be thecase that E[ f(X)]f(EX).Incidentally, quite a lot of people have trouble remembering which waythe inequality goes, and remembering a picture like this is a good way toq...",
        "python_code": [
            "152taking the value b(indicated on the x-axis). Thus, the expected value of Xis given by the midpoint between aandb.We also see the values f(a),f(b) andf(E[X]) indicated on the y-axis.Moreover, the value E[ f(X)] is now the midpoint on the y-axis between f(a)andf(b). From our example, we see that because fis convex, it must be thecase that E[ f(X)]f(EX).Incidentally, quite a lot of people have trouble remembering which waythe inequality goes, and remembering a picture like this is a good way toquickly gure out the answer.Remark. Recall that fis [strictly] concave if and only if fis [strictly]convex (i.e., f00(x)0 orH0). Jensen's inequality also holds for concavefunctionsf, but with the direction of all the inequalities reversed (E[ f(X)]f(EX), etc.).11.3 General EM algorithmsSuppose we have an estimation problem in which we have a training setfx(1);:::;x(n)gconsisting of nindependent examples. We have a latent vari-able model p(x;z;) withzbeing the latent variable (which for simplicity isassumed to take nite number of values). The density for xcan be obtainedby marginalized over the latent variable z:p(x;) =Xzp(x;z;) (11.1)We wish to t the parameters by maximizing the log-likelihood of thedata, dened by`() =nXi=1logp(x(i);) (11.2)We can rewrite the objective in terms of the joint density p(x;z;) by`() =nXi=1logp(x(i);) (11.3)=nXi=1logXz(i)p(x(i);z(i);): (11.4)But, explicitly nding the maximum likelihood estimates of the parametersmay be hard since it will result in dicult non-convex optimization prob-"
        ],
        "formulas": [
            "nXi=1logp(x(i);)",
            "nXi=1logp(x(i);)",
            "nXi=1logXz(i)p(x(i);z(i);):"
        ],
        "explanations": []
    },
    "page_154": {
        "content_preview": "153lems.3Here, thez(i)'s are the latent random variables; and it is often the casethat if the z(i)'s were observed, then maximum likelihood estimation wouldbe easy.In such a setting, the EM algorithm gives an ecient method for max-imum likelihood estimation. Maximizing `() explicitly might be dicult,and our strategy will be to instead repeatedly construct a lower-bound on `(E-step), and then optimize that lower-bound (M-step).4It turns out that the summationPni=1is not essential here, and toward...",
        "python_code": [
            "153lems.3Here, thez(i)'s are the latent random variables; and it is often the casethat if the z(i)'s were observed, then maximum likelihood estimation wouldbe easy.In such a setting, the EM algorithm gives an ecient method for max-imum likelihood estimation. Maximizing `() explicitly might be dicult,and our strategy will be to instead repeatedly construct a lower-bound on `(E-step), and then optimize that lower-bound (M-step).4It turns out that the summationPni=1is not essential here, and towards asimpler exposition of the EM algorithm, we will rst consider optimizing thethe likelihood log p(x) fora single example x. After we derive the algorithmfor optimizing log p(x), we will convert it to an algorithm that works for nexamples by adding back the sum to each of the relevant equations. Thus,now we aim to optimize log p(x;) which can be rewritten aslogp(x;) = logXzp(x;z;) (11.5)LetQbe a distribution over the possible values of z. That is,PzQ(z) = 1,Q(z)0).Consider the following:5logp(x;) = logXzp(x;z;)= logXzQ(z)p(x;z;)Q(z)(11.6)XzQ(z) logp(x;z;)Q(z)(11.7)The last step of this derivation used Jensen's inequality. Specically,f(x) = logxis a concave function, since f00(x) =1=x2<0 over its domain3It's mostly an empirical observation that the optimization problem is dicult to op-timize.4Empirically, the E-step and M-step can often be computed more eciently than op-timizing the function `() directly. However, it doesn't necessarily mean that alternatingthe two steps can always converge to the global optimum of `(). Even for mixture ofGaussians, the EM algorithm can either converge to a global optimum or get stuck, de-pending on the properties of the training data. Empirically, for real-world data, often EMcan converge to a solution with relatively high likelihood (if not the optimum), and thetheory behind it is still largely not understood.5Ifzwere continuous, then Qwould be a density, and the summations over zin ourdiscussion are replaced with integrals over z."
        ],
        "formulas": [
            "summationPni=1is",
            "1=x2<0"
        ],
        "explanations": []
    },
    "page_155": {
        "content_preview": "154x2R+. Also, the termXzQ(z)p(x;z;)Q(z)in the summation is just an expectation of the quantity [ p(x;z;)=Q(z)] withrespect tozdrawn according to the distribution given by Q.6By Jensen'sinequality, we havefEzQp(x;z;)Q(z)EzQfp(x;z;)Q(z);where the \\ zQ\" subscripts above indicate that the expectations are withrespect tozdrawn from Q. This allowed us to go from Equation (11.6) toEquation (11.7).Now, for any distribution Q, the formula (11.7) gives a lower-bound onlogp(x;). There are many possible ch...",
        "python_code": [
            "154x2R+. Also, the termXzQ(z)p(x;z;)Q(z)in the summation is just an expectation of the quantity [ p(x;z;)=Q(z)] withrespect tozdrawn according to the distribution given by Q.6By Jensen'sinequality, we havefEzQp(x;z;)Q(z)EzQfp(x;z;)Q(z);where the \\ zQ\" subscripts above indicate that the expectations are withrespect tozdrawn from Q. This allowed us to go from Equation (11.6) toEquation (11.7).Now, for any distribution Q, the formula (11.7) gives a lower-bound onlogp(x;). There are many possible choices for the Q's. Which should wechoose? Well, if we have some current guess of the parameters, it seemsnatural to try to make the lower-bound tight at that value of . I.e., we willmake the inequality above hold with equality at our particular value of .To make the bound tight for a particular value of , we need for the stepinvolving Jensen's inequality in our derivation above to hold with equality.For this to be true, we know it is sucient that the expectation be takenover a \\constant\"-valued random variable. I.e., we require thatp(x;z;)Q(z)=cfor some constant cthat does not depend on z. This is easily accomplishedby choosingQ(z)/p(x;z;):Actually, since we knowPzQ(z) = 1 (because it is a distribution), thisfurther tells us thatQ(z) =p(x;z;)Pzp(x;z;)=p(x;z;)p(x;)=p(zjx;) (11.8)6We note that the notionp(x;z;)Q(z)only makes sense if Q(z)6= 0 whenever p(x;z;)6= 0.Here we implicitly assume that we only consider those Qwith such a property."
        ],
        "formulas": [
            "6= 0",
            "6= 0.Here"
        ],
        "explanations": []
    },
    "page_156": {
        "content_preview": "155Thus, we simply set the Q's to be the posterior distribution of the z's givenxand the setting of the parameters .Indeed, we can directly verify that when Q(z) =p(zjx;), then equa-tion (11.7) is an equality becauseXzQ(z) logp(x;z;)Q(z)=Xzp(zjx;) logp(x;z;)p(zjx;)=Xzp(zjx;) logp(zjx;)p(x;)p(zjx;)=Xzp(zjx;) logp(x;)= logp(x;)Xzp(zjx;)= logp(x;) (becausePzp(zjx;) = 1)For convenience, we call the expression in Equation (11.7) the evidencelower bound (ELBO) and we denote it byELBO(x;Q;) =XzQ(z) log...",
        "python_code": [
            "155Thus, we simply set the Q's to be the posterior distribution of the z's givenxand the setting of the parameters .Indeed, we can directly verify that when Q(z) =p(zjx;), then equa-tion (11.7) is an equality becauseXzQ(z) logp(x;z;)Q(z)=Xzp(zjx;) logp(x;z;)p(zjx;)=Xzp(zjx;) logp(zjx;)p(x;)p(zjx;)=Xzp(zjx;) logp(x;)= logp(x;)Xzp(zjx;)= logp(x;) (becausePzp(zjx;) = 1)For convenience, we call the expression in Equation (11.7) the evidencelower bound (ELBO) and we denote it byELBO(x;Q;) =XzQ(z) logp(x;z;)Q(z)(11.9)With this equation, we can re-write equation (11.7) as8Q;;x; logp(x;)ELBO(x;Q;) (11.10)Intuitively, the EM algorithm alternatively updates Qandby a) set-tingQ(z) =p(zjx;) following Equation (11.8) so that ELBO( x;Q;) =logp(x;) forxand the current , and b) maximizing ELBO( x;Q;) w.r.twhile xing the choice of Q.Recall that all the discussion above was under the assumption that weaim to optimize the log-likelihood log p(x;) for a single example x. It turnsout that with multiple training examples, the basic idea is the same and weonly needs to take a sum over examples at relevant places. Next, we willbuild the evidence lower bound for multiple training examples and make theEM algorithm formal.Recall we have a training set fx(1);:::;x(n)g. Note that the optimal choiceofQisp(zjx;), and it depends on the particular example x. Therefore herewe will introduce ndistributions Q1;:::;Qn, one for each example x(i). Foreach example x(i), we can build the evidence lower boundlogp(x(i);)ELBO(x(i);Qi;) =Xz(i)Qi(z(i)) logp(x(i);z(i);)Qi(z(i))"
        ],
        "formulas": [],
        "explanations": []
    },
    "page_157": {
        "content_preview": "156Taking sum over all the examples, we obtain a lower bound for the log-likelihood`()XiELBO(x(i);Qi;) (11.11)=XiXz(i)Qi(z(i)) logp(x(i);z(i);)Qi(z(i))Foranyset of distributions Q1;:::;Qn, the formula (11.11) gives a lower-bound on`(), and analogous to the argument around equation (11.8), theQithat attains equality satisesQi(z(i)) =p(z(i)jx(i);)Thus, we simply set the Qi's to be the posterior distribution of the z(i)'sgivenx(i)with the current setting of the parameters .Now, for this choice of t...",
        "python_code": [
            "156Taking sum over all the examples, we obtain a lower bound for the log-likelihood`()XiELBO(x(i);Qi;) (11.11)=XiXz(i)Qi(z(i)) logp(x(i);z(i);)Qi(z(i))Foranyset of distributions Q1;:::;Qn, the formula (11.11) gives a lower-bound on`(), and analogous to the argument around equation (11.8), theQithat attains equality satisesQi(z(i)) =p(z(i)jx(i);)Thus, we simply set the Qi's to be the posterior distribution of the z(i)'sgivenx(i)with the current setting of the parameters .Now, for this choice of the Qi's, Equation (11.11) gives a lower-bound onthe loglikelihood `that we're trying to maximize. This is the E-step. In theM-step of the algorithm, we then maximize our formula in Equation (11.11)with respect to the parameters to obtain a new setting of the 's. Repeatedlycarrying out these two steps gives us the EM algorithm, which is as follows:Repeat until convergence f(E-step) For each i, setQi(z(i)) :=p(z(i)jx(i);):(M-step) Set:= arg maxnXi=1ELBO(x(i);Qi;)= arg maxXiXz(i)Qi(z(i)) logp(x(i);z(i);)Qi(z(i)): (11.12)gHow do we know if this algorithm will converge? Well, suppose (t)and(t+1)are the parameters from two successive iterations of EM. We will nowprove that `((t))`((t+1)), which shows EM always monotonically im-proves the log-likelihood. The key to showing this result lies in our choice of"
        ],
        "formulas": [
            "maxnXi=1ELBO(x(i);Qi;)="
        ],
        "explanations": []
    },
    "page_158": {
        "content_preview": "157theQi's. Specically, on the iteration of EM in which the parameters hadstarted out as (t), we would have chosen Q(t)i(z(i)) :=p(z(i)jx(i);(t)). Wesaw earlier that this choice ensures that Jensen's inequality, as applied to getEquation (11.11), holds with equality, and hence`((t)) =nXi=1ELBO(x(i);Q(t)i;(t)) (11.13)The parameters (t+1)are then obtained by maximizing the right hand sideof the equation above. Thus,`((t+1))nXi=1ELBO(x(i);Q(t)i;(t+1))(because ineqaulity (11.11) holds for all Qand)n...",
        "python_code": [
            "157theQi's. Specically, on the iteration of EM in which the parameters hadstarted out as (t), we would have chosen Q(t)i(z(i)) :=p(z(i)jx(i);(t)). Wesaw earlier that this choice ensures that Jensen's inequality, as applied to getEquation (11.11), holds with equality, and hence`((t)) =nXi=1ELBO(x(i);Q(t)i;(t)) (11.13)The parameters (t+1)are then obtained by maximizing the right hand sideof the equation above. Thus,`((t+1))nXi=1ELBO(x(i);Q(t)i;(t+1))(because ineqaulity (11.11) holds for all Qand)nXi=1ELBO(x(i);Q(t)i;(t)) (see reason below)=`((t)) (by equation (11.13))where the last inequality follows from that (t+1)is chosen explicitly to bearg maxnXi=1ELBO(x(i);Q(t)i;)Hence, EM causes the likelihood to converge monotonically. In our de-scription of the EM algorithm, we said we'd run it until convergence. Giventhe result that we just showed, one reasonable convergence test would beto check if the increase in `() between successive iterations is smaller thansome tolerance parameter, and to declare convergence if EM is improving`() too slowly.Remark. If we dene (by overloading ELBO( ))ELBO(Q;) =nXi=1ELBO(x(i);Qi;) =XiXz(i)Qi(z(i)) logp(x(i);z(i);)Qi(z(i))(11.14)then we know `()ELBO(Q;) from our previous derivation. The EMcan also be viewed an alternating maximization algorithm on ELBO( Q;),in which the E-step maximizes it with respect to Q(check this yourself), andthe M-step maximizes it with respect to ."
        ],
        "formulas": [
            "nXi=1ELBO(x(i);Q(t)i;(t))",
            "nXi=1ELBO(x(i);Q(t)i;(t+1))(because",
            "nXi=1ELBO(x(i);Q(t)i;(t))",
            "maxnXi=1ELBO(x(i);Q(t)i;)Hence,",
            "nXi=1ELBO(x(i);Qi;)"
        ],
        "explanations": []
    },
    "page_159": {
        "content_preview": "15811.3.1 Other interpretation of ELBOLet ELBO( x;Q;) =PzQ(z) logp(x;z;)Q(z)be dened as in equation (11.9).There are several other forms of ELBO. First, we can rewriteELBO(x;Q;) = EzQ[logp(x;z;)]EzQ[logQ(z)]= EzQ[logp(xjz;)]DKL(Qkpz) (11.15)where we use pzto denote the marginal distribution of z(under the distri-butionp(x;z;)), andDKL() denotes the KL divergenceDKL(Qkpz) =XzQ(z) logQ(z)p(z)(11.16)In many cases, the marginal distribution of zdoes not depend on the param-eter. In this case, we can...",
        "python_code": [
            "15811.3.1 Other interpretation of ELBOLet ELBO( x;Q;) =PzQ(z) logp(x;z;)Q(z)be dened as in equation (11.9).There are several other forms of ELBO. First, we can rewriteELBO(x;Q;) = EzQ[logp(x;z;)]EzQ[logQ(z)]= EzQ[logp(xjz;)]DKL(Qkpz) (11.15)where we use pzto denote the marginal distribution of z(under the distri-butionp(x;z;)), andDKL() denotes the KL divergenceDKL(Qkpz) =XzQ(z) logQ(z)p(z)(11.16)In many cases, the marginal distribution of zdoes not depend on the param-eter. In this case, we can see that maximizing ELBO over is equivalentto maximizing the rst term in (11.15). This corresponds to maximizing theconditional likelihood of xconditioned on z, which is often a simpler questionthan the original question.Another form of ELBO( ) is (please verify yourself)ELBO(x;Q;) = logp(x)DKL(Qkpzjx) (11.17)wherepzjxis the conditional distribution of zgivenxunder the parameter. This forms shows that the maximizer of ELBO( Q;) overQis obtainedwhenQ=pzjx, which was shown in equation (11.8) before.11.4 Mixture of Gaussians revisitedArmed with our general denition of the EM algorithm, let's go back to ourold example of tting the parameters ,and in a mixture of Gaussians.For the sake of brevity, we carry out the derivations for the M-step updatesonly forandj, and leave the updates for jas an exercise for the reader.The E-step is easy. Following our algorithm derivation above, we simplycalculatew(i)j=Qi(z(i)=j) =P(z(i)=jjx(i);;; ):Here, \\Qi(z(i)=j)\" denotes the probability of z(i)taking the value junderthe distribution Qi."
        ],
        "formulas": [
            "obtainedwhenQ=pzjx,",
            "j=Qi(z(i)=j)"
        ],
        "explanations": []
    },
    "page_160": {
        "content_preview": "159Next, in the M-step, we need to maximize, with respect to our parameters;; , the quantitynXi=1Xz(i)Qi(z(i)) logp(x(i);z(i);;; )Qi(z(i))=nXi=1kXj=1Qi(z(i)=j) logp(x(i)jz(i)=j;;)p(z(i)=j;)Qi(z(i)=j)=nXi=1kXj=1w(i)jlog1(2)d=2jjj1=2exp12(x(i)j)T1j(x(i)j)jw(i)jLet's maximize this with respect to l. If we take the derivative with respecttol, we ndrlnXi=1kXj=1w(i)jlog1(2)d=2jjj1=2exp12(x(i)j)T1j(x(i)j)jw(i)j=rlnXi=1kXj=1w(i)j12(x(i)j)T1j(x(i)j)=12nXi=1w(i)lrl2Tl1lx(i)Tl1ll=nXi=1w(i)l1lx(i)1llSetting...",
        "python_code": [
            "159Next, in the M-step, we need to maximize, with respect to our parameters;; , the quantitynXi=1Xz(i)Qi(z(i)) logp(x(i);z(i);;; )Qi(z(i))=nXi=1kXj=1Qi(z(i)=j) logp(x(i)jz(i)=j;;)p(z(i)=j;)Qi(z(i)=j)=nXi=1kXj=1w(i)jlog1(2)d=2jjj1=2exp12(x(i)j)T1j(x(i)j)jw(i)jLet's maximize this with respect to l. If we take the derivative with respecttol, we ndrlnXi=1kXj=1w(i)jlog1(2)d=2jjj1=2exp12(x(i)j)T1j(x(i)j)jw(i)j=rlnXi=1kXj=1w(i)j12(x(i)j)T1j(x(i)j)=12nXi=1w(i)lrl2Tl1lx(i)Tl1ll=nXi=1w(i)l1lx(i)1llSetting this to zero and solving for ltherefore yields the update rulel:=Pni=1w(i)lx(i)Pni=1w(i)l;which was what we had in the previous set of notes.Let's do one more example, and derive the M-step update for the param-etersj. Grouping together only the terms that depend on j, we nd thatwe need to maximizenXi=1kXj=1w(i)jlogj:However, there is an additional constraint that the j's sum to 1, since theyrepresent the probabilities j=p(z(i)=j;). To deal with the constraint"
        ],
        "formulas": [
            "quantitynXi=1Xz(i)Qi(z(i))",
            "nXi=1kXj=1Qi(z(i)=j)",
            "nXi=1kXj=1w(i)jlog1(2)d=2jjj1=2exp12(x(i)j)T1j(x(i)j)jw(i)jLet's",
            "ndrlnXi=1kXj=1w(i)jlog1(2)d=2jjj1=2exp12(x(i)j)T1j(x(i)j)jw(i)j=rlnXi=1kXj=1w(i)j12(x(i)j)T1j(x(i)j)=12nXi=1w(i)lrl2Tl1lx(i)Tl1ll=nXi=1w(i)l1lx(i)1llSetting",
            "Pni=1w(i)lx(i)Pni=1w(i)l;which",
            "maximizenXi=1kXj=1w(i)jlogj:However,",
            "j=p(z(i)=j;)."
        ],
        "explanations": []
    },
    "page_161": {
        "content_preview": "160thatPkj=1j= 1, we construct the LagrangianL() =nXi=1kXj=1w(i)jlogj+(kXj=1j1);whereis the Lagrange multiplier.7Taking derivatives, we nd@@jL() =nXi=1w(i)jj+Setting this to zero and solving, we getj=Pni=1w(i)jI.e.,j/Pni=1w(i)j. Using the constraint thatPjj= 1, we easily ndthat=Pni=1Pkj=1w(i)j=Pni=11 =n. (This used the fact that w(i)j=Qi(z(i)=j), and since probabilities sum to 1,Pjw(i)j= 1.) We thereforehave our M-step updates for the parameters j:j:=1nnXi=1w(i)j:The derivation for the M-step up...",
        "python_code": [
            "160thatPkj=1j= 1, we construct the LagrangianL() =nXi=1kXj=1w(i)jlogj+(kXj=1j1);whereis the Lagrange multiplier.7Taking derivatives, we nd@@jL() =nXi=1w(i)jj+Setting this to zero and solving, we getj=Pni=1w(i)jI.e.,j/Pni=1w(i)j. Using the constraint thatPjj= 1, we easily ndthat=Pni=1Pkj=1w(i)j=Pni=11 =n. (This used the fact that w(i)j=Qi(z(i)=j), and since probabilities sum to 1,Pjw(i)j= 1.) We thereforehave our M-step updates for the parameters j:j:=1nnXi=1w(i)j:The derivation for the M-step updates to jare also entirely straightfor-ward.11.5 Variational inference and variationalauto-encoder (optional reading)Loosely speaking, variational auto-encoder Kingma and Welling [2013] gen-erally refers to a family of algorithms that extend the EM algorithms to morecomplex models parameterized by neural networks. It extends the techniqueof variational inference with the additional \\re-parametrization trick\" whichwill be introduced below. Variational auto-encoder may not give the bestperformance for many datasets, but it contains several central ideas abouthow to extend EM algorithms to high-dimensional continuous latent variables7We don't need to worry about the constraint that j0, because as we'll shortly see,the solution we'll nd from this derivation will automatically satisfy that anyway."
        ],
        "formulas": [
            "160thatPkj=1j=",
            "nXi=1kXj=1w(i)jlogj+(kXj=1j1);whereis",
            "nXi=1w(i)jj+Setting",
            "getj=Pni=1w(i)jI.e.,j/Pni=1w(i)j.",
            "thatPjj= 1,",
            "ndthat=Pni=1Pkj=1w(i)j=Pni=11",
            "j=Qi(z(i)=j),",
            "j= 1.)",
            "1nnXi=1w(i)j:The"
        ],
        "explanations": []
    },
    "page_162": {
        "content_preview": "161with non-linear models. Understanding it will likely give you the languageand backgrounds to understand various recent papers related to it.As a running example, we will consider the following parameterization ofp(x;z;) by a neural network. Let be the collection of the weights of aneural network g(z;) that maps z2RktoRd. LetzN(0;Ikk) (11.18)xjzN(g(z;);2Idd) (11.19)HereIkkdenotes identity matrix of dimension kbyk, andis a scalar thatwe assume to be known for simplicity.For the Gaussian mixture...",
        "python_code": [
            "161with non-linear models. Understanding it will likely give you the languageand backgrounds to understand various recent papers related to it.As a running example, we will consider the following parameterization ofp(x;z;) by a neural network. Let be the collection of the weights of aneural network g(z;) that maps z2RktoRd. LetzN(0;Ikk) (11.18)xjzN(g(z;);2Idd) (11.19)HereIkkdenotes identity matrix of dimension kbyk, andis a scalar thatwe assume to be known for simplicity.For the Gaussian mixture models in Section 11.4, the optimal choice ofQ(z) =p(zjx;) for each xed , that is the posterior distribution of z,can be analytically computed. In many more complex models such as themodel (11.19), it's intractable to compute the exact the posterior distributionp(zjx;).Recall that from equation (11.10), ELBO is always a lower bound for anychoice ofQ, and therefore, we can also aim for nding an approximation ofthe true posterior distribution. Often, one has to use some particular formto approximate the true posterior distribution. Let Qbe a family of Q's thatwe are considering, and we will aim to nd a Qwithin the family of Qthat isclosest to the true posterior distribution. To formalize, recall the denition ofthe ELBO lower bound as a function of Qanddened in equation (11.14)ELBO(Q;) =nXi=1ELBO(x(i);Qi;) =XiXz(i)Qi(z(i)) logp(x(i);z(i);)Qi(z(i))Recall that EM can be viewed as alternating maximization ofELBO(Q;). Here instead, we optimize the EBLO over Q2QmaxQ2QmaxELBO(Q;) (11.20)Now the next question is what form of Q(or what structural assumptionsto make about Q) allows us to eciently maximize the objective above. Whenthe latent variable zare high-dimensional discrete variables, one popular as-sumption is the mean eld assumption , which assumes that Qi(z) gives adistribution with independent coordinates, or in other words, Qican be de-composed into Qi(z) =Q1i(z1)Qki(zk). There are tremendous applicationsof mean eld assumptions to learning generative models with discrete latentvariables, and we refer to Blei et al. [2017] for a survey of these models and"
        ],
        "formulas": [
            "nXi=1ELBO(x(i);Qi;)"
        ],
        "explanations": []
    },
    "page_163": {
        "content_preview": "162their impact to a wide range of applications including computational biology,computational neuroscience, social sciences. We will not get into the detailsabout the discrete latent variable cases, and our main focus is to deal withcontinuous latent variables, which requires not only mean eld assumptions,but additional techniques.Whenz2Rkis a continuous latent variable, there are several decisions tomake towards successfully optimizing (11.20). First we need to give a succinctrepresentation of ...",
        "python_code": [
            "162their impact to a wide range of applications including computational biology,computational neuroscience, social sciences. We will not get into the detailsabout the discrete latent variable cases, and our main focus is to deal withcontinuous latent variables, which requires not only mean eld assumptions,but additional techniques.Whenz2Rkis a continuous latent variable, there are several decisions tomake towards successfully optimizing (11.20). First we need to give a succinctrepresentation of the distribution Qibecause it is over an innite number ofpoints. A natural choice is to assume Qiis a Gaussian distribution with somemean and variance. We would also like to have more succinct representationof the means of Qiof all the examples. Note that Qi(z(i)) is supposed toapproximate p(z(i)jx(i);). It would make sense let all the means of the Qi'sbe some function of x(i). Concretely, let q(;);v(;) be two functions thatmap from dimension dtok, which are parameterized by and , we assumethatQi=N(q(x(i););diag(v(x(i); ))2) (11.21)Here diag(w) means the kkmatrix with the entries of w2Rkon thediagonal. In other words, the distribution Qiis assumed to be a Gaussiandistribution with independent coordinates, and the mean and standard de-viations are governed by qandv. Often in variational auto-encoder, qandvare chosen to be neural networks.8In recent deep learning literature, oftenq;vare called encoder (in the sense of encoding the data into latent code),whereasg(z;) if often referred to as the decoder.We remark that Qiof such form in many cases are very far from a good ap-proximation of the true posterior distribution. However, some approximationis necessary for feasible optimization. In fact, the form of Qineeds to satisfyother requirements (which happened to be satised by the form (11.21))Before optimizing the ELBO, let's rst verify whether we can ecientlyevaluate the value of the ELBO for xed Qof the form (11.21) and . Werewrite the ELBO as a function of ; ; byELBO(; ; ) =nXi=1Ez(i)Qilogp(x(i);z(i);)Qi(z(i)); (11.22)whereQi=N(q(x(i););diag(v(x(i); ))2)Note that to evaluate Qi(z(i)) inside the expectation, we should be able tocompute the density of Qi. To estimate the expectation Ez(i)Qi, we8qandvcan also share parameters. We sweep this level of details under the rug in thisnote."
        ],
        "formulas": [
            "assumethatQi=N(q(x(i););diag(v(x(i);",
            "nXi=1Ez(i)Qilogp(x(i);z(i);)Qi(z(i));",
            "whereQi=N(q(x(i););diag(v(x(i);"
        ],
        "explanations": []
    },
    "page_164": {
        "content_preview": "163should be able to sample from distribution Qiso that we can build anempirical estimator with samples. It happens that for Gaussian distributionQi=N(q(x(i););diag(v(x(i); ))2), we are able to be both eciently.Now let's optimize the ELBO. It turns out that we can run gradient ascentover; ; instead of alternating maximization. There is no strong need tocompute the maximum over each variable at a much greater cost. (For Gaus-sian mixture model in Section 11.4, computing the maximum is analyticall...",
        "python_code": [
            "163should be able to sample from distribution Qiso that we can build anempirical estimator with samples. It happens that for Gaussian distributionQi=N(q(x(i););diag(v(x(i); ))2), we are able to be both eciently.Now let's optimize the ELBO. It turns out that we can run gradient ascentover; ; instead of alternating maximization. There is no strong need tocompute the maximum over each variable at a much greater cost. (For Gaus-sian mixture model in Section 11.4, computing the maximum is analyticallyfeasible and relatively cheap, and therefore we did alternating maximization.)Mathematically, let be the learning rate, the gradient ascent step is:=+rELBO(; ; ):=+rELBO(; ; ) := +r ELBO(; ; )Computing the gradient over is simple becauserELBO(; ; ) =rnXi=1Ez(i)Qilogp(x(i);z(i);)Qi(z(i))=rnXi=1Ez(i)Qilogp(x(i);z(i);)=nXi=1Ez(i)Qirlogp(x(i);z(i);); (11.23)But computing the gradient over and is tricky because the sam-pling distribution Qidepends on and . (Abstractly speaking, the is-sue we face can be simplied as the problem of computing the gradi-ent EzQ[f()] with respect to variable . We know that in general,rEzQ[f()]6= EzQ[rf()] because the dependency of Qonhas to betaken into account as well. )The idea that comes to rescue is the so-called re-parameterizationtrick : we rewrite z(i)Qi=N(q(x(i););diag(v(x(i); ))2) in an equivalentway:z(i)=q(x(i);) +v(x(i); )(i)where(i)N(0;Ikk) (11.24)Herexydenotes the entry-wise product of two vectors of the samedimension. Here we used the fact that xN(;2) is equivalent to thatx=+withN(0;1). We mostly just used this fact in every dimensionsimultaneously for the random variable z(i)Qi."
        ],
        "formulas": [
            "distributionQi=N(q(x(i););diag(v(x(i);",
            "rnXi=1Ez(i)Qilogp(x(i);z(i);)Qi(z(i))=rnXi=1Ez(i)Qilogp(x(i);z(i);)=nXi=1Ez(i)Qirlogp(x(i);z(i););",
            "6= EzQ[rf()]",
            "Qi=N(q(x(i););diag(v(x(i);",
            "thatx=+withN(0;1)."
        ],
        "explanations": []
    },
    "page_165": {
        "content_preview": "164With this re-parameterization, we have thatEz(i)Qilogp(x(i);z(i);)Qi(z(i))(11.25)= E(i)N(0;1)logp(x(i);q(x(i);) +v(x(i); )(i);)Qi(q(x(i);) +v(x(i); )(i))It follows thatrEz(i)Qilogp(x(i);z(i);)Qi(z(i))=rE(i)N(0;1)logp(x(i);q(x(i);) +v(x(i); )(i);)Qi(q(x(i);) +v(x(i); )(i))= E(i)N(0;1)rlogp(x(i);q(x(i);) +v(x(i); )(i);)Qi(q(x(i);) +v(x(i); )(i))We can now sample multiple copies of (i)'s to estimate the the expecta-tion in the RHS of the equation above.9We can estimate the gradient withrespect t...",
        "python_code": [
            "164With this re-parameterization, we have thatEz(i)Qilogp(x(i);z(i);)Qi(z(i))(11.25)= E(i)N(0;1)logp(x(i);q(x(i);) +v(x(i); )(i);)Qi(q(x(i);) +v(x(i); )(i))It follows thatrEz(i)Qilogp(x(i);z(i);)Qi(z(i))=rE(i)N(0;1)logp(x(i);q(x(i);) +v(x(i); )(i);)Qi(q(x(i);) +v(x(i); )(i))= E(i)N(0;1)rlogp(x(i);q(x(i);) +v(x(i); )(i);)Qi(q(x(i);) +v(x(i); )(i))We can now sample multiple copies of (i)'s to estimate the the expecta-tion in the RHS of the equation above.9We can estimate the gradient withrespect to similarly, and with these, we can implement the gradient ascentalgorithm to optimize the ELBO over ; ;:There are not many high-dimensional distributions with analytically com-putable density function are known to be re-parameterizable. We refer toKingma and Welling [2013] for a few other choices that can replace Gaussiandistribution.9Empirically people sometimes just use one sample to estimate it for maximum com-putational eciency."
        ],
        "formulas": [],
        "explanations": []
    },
    "page_166": {
        "content_preview": "Chapter 12Principal components analysisIn this set of notes, we will develop a method, Principal Components Analysis(PCA), that tries to identify the subspace in which the data approximatelylies. PCA is computationally ecient: it will require only an eigenvectorcalculation (easily done with the eigfunction in Matlab).Suppose we are given a dataset fx(i);i= 1;:::;ngof attributes of ndif-ferent types of automobiles, such as their maximum speed, turn radius, andso on. Let x(i)2Rdfor eachi(dn). But ...",
        "python_code": [
            "Chapter 12Principal components analysisIn this set of notes, we will develop a method, Principal Components Analysis(PCA), that tries to identify the subspace in which the data approximatelylies. PCA is computationally ecient: it will require only an eigenvectorcalculation (easily done with the eigfunction in Matlab).Suppose we are given a dataset fx(i);i= 1;:::;ngof attributes of ndif-ferent types of automobiles, such as their maximum speed, turn radius, andso on. Let x(i)2Rdfor eachi(dn). But unknown to us, two dierentattributes|some xiandxj|respectively give a car's maximum speed mea-sured in miles per hour, and the maximum speed measured in kilometers perhour. These two attributes are therefore almost linearly dependent, up toonly small dierences introduced by rounding o to the nearest mph or kph.Thus, the data really lies approximately on an n1 dimensional subspace.How can we automatically detect, and perhaps remove, this redundancy?For a less contrived example, consider a dataset resulting from a survey ofpilots for radio-controlled helicopters, where x(i)1is a measure of the pilotingskill of pilot i, andx(i)2captures how much he/she enjoys ying. BecauseRC helicopters are very dicult to y, only the most committed students,ones that truly enjoy ying, become good pilots. So, the two attributesx1andx2are strongly correlated. Indeed, we might posit that that thedata actually likes along some diagonal axis (the u1direction) capturing theintrinsic piloting \\karma\" of a person, with only a small amount of noiselying o this axis. (See gure.) How can we automatically compute this u1direction?165"
        ],
        "formulas": [
            "i= 1;:::;ngof"
        ],
        "explanations": []
    },
    "page_167": {
        "content_preview": "166x1x2(enjoyment)(skill)1uu2We will shortly develop the PCA algorithm. But prior to running PCAper se, typically we rst preprocess the data by normalizing each featureto have mean 0 and variance 1. We do this by subtracting the mean anddividing by the empirical standard deviation:x(i)j x(i)jjjwherej=1nPni=1x(i)jand2j=1nPni=1(x(i)jj)2are the mean variance offeaturej, respectively.Subtracting jzeros out the mean and may be omitted for data knownto have zero mean (for instance, time series corresp...",
        "python_code": [
            "166x1x2(enjoyment)(skill)1uu2We will shortly develop the PCA algorithm. But prior to running PCAper se, typically we rst preprocess the data by normalizing each featureto have mean 0 and variance 1. We do this by subtracting the mean anddividing by the empirical standard deviation:x(i)j x(i)jjjwherej=1nPni=1x(i)jand2j=1nPni=1(x(i)jj)2are the mean variance offeaturej, respectively.Subtracting jzeros out the mean and may be omitted for data knownto have zero mean (for instance, time series corresponding to speech or otheracoustic signals). Dividing by the standard deviation jrescales each coor-dinate to have unit variance, which ensures that dierent attributes are alltreated on the same \\scale.\" For instance, if x1was cars' maximum speed inmph (taking values in the high tens or low hundreds) and x2were the num-ber of seats (taking values around 2-4), then this renormalization rescalesthe dierent attributes to make them more comparable. This rescaling maybe omitted if we had a priori knowledge that the dierent attributes are allon the same scale. One example of this is if each data point represented agrayscale image, and each x(i)jtook a value inf0;1;:::; 255gcorrespondingto the intensity value of pixel jin imagei.Now, having normalized our data, how do we compute the \\major axisof variation\" u|that is, the direction on which the data approximately lies?One way is to pose this problem as nding the unit vector uso that when"
        ],
        "formulas": [
            "jjjwherej=1nPni=1x(i)jand2j=1nPni=1(x(i)jj)2are"
        ],
        "explanations": []
    },
    "page_168": {
        "content_preview": "167the data is projected onto the direction corresponding to u, the variance ofthe projected data is maximized. Intuitively, the data starts o with someamount of variance/information in it. We would like to choose a direction uso that if we were to approximate the data as lying in the direction/subspacecorresponding to u, as much as possible of this variance is still retained.Consider the following dataset, on which we have already carried out thenormalization steps:Now, suppose we pick uto corr...",
        "python_code": [],
        "formulas": [],
        "explanations": [
            "167the data is projected onto the direction corresponding to u, the variance ofthe projected data is maximized",
            "Intuitively, the data starts o with someamount of variance/information in it",
            "We would like to choose a direction uso that if we were to approximate the data as lying in the direction/subspacecorresponding to u, as much as possible of this variance is still retained",
            "Consider the following dataset, on which we have already carried out thenormalization steps:Now, suppose we pick uto correspond the the direction shown in thegure below",
            "The circles denote the projections of the original data onto thisline"
        ]
    },
    "page_169": {
        "content_preview": "168/0/0/1/1/0/0/0/0/1/1/1/1/0 /1/0/0/0/0/1/1/1/1/0 /1 /0/0/0/0/1/1/1/1 /0/0/0/0/0/0/0/0/0/0/0/0/0/0/1/1/1/1/1/1/1/1/1/1/1/1/1/1/0/0/0/0/0/0/0/0/1/1/1/1/1/1/1/1/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/0/0/0/0/0/0/0/0/1/1/1/1/1/1/1/1We see that the projected data still has a fairly large variance, and thepoints tend to be far from zero. In contrast, suppose had instead picked thefollowing direction:/0/0 /1/1/0/0/0/0/1/1/1/1 /0 /1/0/0/0/0/1/1/1/1/0/0/1/1/...",
        "python_code": [],
        "formulas": [],
        "explanations": [
            "168/0/0/1/1/0/0/0/0/1/1/1/1/0 /1/0/0/0/0/1/1/1/1/0 /1 /0/0/0/0/1/1/1/1 /0/0/0/0/0/0/0/0/0/0/0/0/0/0/1/1/1/1/1/1/1/1/1/1/1/1/1/1/0/0/0/0/0/0/0/0/1/1/1/1/1/1/1/1/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/0/0/0/0/0/0/0/0/1/1/1/1/1/1/1/1We see that the projected data still has a fairly large variance, and thepoints tend to be far from zero",
            "In contrast, suppose had instead picked thefollowing direction:/0/0 /1/1/0/0/0/0/1/1/1/1 /0 /1/0/0/0/0/1/1/1/1/0/0/1/1/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1Here, the projections have a signicantly smaller variance, and are muchcloser to the origin",
            "We would like to automatically select the direction ucorresponding tothe rst of the two gures shown above",
            "To formalize this, note that given a"
        ]
    },
    "page_170": {
        "content_preview": "169unit vector uand a point x, the length of the projection of xontouis givenbyxTu. I.e., ifx(i)is a point in our dataset (one of the crosses in the plot),then its projection onto u(the corresponding circle in the gure) is distancexTufrom the origin. Hence, to maximize the variance of the projections, wewould like to choose a unit-length uso as to maximize:1nnXi=1(x(i)Tu)2=1nnXi=1uTx(i)x(i)Tu=uT 1nnXi=1x(i)x(i)T!u:We easily recognize that the maximizing this subject to kuk2= 1 gives theprincipal...",
        "python_code": [
            "169unit vector uand a point x, the length of the projection of xontouis givenbyxTu. I.e., ifx(i)is a point in our dataset (one of the crosses in the plot),then its projection onto u(the corresponding circle in the gure) is distancexTufrom the origin. Hence, to maximize the variance of the projections, wewould like to choose a unit-length uso as to maximize:1nnXi=1(x(i)Tu)2=1nnXi=1uTx(i)x(i)Tu=uT 1nnXi=1x(i)x(i)T!u:We easily recognize that the maximizing this subject to kuk2= 1 gives theprincipal eigenvector of =1nPni=1x(i)x(i)T, which is just the empiricalcovariance matrix of the data (assuming it has zero mean).1To summarize, we have found that if we wish to nd a 1-dimensionalsubspace with with to approximate the data, we should choose uto be theprincipal eigenvector of . More generally, if we wish to project our datainto ak-dimensional subspace ( k<d ), we should choose u1;:::;ukto be thetopkeigenvectors of . The ui's now form a new, orthogonal basis for thedata.2Then, to represent x(i)in this basis, we need only compute the corre-sponding vectory(i)=26664uT1x(i)uT2x(i)...uTkx(i)377752Rk:Thus, whereas x(i)2Rd, the vector y(i)now gives a lower, k-dimensional,approximation/representation for x(i). PCA is therefore also referred to asadimensionality reduction algorithm. The vectors u1;:::;ukare calledthe rstkprincipal components of the data.Remark. Although we have shown it formally only for the case of k= 1,using well-known properties of eigenvectors it is straightforward to show that1If you haven't seen this before, try using the method of Lagrange multipliers to max-imizeuTusubject to that uTu= 1. You should be able to show that u=u, for some, which implies uis an eigenvector of , with eigenvalue .2Because is symmetric, the ui's will (or always can be chosen to be) orthogonal toeach other."
        ],
        "formulas": [
            "1nnXi=1(x(i)Tu)2=1nnXi=1uTx(i)x(i)Tu=uT",
            "1nnXi=1x(i)x(i)T!u:We",
            "kuk2= 1",
            "of =1nPni=1x(i)x(i)T,",
            "k= 1,using",
            "uTu= 1.",
            "u=u,"
        ],
        "explanations": []
    },
    "page_171": {
        "content_preview": "170of all possible orthogonal bases u1;:::;uk, the one that we have chosen max-imizesPiky(i)k22. Thus, our choice of a basis preserves as much variabilityas possible in the original data.PCA can also be derived by picking the basis that minimizes the ap-proximation error arising from projecting the data onto the k-dimensionalsubspace spanned by them. (See more in homework.)PCA has many applications; we will close our discussion with a few exam-ples. First, compression|representing x(i)'s with lo...",
        "python_code": [
            "170of all possible orthogonal bases u1;:::;uk, the one that we have chosen max-imizesPiky(i)k22. Thus, our choice of a basis preserves as much variabilityas possible in the original data.PCA can also be derived by picking the basis that minimizes the ap-proximation error arising from projecting the data onto the k-dimensionalsubspace spanned by them. (See more in homework.)PCA has many applications; we will close our discussion with a few exam-ples. First, compression|representing x(i)'s with lower dimension y(i)'s|isan obvious application. If we reduce high dimensional data to k= 2 or 3 di-mensions, then we can also plot the y(i)'s to visualize the data. For instance,if we were to reduce our automobiles data to 2 dimensions, then we can plotit (one point in our plot would correspond to one car type, say) to see whatcars are similar to each other and what groups of cars may cluster together.Another standard application is to preprocess a dataset to reduce itsdimension before running a supervised learning learning algorithm with thex(i)'s as inputs. Apart from computational benets, reducing the data'sdimension can also reduce the complexity of the hypothesis class consideredand help avoid overtting (e.g., linear classiers over lower dimensional inputspaces will have smaller VC dimension).Lastly, as in our RC pilot example, we can also view PCA as a noisereduction algorithm. In our example it, estimates the intrinsic \\pilotingkarma\" from the noisy measures of piloting skill and enjoyment. In class, wealso saw the application of this idea to face images, resulting in eigenfacesmethod. Here, each point x(i)2R100100was a 10000 dimensional vector,with each coordinate corresponding to a pixel intensity value in a 100x100image of a face. Using PCA, we represent each image x(i)with a much lower-dimensional y(i). In doing so, we hope that the principal components wefound retain the interesting, systematic variations between faces that capturewhat a person really looks like, but not the \\noise\" in the images introducedby minor lighting variations, slightly dierent imaging conditions, and so on.We then measure distances between faces iandjby working in the reduceddimension, and computing ky(i)y(j)k2. This resulted in a surprisingly goodface-matching and retrieval algorithm."
        ],
        "formulas": [
            "k= 2"
        ],
        "explanations": []
    },
    "page_172": {
        "content_preview": "Chapter 13Independent componentsanalysisOur next topic is Independent Components Analysis (ICA). Similar to PCA,this will nd a new basis in which to represent our data. However, the goalis very dierent.As a motivating example, consider the \\cocktail party problem.\" Here, dspeakers are speaking simultaneously at a party, and any microphone placedin the room records only an overlapping combination of the dspeakers' voices.But lets say we have ddierent microphones placed in the room, and becauseeac...",
        "python_code": [
            "Chapter 13Independent componentsanalysisOur next topic is Independent Components Analysis (ICA). Similar to PCA,this will nd a new basis in which to represent our data. However, the goalis very dierent.As a motivating example, consider the \\cocktail party problem.\" Here, dspeakers are speaking simultaneously at a party, and any microphone placedin the room records only an overlapping combination of the dspeakers' voices.But lets say we have ddierent microphones placed in the room, and becauseeach microphone is a dierent distance from each of the speakers, it records adierent combination of the speakers' voices. Using these microphone record-ings, can we separate out the original dspeakers' speech signals?To formalize this problem, we imagine that there is some data s2Rdthat is generated via dindependent sources. What we observe isx=As;whereAis an unknown square matrix called the mixing matrix . Repeatedobservations gives us a dataset fx(i);i= 1;:::;ng, and our goal is to recoverthe sources s(i)that had generated our data ( x(i)=As(i)).In our cocktail party problem, s(i)is and-dimensional vector, and s(i)jisthe sound that speaker jwas uttering at time i. Also,x(i)in and-dimensionalvector, and x(i)jis the acoustic reading recorded by microphone jat timei.LetW=A1be the unmixing matrix. Our goal is to nd W, sothat given our microphone recordings x(i), we can recover the sources bycomputing s(i)=Wx(i). For notational convenience, we also let wTidenote171"
        ],
        "formulas": [
            "isx=As;whereAis",
            "i= 1;:::;ng,",
            "LetW=A1be"
        ],
        "explanations": []
    },
    "page_173": {
        "content_preview": "172thei-th row ofW, so thatW=264|wT1|...|wTd|375:Thus,wi2Rd, and thej-th source can be recovered as s(i)j=wTjx(i).13.1 ICA ambiguitiesTo what degree can W=A1be recovered? If we have no prior knowledgeabout the sources and the mixing matrix, it is easy to see that there are someinherent ambiguities in Athat are impossible to recover, given only the x(i)'s.Specically, let Pbe anyd-by-dpermutation matrix. This means thateach row and each column of Phas exactly one \\1.\" Here are some examplesof perm...",
        "python_code": [
            "172thei-th row ofW, so thatW=264|wT1|...|wTd|375:Thus,wi2Rd, and thej-th source can be recovered as s(i)j=wTjx(i).13.1 ICA ambiguitiesTo what degree can W=A1be recovered? If we have no prior knowledgeabout the sources and the mixing matrix, it is easy to see that there are someinherent ambiguities in Athat are impossible to recover, given only the x(i)'s.Specically, let Pbe anyd-by-dpermutation matrix. This means thateach row and each column of Phas exactly one \\1.\" Here are some examplesof permutation matrices:P=240 1 01 0 00 0 135;P=0 11 0;P=1 00 1:Ifzis a vector, then Pzis another vector that contains a permuted versionofz's coordinates. Given only the x(i)'s, there will be no way to distinguishbetweenWandPW. Specically, the permutation of the original sources isambiguous, which should be no surprise. Fortunately, this does not matterfor most applications.Further, there is no way to recover the correct scaling of the wi's. For in-stance, ifAwere replaced with 2 A, and every s(i)were replaced with (0 :5)s(i),then our observed x(i)= 2A(0:5)s(i)would still be the same. More broadly,if a single column of Awere scaled by a factor of , and the correspondingsource were scaled by a factor of 1 =, then there is again no way to determinethat this had happened given only the x(i)'s. Thus, we cannot recover the\\correct\" scaling of the sources. However, for the applications that we areconcerned with|including the cocktail party problem|this ambiguity alsodoes not matter. Specically, scaling a speaker's speech signal s(i)jby somepositive factor aects only the volume of that speaker's speech. Also, signchanges do not matter, and s(i)jands(i)jsound identical when played on aspeaker. Thus, if the wifound by an algorithm is scaled by any non-zero realnumber, the corresponding recovered source si=wTixwill be scaled by the"
        ],
        "formulas": [
            "thatW=264|wT1|...|wTd|375:Thus,wi2Rd,",
            "j=wTjx(i).13.1",
            "W=A1be",
            "P=240",
            "P=0",
            "P=1",
            "1 =,",
            "si=wTixwill"
        ],
        "explanations": []
    },
    "page_174": {
        "content_preview": "173same factor; but this usually does not matter. (These comments also applyto ICA for the brain/MEG data that we talked about in class.)Are these the only sources of ambiguity in ICA? It turns out that theyare, so long as the sources siarenon-Gaussian . To see what the diculty iswith Gaussian data, consider an example in which n= 2, andsN (0;I).Here,Iis the 2x2 identity matrix. Note that the contours of the density ofthe standard normal distribution N(0;I) are circles centered on the origin,and...",
        "python_code": [
            "173same factor; but this usually does not matter. (These comments also applyto ICA for the brain/MEG data that we talked about in class.)Are these the only sources of ambiguity in ICA? It turns out that theyare, so long as the sources siarenon-Gaussian . To see what the diculty iswith Gaussian data, consider an example in which n= 2, andsN (0;I).Here,Iis the 2x2 identity matrix. Note that the contours of the density ofthe standard normal distribution N(0;I) are circles centered on the origin,and the density is rotationally symmetric.Now, suppose we observe some x=As, whereAis our mixing matrix.Then, the distribution of xwill be Gaussian, xN(0;AAT), sinceEsN(0;I)[x] = E[As] =AE[s] = 0Cov[x] = EsN(0;I)[xxT] = E[AssTAT] =AE[ssT]AT=ACov[s]AT=AATNow, letRbe an arbitrary orthogonal (less formally, a rotation/reection)matrix, so that RRT=RTR=I, and letA0=AR. Then if the data hadbeen mixed according to A0instead ofA, we would have instead observedx0=A0s. The distribution of x0is also Gaussian, x0N (0;AAT), sinceEsN(0;I)[x0(x0)T] = E[A0ssT(A0)T] = E[ARssT(AR)T] =ARRTAT=AAT.Hence, whether the mixing matrix is AorA0, we would observe data fromaN(0;AAT) distribution. Thus, there is no way to tell if the sources weremixed using AandA0. There is an arbitrary rotational component in themixing matrix that cannot be determined from the data, and we cannotrecover the original sources.Our argument above was based on the fact that the multivariate standardnormal distribution is rotationally symmetric. Despite the bleak picture thatthis paints for ICA on Gaussian data, it turns out that, so long as the data isnotGaussian, it is possible, given enough data, to recover the dindependentsources.13.2 Densities and linear transformationsBefore moving on to derive the ICA algorithm proper, we rst digress brieyto talk about the eect of linear transformations on densities.Suppose a random variable sis drawn according to some density ps(s).For simplicity, assume for now that s2Ris a real number. Now, let therandom variable xbe dened according to x=As(here,x2R;A2R). Letpxbe the density of x. What ispx?LetW=A1. To calculate the \\probability\" of a particular value of x,it is tempting to compute s=Wx, then then evaluate psat that point, and"
        ],
        "formulas": [
            "n= 2,",
            "x=As,",
            "AT=ACov[s]AT=AATNow,",
            "RRT=RTR=I,",
            "letA0=AR.",
            "observedx0=A0s.",
            "ARRTAT=AAT.Hence,",
            "x=As(here,x2R;A2R).",
            "LetW=A1.",
            "s=Wx,"
        ],
        "explanations": []
    },
    "page_175": {
        "content_preview": "174conclude that \\ px(x) =ps(Wx).\" However, this is incorrect . For example,letsUniform[0;1], sops(s) = 1f0s1g. Now, let A= 2, sox= 2s.Clearly,xis distributed uniformly in the interval [0 ;2]. Thus, its density isgiven bypx(x) = (0:5)1f0x2g. This does not equal ps(Wx), whereW= 0:5 =A1. Instead, the correct formula is px(x) =ps(Wx)jWj.More generally, if sis a vector-valued distribution with density ps, andx=Asfor a square, invertible matrix A, then the density of xis given bypx(x) =ps(Wx)jWj;wher...",
        "python_code": [
            "174conclude that \\ px(x) =ps(Wx).\" However, this is incorrect . For example,letsUniform[0;1], sops(s) = 1f0s1g. Now, let A= 2, sox= 2s.Clearly,xis distributed uniformly in the interval [0 ;2]. Thus, its density isgiven bypx(x) = (0:5)1f0x2g. This does not equal ps(Wx), whereW= 0:5 =A1. Instead, the correct formula is px(x) =ps(Wx)jWj.More generally, if sis a vector-valued distribution with density ps, andx=Asfor a square, invertible matrix A, then the density of xis given bypx(x) =ps(Wx)jWj;whereW=A1.Remark. If you're seen the result that Amaps [0;1]dto a set of volume jAj,then here's another way to remember the formula for pxgiven above, that alsogeneralizes our previous 1-dimensional example. Specically, let A2Rddbegiven, and let W=A1as usual. Also let C1= [0;1]dbe thed-dimensionalhypercube, and dene C2=fAs:s2C1gRdto be the image of C1under the mapping given by A. Then it is a standard result in linear algebra(and, indeed, one of the ways of dening determinants) that the volume ofC2is given byjAj. Now, suppose sis uniformly distributed in [0 ;1]d, so itsdensity isps(s) = 1fs2C1g. Then clearly xwill be uniformly distributedinC2. Its density is therefore found to be px(x) = 1fx2C2g=vol(C2) (sinceit must integrate over C2to 1). But using the fact that the determinantof the inverse of a matrix is just the inverse of the determinant, we have1=vol(C2) = 1=jAj=jA1j=jWj. Thus,px(x) = 1fx2C2gjWj= 1fWx2C1gjWj=ps(Wx)jWj.13.3 ICA algorithmWe are now ready to derive an ICA algorithm. We describe an algorithmby Bell and Sejnowski, and we give an interpretation of their algorithm as amethod for maximum likelihood estimation. (This is dierent from their orig-inal interpretation involving a complicated idea called the infomax principalwhich is no longer necessary given the modern understanding of ICA.)We suppose that the distribution of each source sjis given by a densityps, and that the joint distribution of the sources sis given byp(s) =dYj=1ps(sj):"
        ],
        "formulas": [
            "A= 2,",
            "sox= 2s.Clearly,xis",
            "whereW= 0:5",
            "andx=Asfor",
            "whereW=A1.Remark.",
            "W=A1as",
            "C1= [0;1]dbe",
            "C2=fAs:s2C1gRdto",
            "1fx2C2g=vol(C2)",
            "have1=vol(C2)",
            "1=jAj=jA1j=jWj.",
            "1fx2C2gjWj= 1fWx2C1gjWj=ps(Wx)jWj.13.3",
            "dYj=1ps(sj):"
        ],
        "explanations": []
    },
    "page_176": {
        "content_preview": "175Note that by modeling the joint distribution as a product of marginals, wecapture the assumption that the sources are independent. Using our formulasfrom the previous section, this implies the following density on x=As=W1s:p(x) =dYj=1ps(wTjx)jWj:All that remains is to specify a density for the individual sources ps.Recall that, given a real-valued random variable z, its cumulative distri-bution function (cdf) Fis dened by F(z0) =P(zz0) =Rz01pz(z)dzandthe density is the derivative of the cdf: ...",
        "python_code": [
            "175Note that by modeling the joint distribution as a product of marginals, wecapture the assumption that the sources are independent. Using our formulasfrom the previous section, this implies the following density on x=As=W1s:p(x) =dYj=1ps(wTjx)jWj:All that remains is to specify a density for the individual sources ps.Recall that, given a real-valued random variable z, its cumulative distri-bution function (cdf) Fis dened by F(z0) =P(zz0) =Rz01pz(z)dzandthe density is the derivative of the cdf: pz(z) =F0(z).Thus, to specify a density for the si's, all we need to do is to specify somecdf for it. A cdf has to be a monotonic function that increases from zeroto one. Following our previous discussion, we cannot choose the Gaussiancdf, as ICA doesn't work on Gaussian data. What we'll choose instead asa reasonable \\default\" cdf that slowly increases from 0 to 1, is the sigmoidfunctiong(s) = 1=(1 +es). Hence,ps(s) =g0(s).1The square matrix Wis the parameter in our model. Given a trainingsetfx(i);i= 1;:::;ng, the log likelihood is given by`(W) =nXi=1 dXj=1logg0(wTjx(i)) + logjWj!:We would like to maximize this in terms W. By taking derivatives and usingthe fact (from the rst set of notes) that rWjWj=jWj(W1)T, we easilyderive a stochastic gradient ascent learning rule. For a training example x(i),the update rule is:W:=W+0BBB@2666412g(wT1x(i))12g(wT2x(i))...12g(wTdx(i))37775x(i)T+ (WT)11CCCA;1If you have prior knowledge that the sources' densities take a certain form, then itis a good idea to substitute that in here. But in the absence of such knowledge, thesigmoid function can be thought of as a reasonable default that seems to work well formany problems. Also, the presentation here assumes that either the data x(i)has beenpreprocessed to have zero mean, or that it can naturally be expected to have zero mean(such as acoustic signals). This is necessary because our assumption that ps(s) =g0(s)implies E[s] = 0 (the derivative of the logistic function is a symmetric function, andhence gives a density corresponding to a random variable with zero mean), which impliesE[x] = E[As] = 0."
        ],
        "formulas": [
            "x=As=W1s:p(x)",
            "dYj=1ps(wTjx)jWj:All",
            "1=(1",
            "i= 1;:::;ng,",
            "nXi=1",
            "dXj=1logg0(wTjx(i))",
            "rWjWj=jWj(W1)T,"
        ],
        "explanations": []
    },
    "page_177": {
        "content_preview": "176whereis the learning rate.After the algorithm converges, we then compute s(i)=Wx(i)to recoverthe original sources.Remark. When writing down the likelihood of the data, we implicitly as-sumed that the x(i)'s were independent of each other (for dierent valuesofi; note this issue is dierent from whether the dierent coordinates ofx(i)are independent), so that the likelihood of the training set was givenbyQip(x(i);W). This assumption is clearly incorrect for speech data andother time series where ...",
        "python_code": [
            "176whereis the learning rate.After the algorithm converges, we then compute s(i)=Wx(i)to recoverthe original sources.Remark. When writing down the likelihood of the data, we implicitly as-sumed that the x(i)'s were independent of each other (for dierent valuesofi; note this issue is dierent from whether the dierent coordinates ofx(i)are independent), so that the likelihood of the training set was givenbyQip(x(i);W). This assumption is clearly incorrect for speech data andother time series where the x(i)'s are dependent, but it can be shown thathaving correlated training examples will not hurt the performance of the al-gorithm if we have sucient data. However, for problems where successivetraining examples are correlated, when implementing stochastic gradient as-cent, it sometimes helps accelerate convergence if we visit training examplesin a randomly permuted order. (I.e., run stochastic gradient ascent on arandomly shued copy of the training set.)"
        ],
        "formulas": [],
        "explanations": []
    },
    "page_178": {
        "content_preview": "Chapter 14Self-supervised learning andfoundation modelsDespite its huge success, supervised learning with neural networks typicallyrelies on the availability of a labeled dataset of decent size, which is some-times costly to collect. Recently, AI and machine learning are undergoing aparadigm shift with the rise of models (e.g., BERT [Devlin et al., 2019] andGPT-3 [Brown et al., 2020]) that are pre-trained on broad data at scale andare adaptable to a wide range of downstream tasks. These models, ...",
        "python_code": [],
        "formulas": [],
        "explanations": [
            "Chapter 14Self-supervised learning andfoundation modelsDespite its huge success, supervised learning with neural networks typicallyrelies on the availability of a labeled dataset of decent size, which is some-times costly to collect",
            "Recently, AI and machine learning are undergoing aparadigm shift with the rise of models (e",
            ", 2019] andGPT-3 [Brown et al",
            ", 2020]) that are pre-trained on broad data at scale andare adaptable to a wide range of downstream tasks",
            "These models, calledfoundation models by Bommasani et al",
            "[2021], oftentimes leverage massiveunlabeled data so that much fewer labeled data in the downstream tasks areneeded",
            "Moreover, though foundation models are based on standard deeplearning and transfer learning, their scale results in new emergent capabil-ities",
            "These models are typically (pre-)trained by self-supervised learningmethods where the supervisions/labels come from parts of the inputs",
            "This chapter will introduce the paradigm of foundation models and basicrelated concepts",
            "1 Pretraining and adaptationThe foundation models paradigm consists of two phases: pretraining (or sim-ply training) and adaptation",
            "We rst pretrain a large model on a massiveunlabeled dataset (e",
            ", billions of unlabeled images)",
            "1Then, we adapt thepretrained model to a downstream task (e",
            ", detecting cancer from scan im-ages)",
            "These downstream tasks are often prediction tasks with limited or1Sometimes, pretraining can involve large-scale labeled datasets as well (e",
            ", the Ima-geNet dataset)"
        ]
    },
    "page_179": {
        "content_preview": "178even no labeled data. The intuition is that the pretrained models learn goodrepresentations that capture intrinsic semantic structure/ information aboutthe data, and the adaptation phase customizes the model to a particulardownstream task by, e.g., retrieving the information specic to it. For ex-ample, a model pretrained on massive unlabeled image data may learn goodgeneral visual representations/features, and we adapt the representations tosolve biomedical imagining tasks.We formalize the tw...",
        "python_code": [
            "178even no labeled data. The intuition is that the pretrained models learn goodrepresentations that capture intrinsic semantic structure/ information aboutthe data, and the adaptation phase customizes the model to a particulardownstream task by, e.g., retrieving the information specic to it. For ex-ample, a model pretrained on massive unlabeled image data may learn goodgeneral visual representations/features, and we adapt the representations tosolve biomedical imagining tasks.We formalize the two phases below.Pretraining. Suppose we have an unlabeled pretraining datasetfx(1);x(2);x(n)gthat consists of nexamples in Rd. Letbe a model thatis parameterized by and maps the input xto somem-dimensional represen-tation(x). (People also call (x)2Rmthe embedding or features of theexamplex.) We pretrain the model with a pretraining loss, which is oftenan average of loss functions on all the examples: Lpre() =1nPni=1`pre(;x(i)).Here`preis a so-called self-supervised loss on a single datapoint x(i), becauseas shown later, e.g., in Section 14.3, the \\supervision\" comes from the datapointx(i)itself. It is also possible that the pretraining loss is not a sumof losses on individual examples. We will discuss two pretraining losses inSection 14.2 and Section 14.3.We use some optimizers (mostly likely SGD or ADAM [Kingma and Ba,2014]) to minimize Lpre(). We denote the obtained pretrained model by ^.Adaptation. For a downstream task, we usually have a labeled datasetf(x(1)task;y(1)task);;(x(ntask)task;y(ntask)task )gwithntaskexamples. The setting whenntask= 0 is called zero-shot learning|the downstream task doesn't have anylabeled examples. When ntaskis relatively small (say, between 1 and 50), thesetting is called few-shot learning. It's also pretty common to have a largerntaskon the order of ranging from hundreds to tens of thousands.An adaptation algorithm generally takes in a downstream dataset and thepretrained model ^, and outputs a variant of ^that solves the downstreamtask. We will discuss below two popular and general adaptation methods,linear probe and netuning. In addition, two other methods specic to lan-guage problems are introduced in 14.3.2.The linear probe approach uses a linear head on top of the representationto predict the downstream labels. Mathematically, the adapted model out-putsw>^(x), wherew2Rmis a parameter to be learned, and ^is exactlythe pretrained model (xed). We can use SGD (or other optimizers) to train"
        ],
        "formulas": [
            "1nPni=1`pre(;x(i)).Here`preis",
            "whenntask= 0"
        ],
        "explanations": []
    },
    "page_180": {
        "content_preview": "179won the downstream task loss to predict the task labelminw2Rm1ntaskntaskXi=1`task(y(i)task;w>^(x(i)task)) (14.1)E.g., if the downstream task is a regression problem, we will have`task(ytask;w>^(xtask)) = (ytaskw>^(xtask))2.The netuning algorithm uses a similar structure for the downstreamprediction model, but also further netunes the pretrained model (insteadof keeping it xed). Concretely, the prediction model is w>(x) with pa-rameterswand:We optimize both wandto t the downstream data,but ini...",
        "python_code": [
            "179won the downstream task loss to predict the task labelminw2Rm1ntaskntaskXi=1`task(y(i)task;w>^(x(i)task)) (14.1)E.g., if the downstream task is a regression problem, we will have`task(ytask;w>^(xtask)) = (ytaskw>^(xtask))2.The netuning algorithm uses a similar structure for the downstreamprediction model, but also further netunes the pretrained model (insteadof keeping it xed). Concretely, the prediction model is w>(x) with pa-rameterswand:We optimize both wandto t the downstream data,but initialize with the pretrained model ^. The linear head wis usuallyinitialized randomly.minimizew;1ntaskntaskXi=1`task(y(i)task;w>(x(i)task)) (14.2)with initialization w random vector (14.3) ^ (14.4)Various other adaptation methods exists and are sometimes specializedto the particular pretraining methods. We will discuss one of them in Sec-tion 14.3.2.14.2 Pretraining methods in computer visionThis section introduces two concrete pretraining methods for computer vi-sion: supervised pretraining and contrastive learning.Supervised pretraining. Here, the pretraining dataset is a large-scalelabeled dataset (e.g., ImageNet), and the pretrained models are simply aneural network trained with vanilla supervised learning (with the last layerbeing removed). Concretely, suppose we write the learned neural network asU^(x), whereUis the last (fully-connected) layer parameters, ^correspondsto the parameters of all the other layers, and ^(x) are the penultimateactivations layer (which serves as the representation). We simply discard Uand use^(x) as the pretrained model.Contrastive learning. Contrastive learning is a self-supervised pretrainingmethod that uses only unlabeled data. The main intuition is that a goodrepresentation function () should map semantically similar images to sim-ilar representations, and that random pair of images should generally have"
        ],
        "formulas": [
            "labelminw2Rm1ntaskntaskXi=1`task(y(i)task;w>^(x(i)task))",
            "1ntaskntaskXi=1`task(y(i)task;w>(x(i)task))"
        ],
        "explanations": []
    },
    "page_181": {
        "content_preview": "180distinct representations. E.g., we may want to map images of two huskies tosimilar representations, but a husky and an elephant should have dierentrepresentations. One denition of similarity is that images from the sameclass are similar. Using this denition will result in the so-called supervisedcontrastive algorithms that work well when labeled pretraining datasets areavailable.Without labeled data, we can use data augmentation to generate a pairof \\similar\" augmented images given an origina...",
        "python_code": [
            "180distinct representations. E.g., we may want to map images of two huskies tosimilar representations, but a husky and an elephant should have dierentrepresentations. One denition of similarity is that images from the sameclass are similar. Using this denition will result in the so-called supervisedcontrastive algorithms that work well when labeled pretraining datasets areavailable.Without labeled data, we can use data augmentation to generate a pairof \\similar\" augmented images given an original image x. Data augmenta-tion typically means that we apply random cropping, ipping, and/or colortransformation on the original image xto generate a variant. We can taketwo random augmentations, denoted by ^ xand ~x, of the same original imagex, and call them a positive pair. We observe that positive pairs of imagesare often semantically related because they are augmentations of the sameimage. We will design a loss function for such that the representations ofa positive pair, (^x);(~x), as close to each other as possible.On the other hand, we can also take another random image zfrom thepretraining dataset and generate an augmentation ^ zfromz. Note that (^ x;^z)are from dierent images; therefore, with a good chance, they are not seman-tically related. We call (^ x;^z) a negative or random pair.2We will design aloss to push the representation of random pairs, (^x);(^z), far away fromeach other.There are many recent algorithms based on the contrastive learning prin-ciple, and here we introduce SIMCLR [Chen et al., 2020] as an concreteexample. The loss function is dened on a batch of examples ( x1;;x(B))with batch size B. The algorithm computes two random augmentations foreach example x(i)in the batch, denoted by ^ x(i)and ~x(i):As a result, wehave the augmented batch of 2 Bexamples: ^x1;;^x(B), ~x1;;~x(B). TheSIMCLR loss is dened as3Lpre() =BXi=1logexp(^x(i))>(~x(i))exp ((^x(i))>(~x(i))) +Pj6=iexp ((^x(i))>(~x(j))):The intuition is as follows. The loss is increasing in (^x(i))>(~x(j)), andthus minimizing the loss encourages (^x(i))>(~x(j)) to be small, making(^x(i)) far away from (~x(j)). On the other hand, the loss is decreasing in2Random pair may be a more accurate term because it's still possible (though notlikely) that xandzare semantically related, so are ^ xand ^z. But in the literature, theterm negative pair seems to be also common.3This is a variant and simplication of the original loss that does not change the essence(but may change the eciency slightly)."
        ],
        "formulas": [
            "BXi=1logexp(^x(i))>(~x(i))exp",
            "Pj6=iexp"
        ],
        "explanations": []
    },
    "page_182": {
        "content_preview": "181(^x(i))>(~x(i)), and thus minimizing the loss encourages (^x(i))>(~x(i))to be large, resulting in (^x(i)) and(~x(i)) to be close.414.3 Pretrained large language modelsNatural language processing is another area where pretraining models areparticularly successful. In language problems, an example typically corre-sponds to a document or generally a sequence/trunk of words,5denotedbyx= (x1;;xT) whereTis the length of the document/sequence,xi2f1;;Vgare words in the document, and Vis the vocabular...",
        "python_code": [
            "181(^x(i))>(~x(i)), and thus minimizing the loss encourages (^x(i))>(~x(i))to be large, resulting in (^x(i)) and(~x(i)) to be close.414.3 Pretrained large language modelsNatural language processing is another area where pretraining models areparticularly successful. In language problems, an example typically corre-sponds to a document or generally a sequence/trunk of words,5denotedbyx= (x1;;xT) whereTis the length of the document/sequence,xi2f1;;Vgare words in the document, and Vis the vocabulary size.6A language model is a probabilistic model representing the probability ofa document, denoted by p(x1;;xT):This probability distribution is verycomplex because its support size is VT| exponential in the length of thedocument. Instead of modeling the distribution of a document itself, we canapply the chain rule of conditional probability to decompose it as follows:p(x1;;xT) =p(x1)p(x2jx1)p(xTjx1;;xT1): (14.5)Now the support size of each of the conditional probability p(xtjx1;;xt1)isV.We will model the conditional probability p(xtjx1;;xt1) as a functionofx1;:::;xt1parameterized by some parameter .A parameterized model takes in numerical inputs and therefore we rstintroduce embeddings or representations fo the words. Let ei2Rdbe theembedding of the word i2f1;2;;Vg:We call [e1;;eV]2RdVtheembedding matrix.The most commonly used model is Transformer [Vaswani et al., 2017]. Inthis subsection, we will introduce the input-output interface of a Transformer,but treat the intermediate computation in the Transformer as a blackbox. Werefer the students to the transformer paper or more advanced courses for moredetails. As shown in Figure 14.1, given a document ( x1;;xT), we rsttranslate the sequence of discrete variables into a sequence of corresponding4To see this, you can verify that the function logpp+qis decreasing in p, and increasinginqwhenp;q> 0:5In the practical implementations, typically all the data are concatenated into a singlesequence in some order, and each example typically corresponds a sub-sequence of consec-utive words which may corresponds to a subset of a document or may span across multipledocuments.6Technically, words may be decomposed into tokens which could be words or sub-words(combinations of letters), but this note omits this technicality. In fact most commons wordsare a single token themselves."
        ],
        "formulas": [
            "5denotedbyx= (x1;;xT)"
        ],
        "explanations": []
    },
    "page_183": {
        "content_preview": "182word embeddings ( ex1;;exT). We also introduce a xed special tokenx0=?in the vocabulary with corresponding embedding ex0to mark thebeginning of a document. Then, the word embeddings are passed into aTransformer model, which takes in a sequence of vectors ( ex0;ex1;;exT)and outputs a sequence of vectors ( u1;u2;;uT+1), whereut2RVwill beinterpreted as the logits for the probability distribution of the next word.Here we use the autoregressive version of the Transformers which by designensuresuto...",
        "python_code": [
            "182word embeddings ( ex1;;exT). We also introduce a xed special tokenx0=?in the vocabulary with corresponding embedding ex0to mark thebeginning of a document. Then, the word embeddings are passed into aTransformer model, which takes in a sequence of vectors ( ex0;ex1;;exT)and outputs a sequence of vectors ( u1;u2;;uT+1), whereut2RVwill beinterpreted as the logits for the probability distribution of the next word.Here we use the autoregressive version of the Transformers which by designensuresutonly depends on x1;;xt1(note that this property does nothold in masked language models [Devlin et al., 2019] where the losses arealso dierent.) We view the whole mapping from x's tou's a blackbox inthis subsection and call it a Transformer, denoted it by f, whereincludeboth the parameters in the Transformer and the input embeddings. We writeut=f(x0;x1;:::;xt1) wherefdenotes the mapping from the input to theoutputs.𝑥!𝑥\"𝑥#𝑒$!𝑒$\"𝑒$#…Transformer 𝑓%(𝑥)𝑥&𝑒$$𝑢\"𝑢'𝑢#(!𝑢!…Figure 14.1: The inputs and outputs of a Transformer model.The conditional probability p(xtjx1;;xt1) is the softtmax of the logits:26664p(xt= 1jx1;xt1)p(xt= 2jx1;xt1)...p(xt=Vjx1;xt1)37775= softmax( ut)2RV(14.6)= softmax( f(x0;:::;xt1)) (14.7)We train the Transformer parameter by minimizing the negative log-likelihood of seeing the data under the probabilistic model dened by ,"
        ],
        "formulas": [
            "tokenx0=?in",
            "writeut=f(x0;x1;:::;xt1)",
            "xt= 1jx1;xt1)p(xt=",
            "xt=Vjx1;xt1)37775="
        ],
        "explanations": []
    },
    "page_184": {
        "content_preview": "183which is the cross-entropy loss on the logitis.loss() =1TTXt=1log(p(xtjx1;:::;xt1)) (14.8)=1TTXt=1`ce(f(x0;x1;;xt1);xt)=1TTXt=1log(softmax( f(x0;x1;;xt1))xt):Autoregressive text decoding / generation. Given a autoregressiveTransformer, we can simply sample text from it sequentially. Given a pre-xx1;:::xt, we generate text completion xt+1;:::xTsequentially using theconditional distribution.xt+1softmax(f(x0;x1;;xt)) (14.9)xt+2softmax(f(x0;x1;;xt+1)) (14.10)::: (14.11)xTsoftmax(f(x0;x1;;xT1)): (...",
        "python_code": [
            "183which is the cross-entropy loss on the logitis.loss() =1TTXt=1log(p(xtjx1;:::;xt1)) (14.8)=1TTXt=1`ce(f(x0;x1;;xt1);xt)=1TTXt=1log(softmax( f(x0;x1;;xt1))xt):Autoregressive text decoding / generation. Given a autoregressiveTransformer, we can simply sample text from it sequentially. Given a pre-xx1;:::xt, we generate text completion xt+1;:::xTsequentially using theconditional distribution.xt+1softmax(f(x0;x1;;xt)) (14.9)xt+2softmax(f(x0;x1;;xt+1)) (14.10)::: (14.11)xTsoftmax(f(x0;x1;;xT1)): (14.12)Note that each generated token is used as the input to the model when gen-erating the following tokens. In practice, people often introduce a parameter > 0 named temperature to further adjust the entropy/sharpness of thegenerated distribution,xt+1softmax(f(x0;x1;;xt)=) (14.13)xt+2softmax(f(x0;x1;;xt+1)=) (14.14)::: (14.15)xTsoftmax(f(x0;x1;;xT1)=): (14.16)When= 1, the text is sampled from the original conditional probabilitydened by the model. With a decreasing , the generated text graduallybecomes more \\deterministic\". !0 reduces to greedy decoding, where wegenerate the most probable next token from the conditional probability.14.3.1 Zero-shot learning and in-context learningFor language models, there are many ways to adapt a pretrained model todownstream tasks. In this notes, we discuss three of them: netuning, zero-shot learning, and in-context learning."
        ],
        "formulas": [
            "1TTXt=1log(p(xtjx1;:::;xt1))",
            "1TTXt=1`ce(f(x0;x1;;xt1);xt)=1TTXt=1log(softmax(",
            "When= 1,"
        ],
        "explanations": []
    },
    "page_185": {
        "content_preview": "184Finetuning is not very common for the autoregressive language models thatwe introduced in Section 14.3 but much more common for other variantssuch as masked language models which has similar input-output interfacesbut are pretrained dierently [Devlin et al., 2019]. The netuning method isthe same as introduced generally in Section 14.1|the only question is howwe dene the prediction task with an additional linear head. One optionis to treat cT+1=(x1;;xT) as the representation and use w>cT+1=w>(...",
        "python_code": [
            "184Finetuning is not very common for the autoregressive language models thatwe introduced in Section 14.3 but much more common for other variantssuch as masked language models which has similar input-output interfacesbut are pretrained dierently [Devlin et al., 2019]. The netuning method isthe same as introduced generally in Section 14.1|the only question is howwe dene the prediction task with an additional linear head. One optionis to treat cT+1=(x1;;xT) as the representation and use w>cT+1=w>(x1;;xT) to predict task label. As described in Section 14.1, weinitializeto the pretrained model ^and then optimize both wand.Zero-shot adaptation or zero-shot learning is the setting where there is noinput-output pairs from the downstream tasks. For language problems tasks,typically the task is formatted as a question or a cloze test form via naturallanguage. For example, we can format an example as a question:xtask= (xtask;1;;xtask;T) = \\Is the speed of light a universal constant?\"Then, we compute the most likely next word predicted by the lan-guage model given this question, that is, computing argmaxxT+1p(xT+1jxtask;1;;xtask;T). In this case, if the most likely next word xT+1is \\No\",then we solve the task. (The speed of light is only a constant in vacuum).We note that there are many ways to decode the answer from the languagemodels, e.g., instead of computing the argmax, we may use the languagemodel to generate a few words word. It is an active research question to ndthe best way to utilize the language models.In-context learning is mostly used for few-shot settings where we have afew labeled examples ( x(1)task;y(1)task);;(x(ntask)task;y(ntask)task ). Given a test examplextest, we construct a document ( x1;;xT), which is more commonly calleda \\prompt\" in this context, by concatenating the labeled examples and thetext example in some format. For example, we may construct the prompt asfollowsx1;;xT= \\Q: 23 = ? x(1)taskA: 5 y(1)taskQ: 67 = ? x(2)taskA: 13 y(2)taskQ: 152 = ?\" xtest"
        ],
        "formulas": [
            "1=(x1;;xT)",
            "1=w>(x1;;xT)",
            "xtask= (xtask;1;;xtask;T)",
            "xT= \\Q:",
            "23 = ?",
            "67 = ?",
            "152 = ?\""
        ],
        "explanations": []
    },
    "page_186": {
        "content_preview": "185Then, we let the pretrained model generate the most likely xT+1;xT+2;:In this case, if the model can \\learn\" that the symbol means addition fromthe few examples, we will obtain the following which suggests the answer is17.xT+1;xT+2;=\\A: 17\":The area of foundation models is very new and quickly growing. The noteshere only attempt to introduce these models on a conceptual level with asignicant amount of simplication. We refer the readers to other materials,e.g., Bommasani et al. [2021], for mor...",
        "python_code": [
            "185Then, we let the pretrained model generate the most likely xT+1;xT+2;:In this case, if the model can \\learn\" that the symbol means addition fromthe few examples, we will obtain the following which suggests the answer is17.xT+1;xT+2;=\\A: 17\":The area of foundation models is very new and quickly growing. The noteshere only attempt to introduce these models on a conceptual level with asignicant amount of simplication. We refer the readers to other materials,e.g., Bommasani et al. [2021], for more details."
        ],
        "formulas": [],
        "explanations": []
    },
    "page_187": {
        "content_preview": "Part VReinforcement Learning andControl186...",
        "python_code": [],
        "formulas": [],
        "explanations": [
            "Part VReinforcement Learning andControl186"
        ]
    },
    "page_188": {
        "content_preview": "Chapter 15Reinforcement learningWe now begin our study of reinforcement learning and adaptive control.In supervised learning, we saw algorithms that tried to make their outputsmimic the labels ygiven in the training set. In that setting, the labels gavean unambiguous \\right answer\" for each of the inputs x. In contrast, formany sequential decision making and control problems, it is very dicult toprovide this type of explicit supervision to a learning algorithm. For example,if we have just built ...",
        "python_code": [],
        "formulas": [],
        "explanations": [
            "Chapter 15Reinforcement learningWe now begin our study of reinforcement learning and adaptive control",
            "In supervised learning, we saw algorithms that tried to make their outputsmimic the labels ygiven in the training set",
            "In that setting, the labels gavean unambiguous \\right answer\" for each of the inputs x",
            "In contrast, formany sequential decision making and control problems, it is very dicult toprovide this type of explicit supervision to a learning algorithm",
            "For example,if we have just built a four-legged robot and are trying to program it to walk,then initially we have no idea what the \\correct\" actions to take are to makeit walk, and so do not know how to provide explicit supervision for a learningalgorithm to try to mimic",
            "In the reinforcement learning framework, we will instead provide our al-gorithms only a reward function, which indicates to the learning agent whenit is doing well, and when it is doing poorly",
            "In the four-legged walking ex-ample, the reward function might give the robot positive rewards for movingforwards, and negative rewards for either moving backwards or falling over",
            "It will then be the learning algorithm's job to gure out how to choose actionsover time so as to obtain large rewards",
            "Reinforcement learning has been successful in applications as diverse asautonomous helicopter ight, robot legged locomotion, cell-phone networkrouting, marketing strategy selection, factory control, and ecient web-pageindexing",
            "Our study of reinforcement learning will begin with a denition oftheMarkov decision processes (MDP) , which provides the formalism inwhich RL problems are usually posed"
        ]
    },
    "page_189": {
        "content_preview": "18815.1 Markov decision processesA Markov decision process is a tuple ( S;A;fPsag;;R ), where:•Sis a set of states . (For example, in autonomous helicopter ight, Smight be the set of all possible positions and orientations of the heli-copter.)•Ais a set of actions . (For example, the set of all possible directions inwhich you can push the helicopter's control sticks.)•Psaare the state transition probabilities. For each state s2Sandactiona2A,Psais a distribution over the state space. We'll say mo...",
        "python_code": [],
        "formulas": [],
        "explanations": [
            "1 Markov decision processesA Markov decision process is a tuple ( S;A;fPsag;;R ), where:•Sis a set of states",
            "(For example, in autonomous helicopter ight, Smight be the set of all possible positions and orientations of the heli-copter",
            ")•Ais a set of actions",
            "(For example, the set of all possible directions inwhich you can push the helicopter's control sticks",
            ")•Psaare the state transition probabilities",
            "For each state s2Sandactiona2A,Psais a distribution over the state space",
            "We'll say moreabout this later, but briey, Psagives the distribution over what stateswe will transition to if we take action ain states",
            "•2[0;1) is called the discount factor",
            "•R:SA7!Ris the reward function",
            "(Rewards are sometimes alsowritten as a function of a state Sonly, in which case we would haveR:S7!R)",
            "The dynamics of an MDP proceeds as follows: We start in some state s0,and get to choose some action a02Ato take in the MDP",
            "As a result of ourchoice, the state of the MDP randomly transitions to some successor states1, drawn according to s1Ps0a0",
            "Then, we get to pick another action a1",
            "As a result of this action, the state transitions again, now to some s2Ps1a1",
            "We then pick a2, and so on",
            "Pictorially, we can represent this process asfollows:s0a0!s1a1!s2a2!s3a3!:::Upon visiting the sequence of states s0;s1;:::with actions a0;a1;:::, ourtotal payo is given byR(s0;a0) +R(s1;a1) +2R(s2;a2) +:Or, when we are writing rewards as a function of the states only, this becomesR(s0) +R(s1) +2R(s2) +:For most of our development, we will use the simpler state-rewards R(s),though the generalization to state-action rewards R(s;a) oers no specialdiculties"
        ]
    },
    "page_190": {
        "content_preview": "189Our goal in reinforcement learning is to choose actions over time so as tomaximize the expected value of the total payo:ER(s0) +R(s1) +2R(s2) +Note that the reward at timestep tisdiscounted by a factor of t. Thus, tomake this expectation large, we would like to accrue positive rewards as soonas possible (and postpone negative rewards as long as possible). In economicapplications where R() is the amount of money made, also has a naturalinterpretation in terms of the interest rate (where a doll...",
        "python_code": [
            "189Our goal in reinforcement learning is to choose actions over time so as tomaximize the expected value of the total payo:ER(s0) +R(s1) +2R(s2) +Note that the reward at timestep tisdiscounted by a factor of t. Thus, tomake this expectation large, we would like to accrue positive rewards as soonas possible (and postpone negative rewards as long as possible). In economicapplications where R() is the amount of money made, also has a naturalinterpretation in terms of the interest rate (where a dollar today is worthmore than a dollar tomorrow).Apolicy is any function :S7!Amapping from the states to theactions. We say that we are executing some policy if, whenever we arein states, we take action a=(s). We also dene the value function fora policyaccording toV(s) = ER(s0) +R(s1) +2R(s2) +s0=s;]:V(s) is simply the expected sum of discounted rewards upon starting instates, and taking actions according to .1Given a xed policy , its value function Vsatises the Bellman equa-tions :V(s) =R(s) +Xs02SPs(s)(s0)V(s0):This says that the expected sum of discounted rewards V(s) for startinginsconsists of two terms: First, the immediate reward R(s) that we getright away simply for starting in state s, and second, the expected sum offuture discounted rewards. Examining the second term in more detail, wesee that the summation term above can be rewritten E s0Ps(s)[V(s0)]. Thisis the expected sum of discounted rewards for starting in state s0, wheres0is distributed according Ps(s), which is the distribution over where we willend up after taking the rst action (s) in the MDP from state s. Thus, thesecond term above gives the expected sum of discounted rewards obtainedafter the rst step in the MDP.Bellman's equations can be used to eciently solve for V. Specically,in a nite-state MDP ( jSj<1), we can write down one such equation forV(s) for every state s. This gives us a set of jSjlinear equations in jSjvariables (the unknown V(s)'s, one for each state), which can be ecientlysolved for the V(s)'s.1This notation in which we condition on isn't technically correct because isn't arandom variable, but this is quite standard in the literature."
        ],
        "formulas": [
            "a=(s).",
            "s0=s;]:V(s)"
        ],
        "explanations": []
    },
    "page_191": {
        "content_preview": "190We also dene the optimal value function according toV(s) = maxV(s): (15.1)In other words, this is the best possible expected sum of discounted rewardsthat can be attained using any policy. There is also a version of Bellman'sequations for the optimal value function:V(s) =R(s) + maxa2AXs02SPsa(s0)V(s0): (15.2)The rst term above is the immediate reward as before. The second termis the maximum over all actions aof the expected future sum of discountedrewards we'll get upon after action a. You sh...",
        "python_code": [
            "190We also dene the optimal value function according toV(s) = maxV(s): (15.1)In other words, this is the best possible expected sum of discounted rewardsthat can be attained using any policy. There is also a version of Bellman'sequations for the optimal value function:V(s) =R(s) + maxa2AXs02SPsa(s0)V(s0): (15.2)The rst term above is the immediate reward as before. The second termis the maximum over all actions aof the expected future sum of discountedrewards we'll get upon after action a. You should make sure you understandthis equation and see why it makes sense.We also dene a policy :S7!Aas follows:(s) = arg maxa2AXs02SPsa(s0)V(s0): (15.3)Note that(s) gives the action athat attains the maximum in the \\max\"in Equation (15.2).It is a fact that for every state sand every policy , we haveV(s) =V(s)V(s):The rst equality says that the V, the value function for , is equal to theoptimal value function Vfor every state s. Further, the inequality abovesays that's value is at least a large as the value of any other other policy.In other words, as dened in Equation (15.3) is the optimal policy.Note thathas the interesting property that it is the optimal policyforallstatess. Specically, it is not the case that if we were starting insome state sthen there'd be some optimal policy for that state, and if wewere starting in some other state s0then there'd be some other policy that'soptimal policy for s0. The same policy attains the maximum in Equa-tion (15.1) for allstatess. This means that we can use the same policy no matter what the initial state of our MDP is.15.2 Value iteration and policy iterationWe now describe two ecient algorithms for solving nite-state MDPs. Fornow, we will consider only MDPs with nite state and action spaces ( jSj<"
        ],
        "formulas": [],
        "explanations": []
    },
    "page_192": {
        "content_preview": "1911;jAj<1). In this section, we will also assume that we know the statetransition probabilities fPsagand the reward function R.The rst algorithm, value iteration , is as follows:Algorithm 4 Value Iteration1:For each state s, initializeV(s) := 0.2:foruntil convergence do3: For every state, updateV(s) :=R(s) + maxa2AXs0Psa(s0)V(s0): (15.4)This algorithm can be thought of as repeatedly trying to update theestimated value function using Bellman Equations (15.2).There are two possible ways of perfor...",
        "python_code": [
            "1911;jAj<1). In this section, we will also assume that we know the statetransition probabilities fPsagand the reward function R.The rst algorithm, value iteration , is as follows:Algorithm 4 Value Iteration1:For each state s, initializeV(s) := 0.2:foruntil convergence do3: For every state, updateV(s) :=R(s) + maxa2AXs0Psa(s0)V(s0): (15.4)This algorithm can be thought of as repeatedly trying to update theestimated value function using Bellman Equations (15.2).There are two possible ways of performing the updates in the inner loop ofthe algorithm. In the rst, we can rst compute the new values for V(s) forevery state s, and then overwrite all the old values with the new values. Thisis called a synchronous update. In this case, the algorithm can be viewed asimplementing a \\Bellman backup operator\" that takes a current estimate ofthe value function, and maps it to a new estimate. (See homework problemfor details.) Alternatively, we can also perform asynchronous updates.Here, we would loop over the states (in some order), updating the values oneat a time.Under either synchronous or asynchronous updates, it can be shown thatvalue iteration will cause Vto converge to V. Having found V, we canthen use Equation (15.3) to nd the optimal policy.Apart from value iteration, there is a second standard algorithm for nd-ing an optimal policy for an MDP. The policy iteration algorithm proceedsas follows:Thus, the inner-loop repeatedly computes the value function for the cur-rent policy, and then updates the policy using the current value function.(The policy found in step (b) is also called the policy that is greedy withrespect to V.) Note that step (a) can be done via solving Bellman's equa-tions as described earlier, which in the case of a xed policy, is just a set ofjSjlinear equations in jSjvariables.After at most a nite number of iterations of this algorithm, Vwill con-verge toV, andwill converge to .22Note that value iteration cannot reach the exact Vin a nite number of iterations,"
        ],
        "formulas": [],
        "explanations": []
    },
    "page_193": {
        "content_preview": "192Algorithm 5 Policy Iteration1:Initializerandomly.2:foruntil convergence do3: LetV:=V. .typically by linear system solver4: For each state s, let(s) := arg maxa2AXs0Psa(s0)V(s0):Both value iteration and policy iteration are standard algorithms for solv-ing MDPs, and there isn't currently universal agreement over which algo-rithm is better. For small MDPs, policy iteration is often very fats andconverges with very few iterations. However, for MDPs with large statespaces, solving for Vexplicitly...",
        "python_code": [
            "192Algorithm 5 Policy Iteration1:Initializerandomly.2:foruntil convergence do3: LetV:=V. .typically by linear system solver4: For each state s, let(s) := arg maxa2AXs0Psa(s0)V(s0):Both value iteration and policy iteration are standard algorithms for solv-ing MDPs, and there isn't currently universal agreement over which algo-rithm is better. For small MDPs, policy iteration is often very fats andconverges with very few iterations. However, for MDPs with large statespaces, solving for Vexplicitly would involve solving a large system of lin-ear equations, and could be dicult (and note that one has to solve thelinear system multiple times in policy iteration). In these problems, valueiteration may be preferred. For this reason, in practice value iteration seemsto be used more often than policy iteration. For some more discussions onthe comparison and connection of value iteration and policy iteration, pleasesee Section 15.5.15.3 Learning a model for an MDPSo far, we have discussed MDPs and algorithms for MDPs assuming that thestate transition probabilities and rewards are known. In many realistic prob-lems, we are not given state transition probabilities and rewards explicitly,but must instead estimate them from data. (Usually, S;A andare known.)For example, suppose that, for the inverted pendulum problem (see prob-whereas policy iteration with an exact linear system solver, can. This is because whenthe actions space and policy space are discrete and nite, and once the policy reaches theoptimal policy in policy iteration, then it will not change at all. On the other hand, eventhough value iteration will converge to the V, but there is always some non-zero error inthe learned value function."
        ],
        "formulas": [],
        "explanations": []
    },
    "page_194": {
        "content_preview": "193lem set 4), we had a number of trials in the MDP, that proceeded as follows:s(1)0a(1)0!s(1)1a(1)1!s(1)2a(1)2!s(1)3a(1)3!:::s(2)0a(2)0!s(2)1a(2)1!s(2)2a(2)2!s(2)3a(2)3!::::::Here,s(j)iis the state we were at time iof trialj, anda(j)iis the cor-responding action that was taken from that state. In practice, each of thetrials above might be run until the MDP terminates (such as if the pole fallsover in the inverted pendulum problem), or it might be run for some largebut nite number of timesteps.G...",
        "python_code": [
            "193lem set 4), we had a number of trials in the MDP, that proceeded as follows:s(1)0a(1)0!s(1)1a(1)1!s(1)2a(1)2!s(1)3a(1)3!:::s(2)0a(2)0!s(2)1a(2)1!s(2)2a(2)2!s(2)3a(2)3!::::::Here,s(j)iis the state we were at time iof trialj, anda(j)iis the cor-responding action that was taken from that state. In practice, each of thetrials above might be run until the MDP terminates (such as if the pole fallsover in the inverted pendulum problem), or it might be run for some largebut nite number of timesteps.Given this \\experience\" in the MDP consisting of a number of trials,we can then easily derive the maximum likelihood estimates for the statetransition probabilities:Psa(s0) =#times took we action ain statesand got to s0#times we took action a in state s(15.5)Or, if the ratio above is \\0/0\"|corresponding to the case of never havingtaken action ain statesbefore|the we might simply estimate Psa(s0) to be1=jSj. (I.e., estimate Psato be the uniform distribution over all states.)Note that, if we gain more experience (observe more trials) in the MDP,there is an ecient way to update our estimated state transition probabilitiesusing the new experience. Specically, if we keep around the counts for boththe numerator and denominator terms of (15.5), then as we observe moretrials, we can simply keep accumulating those counts. Computing the ratioof these counts then given our estimate of Psa.Using a similar procedure, if Ris unknown, we can also pick our estimateof the expected immediate reward R(s) in statesto be the average rewardobserved in state s.Having learned a model for the MDP, we can then use either value it-eration or policy iteration to solve the MDP using the estimated transitionprobabilities and rewards. For example, putting together model learning andvalue iteration, here is one possible algorithm for learning in an MDP withunknown state transition probabilities:1. Initialize randomly.2. Repeatf(a) Execute in the MDP for some number of trials."
        ],
        "formulas": [
            "be1=jSj."
        ],
        "explanations": []
    },
    "page_195": {
        "content_preview": "194(b) Using the accumulated experience in the MDP, update our esti-mates forPsa(andR, if applicable).(c) Apply value iteration with the estimated state transition probabil-ities and rewards to get a new estimated value function V.(d) Update to be the greedy policy with respect to V.gWe note that, for this particular algorithm, there is one simple optimiza-tion that can make it run much more quickly. Specically, in the inner loopof the algorithm where we apply value iteration, if instead of init...",
        "python_code": [
            "194(b) Using the accumulated experience in the MDP, update our esti-mates forPsa(andR, if applicable).(c) Apply value iteration with the estimated state transition probabil-ities and rewards to get a new estimated value function V.(d) Update to be the greedy policy with respect to V.gWe note that, for this particular algorithm, there is one simple optimiza-tion that can make it run much more quickly. Specically, in the inner loopof the algorithm where we apply value iteration, if instead of initializing valueiteration with V= 0, we initialize it with the solution found during the pre-vious iteration of our algorithm, then that will provide value iteration witha much better initial starting point and make it converge more quickly.15.4 Continuous state MDPsSo far, we've focused our attention on MDPs with a nite number of states.We now discuss algorithms for MDPs that may have an innite number ofstates. For example, for a car, we might represent the state as ( x;y;; _x;_y;_),comprising its position ( x;y); orientation ; velocity in the xandydirections_xand _y; and angular velocity _. Hence,S=R6is an innite set of states,because there is an innite number of possible positions and orientationsfor the car.3Similarly, the inverted pendulum you saw in PS4 has states(x;; _x;_), whereis the angle of the pole. And, a helicopter ying in 3dspace has states of the form ( x;y;z;;; ; _x;_y;_z;_;_;_ ), where here the roll, pitch, and yaw angles specify the 3d orientation of the helicopter.In this section, we will consider settings where the state space is S=Rd,and describe ways for solving such MDPs.15.4.1 DiscretizationPerhaps the simplest way to solve a continuous-state MDP is to discretizethe state space, and then to use an algorithm like value iteration or policyiteration, as described previously.For example, if we have 2d states ( s1;s2), we can use a grid to discretizethe state space:3Technically, is an orientation and so the range of is better written 2[;) than2R; but for our purposes, this distinction is not important."
        ],
        "formulas": [
            "V= 0,",
            "S=R6is",
            "S=Rd,and"
        ],
        "explanations": []
    },
    "page_196": {
        "content_preview": "195[t]Here, each grid cell represents a separate discrete state s. We canthen approximate the continuous-state MDP via a discrete-state one(S;A;fPsag;;R ), where Sis the set of discrete states, fPsagare our statetransition probabilities over the discrete states, and so on. We can then usevalue iteration or policy iteration to solve for the V(s) and(s) in thediscrete state MDP ( S;A;fPsag;;R ). When our actual system is in somecontinuous-valued state s2Sand we need to pick an action to execute, w...",
        "python_code": [],
        "formulas": [],
        "explanations": [
            "195[t]Here, each grid cell represents a separate discrete state s",
            "We canthen approximate the continuous-state MDP via a discrete-state one(S;A;fPsag;;R ), where Sis the set of discrete states, fPsagare our statetransition probabilities over the discrete states, and so on",
            "We can then usevalue iteration or policy iteration to solve for the V(s) and(s) in thediscrete state MDP ( S;A;fPsag;;R )",
            "When our actual system is in somecontinuous-valued state s2Sand we need to pick an action to execute, wecompute the corresponding discretized state s, and execute action (s)",
            "This discretization approach can work well for many problems",
            "However,there are two downsides",
            "First, it uses a fairly naive representation for V(and)",
            "Specically, it assumes that the value function is takes a constantvalue over each of the discretization intervals (i",
            ", that the value function ispiecewise constant in each of the gridcells)",
            "To better understand the limitations of such a representation, consider asupervised learning problem of tting a function to this dataset:[t]1 2 3 4 5 6 7 81"
        ]
    },
    "page_197": {
        "content_preview": "196Clearly, linear regression would do ne on this problem. However, if weinstead discretize the x-axis, and then use a representation that is piecewiseconstant in each of the discretization intervals, then our t to the data wouldlook like this:[t]1 2 3 4 5 6 7 81.522.533.544.555.5xyThis piecewise constant representation just isn't a good representation formany smooth functions. It results in little smoothing over the inputs, and nogeneralization over the dierent grid cells. Using this sort of re...",
        "python_code": [
            "196Clearly, linear regression would do ne on this problem. However, if weinstead discretize the x-axis, and then use a representation that is piecewiseconstant in each of the discretization intervals, then our t to the data wouldlook like this:[t]1 2 3 4 5 6 7 81.522.533.544.555.5xyThis piecewise constant representation just isn't a good representation formany smooth functions. It results in little smoothing over the inputs, and nogeneralization over the dierent grid cells. Using this sort of representation,we would also need a very ne discretization (very small grid cells) to get agood approximation.A second downside of this representation is called the curse of dimen-sionality . SupposeS=Rd, and we discretize each of the ddimensions of thestate intokvalues. Then the total number of discrete states we have is kd.This grows exponentially quickly in the dimension of the state space d, andthus does not scale well to large problems. For example, with a 10d state, ifwe discretize each state variable into 100 values, we would have 10010= 1020discrete states, which is far too many to represent even on a modern desktopcomputer.As a rule of thumb, discretization usually works extremely well for 1dand 2d problems (and has the advantage of being simple and quick to im-plement). Perhaps with a little bit of cleverness and some care in choosingthe discretization method, it often works well for problems with up to 4dstates. If you're extremely clever, and somewhat lucky, you may even get itto work for some 6d problems. But it very rarely works for problems anyhigher dimensional than that."
        ],
        "formulas": [
            "SupposeS=Rd,",
            "10010= 1020discrete"
        ],
        "explanations": []
    },
    "page_198": {
        "content_preview": "19715.4.2 Value function approximationWe now describe an alternative method for nding policies in continuous-state MDPs, in which we approximate Vdirectly, without resorting to dis-cretization. This approach, called value function approximation, has beensuccessfully applied to many RL problems.Using a model or simulatorTo develop a value function approximation algorithm, we will assume thatwe have a model , orsimulator , for the MDP. Informally, a simulator isa black-box that takes as input any ...",
        "python_code": [],
        "formulas": [],
        "explanations": [
            "2 Value function approximationWe now describe an alternative method for nding policies in continuous-state MDPs, in which we approximate Vdirectly, without resorting to dis-cretization",
            "This approach, called value function approximation, has beensuccessfully applied to many RL problems",
            "Using a model or simulatorTo develop a value function approximation algorithm, we will assume thatwe have a model , orsimulator , for the MDP",
            "Informally, a simulator isa black-box that takes as input any (continuous-valued) state stand actionat, and outputs a next-state st+1sampled according to the state transitionprobabilities Pstat:[t]There are several ways that one can get such a model",
            "One is to usephysics simulation",
            "For example, the simulator for the inverted pendulumin PS4 was obtained by using the laws of physics to calculate what positionand orientation the cart/pole will be in at time t+ 1, given the current stateat timetand the action ataken, assuming that we know all the parametersof the system such as the length of the pole, the mass of the pole, and soon",
            "Alternatively, one can also use an o-the-shelf physics simulation softwarepackage which takes as input a complete physical description of a mechanicalsystem, the current state stand actionat, and computes the state st+1of thesystem a small fraction of a second into the future",
            "4An alternative way to get a model is to learn one from data collected inthe MDP",
            "For example, suppose we execute ntrials in which we repeatedlytake actions in an MDP, each trial for Ttimesteps",
            "This can be done pickingactions at random, executing some specic policy, or via some other way of4Open Dynamics Engine (http://www",
            "com) is one example of a free/open-sourcephysics simulator that can be used to simulate systems like the inverted pendulum, andthat has been a reasonably popular choice among RL researchers"
        ]
    },
    "page_199": {
        "content_preview": "198choosing actions. We would then observe nstate sequences like the following:s(1)0a(1)0!s(1)1a(1)1!s(1)2a(1)2!a(1)T1!s(1)Ts(2)0a(2)0!s(2)1a(2)1!s(2)2a(2)2!a(2)T1!s(2)Ts(n)0a(n)0!s(n)1a(n)1!s(n)2a(n)2!a(n)T1!s(n)TWe can then apply a learning algorithm to predict st+1as a function of standat.For example, one may choose to learn a linear model of the formst+1=Ast+Bat; (15.6)using an algorithm similar to linear regression. Here, the parameters of themodel are the matrices AandB, and we can estimat...",
        "python_code": [
            "198choosing actions. We would then observe nstate sequences like the following:s(1)0a(1)0!s(1)1a(1)1!s(1)2a(1)2!a(1)T1!s(1)Ts(2)0a(2)0!s(2)1a(2)1!s(2)2a(2)2!a(2)T1!s(2)Ts(n)0a(n)0!s(n)1a(n)1!s(n)2a(n)2!a(n)T1!s(n)TWe can then apply a learning algorithm to predict st+1as a function of standat.For example, one may choose to learn a linear model of the formst+1=Ast+Bat; (15.6)using an algorithm similar to linear regression. Here, the parameters of themodel are the matrices AandB, and we can estimate them using the datacollected from our ntrials, by pickingarg minA;BnXi=1T1Xt=0s(i)t+1As(i)t+Ba(i)t22:We could also potentially use other loss functions for learning the model.For example, it has been found in recent work Luo et al. [2018] that usingkk 2norm (without the square) may be helpful in certain cases.Having learned AandB, one option is to build a deterministic model,in which given an input standat, the output st+1is exactly determined.Specically, we always compute st+1according to Equation (15.6). Alter-natively, we may also build a stochastic model, in which st+1is a randomfunction of the inputs, by modeling it asst+1=Ast+Bat+t;where here tis a noise term, usually modeled as tN(0;). (The covari-ance matrix can also be estimated from data in a straightforward way.)Here, we've written the next-state st+1as a linear function of the currentstate and action; but of course, non-linear functions are also possible. Specif-ically, one can learn a model st+1=As(st) +Ba(at), wheresandaaresome non-linear feature mappings of the states and actions. Alternatively,one can also use non-linear learning algorithms, such as locally weighted lin-ear regression, to learn to estimate st+1as a function of standat. Theseapproaches can also be used to build either deterministic or stochastic sim-ulators of an MDP."
        ],
        "formulas": [
            "1=Ast+Bat;",
            "BnXi=1T1Xt=0s(i)t+1As(i)t+Ba(i)t22:We",
            "1=Ast+Bat+t;where",
            "1=As(st)"
        ],
        "explanations": []
    },
    "page_200": {
        "content_preview": "199Fitted value iterationWe now describe the tted value iteration algorithm for approximatingthe value function of a continuous state MDP. In the sequel, we will assumethat the problem has a continuous state space S=Rd, but that the actionspaceAis small and discrete.5Recall that in value iteration, we would like to perform the updateV(s) :=R(s) +maxaZs0Psa(s0)V(s0)ds0(15.7)=R(s) +maxaEs0Psa[V(s0)] (15.8)(In Section 15.2, we had written the value iteration update with a summationV(s) :=R(s) +maxa...",
        "python_code": [
            "199Fitted value iterationWe now describe the tted value iteration algorithm for approximatingthe value function of a continuous state MDP. In the sequel, we will assumethat the problem has a continuous state space S=Rd, but that the actionspaceAis small and discrete.5Recall that in value iteration, we would like to perform the updateV(s) :=R(s) +maxaZs0Psa(s0)V(s0)ds0(15.7)=R(s) +maxaEs0Psa[V(s0)] (15.8)(In Section 15.2, we had written the value iteration update with a summationV(s) :=R(s) +maxaPs0Psa(s0)V(s0) rather than an integral over states;the new notation reects that we are now working in continuous states ratherthan discrete states.)The main idea of tted value iteration is that we are going to approxi-mately carry out this step, over a nite sample of states s(1);:::;s(n). Specif-ically, we will use a supervised learning algorithm|linear regression in ourdescription below|to approximate the value function as a linear or non-linearfunction of the states:V(s) =T(s):Here,is some appropriate feature mapping of the states.For each state sin our nite sample of nstates, tted value iterationwill rst compute a quantity y(i), which will be our approximation to R(s) +maxaEs0Psa[V(s0)] (the right hand side of Equation 15.8). Then, it willapply a supervised learning algorithm to try to get V(s) close toR(s) +maxaEs0Psa[V(s0)] (or, in other words, to try to get V(s) close toy(i)).In detail, the algorithm is as follows:1. Randomly sample nstatess(1);s(2);:::s(n)2S.2. Initialize := 0.3. RepeatfFori= 1;:::;nf5In practice, most MDPs have much smaller action spaces than state spaces. E.g., a carhas a 6d state space, and a 2d action space (steering and velocity controls); the invertedpendulum has a 4d state space, and a 1d action space; a helicopter has a 12d state space,and a 4d action space. So, discretizing this set of actions is usually less of a problem thandiscretizing the state space would have been."
        ],
        "formulas": [
            "S=Rd,",
            "RepeatfFori= 1;:::;nf5In"
        ],
        "explanations": []
    },
    "page_201": {
        "content_preview": "200For each action a2AfSamples01;:::;s0kPs(i)a(using a model of the MDP).Setq(a) =1kPkj=1R(s(i)) +V(s0j)==Hence,q(a) is an estimate of R(s(i)) +Es0Ps(i)a[V(s0)].gSety(i)= maxaq(a).==Hence,y(i)is an estimate of R(s(i)) +maxaEs0Ps(i)a[V(s0)].g==In the original value iteration algorithm (over discrete states)==we updated the value function according to V(s(i)) :=y(i).==In this algorithm, we want V(s(i))y(i), which we'll achieve==using supervised learning (linear regression).Set:= arg min 12Pni=1T(s...",
        "python_code": [
            "200For each action a2AfSamples01;:::;s0kPs(i)a(using a model of the MDP).Setq(a) =1kPkj=1R(s(i)) +V(s0j)==Hence,q(a) is an estimate of R(s(i)) +Es0Ps(i)a[V(s0)].gSety(i)= maxaq(a).==Hence,y(i)is an estimate of R(s(i)) +maxaEs0Ps(i)a[V(s0)].g==In the original value iteration algorithm (over discrete states)==we updated the value function according to V(s(i)) :=y(i).==In this algorithm, we want V(s(i))y(i), which we'll achieve==using supervised learning (linear regression).Set:= arg min 12Pni=1T(s(i))y(i)2gAbove, we had written out tted value iteration using linear regressionas the algorithm to try to make V(s(i)) close toy(i). That step of the algo-rithm is completely analogous to a standard supervised learning (regression)problem in which we have a training set ( x(1);y(1));(x(2);y(2));:::; (x(n);y(n)),and want to learn a function mapping from xtoy; the only dierence is thatheresplays the role of x. Even though our description above used linear re-gression, clearly other regression algorithms (such as locally weighted linearregression) can also be used.Unlike value iteration over a discrete set of states, tted value iterationcannot be proved to always to converge. However, in practice, it often doesconverge (or approximately converge), and works well for many problems.Note also that if we are using a deterministic simulator/model of the MDP,then tted value iteration can be simplied by setting k= 1 in the algorithm.This is because the expectation in Equation (15.8) becomes an expectationover a deterministic distribution, and so a single example is sucient toexactly compute that expectation. Otherwise, in the algorithm above, wehad to draw ksamples, and average to try to approximate that expectation(see the denition of q(a), in the algorithm pseudo-code)."
        ],
        "formulas": [
            "1kPkj=1R(s(i))",
            "g==In",
            "achieve==using",
            "12Pni=1T(s(i))y(i)2gAbove,",
            "k= 1"
        ],
        "explanations": []
    },
    "page_202": {
        "content_preview": "201Finally, tted value iteration outputs V, which is an approximation toV. This implicitly denes our policy. Specically, when our system is insome state s, and we need to choose an action, we would like to choose theactionarg maxaEs0Psa[V(s0)] (15.9)The process for computing/approximating this is similar to the inner-loop oftted value iteration, where for each action, we sample s01;:::;s0kPsatoapproximate the expectation. (And again, if the simulator is deterministic,we can setk= 1.)In practice,...",
        "python_code": [
            "201Finally, tted value iteration outputs V, which is an approximation toV. This implicitly denes our policy. Specically, when our system is insome state s, and we need to choose an action, we would like to choose theactionarg maxaEs0Psa[V(s0)] (15.9)The process for computing/approximating this is similar to the inner-loop oftted value iteration, where for each action, we sample s01;:::;s0kPsatoapproximate the expectation. (And again, if the simulator is deterministic,we can setk= 1.)In practice, there are often other ways to approximate this step as well.For example, one very common case is if the simulator is of the form st+1=f(st;at) +t, wherefis some deterministic function of the states (such asf(st;at) =Ast+Bat), andis zero-mean Gaussian noise. In this case, wecan pick the action given byarg maxaV(f(s;a)):In other words, here we are just setting t= 0 (i.e., ignoring the noise inthe simulator), and setting k= 1. Equivalent, this can be derived fromEquation (15.9) using the approximationEs0[V(s0)]V(Es0[s0]) (15.10)=V(f(s;a)); (15.11)where here the expectation is over the random s0Psa. So long as the noisetermstare small, this will usually be a reasonable approximation.However, for problems that don't lend themselves to such approximations,having to sample kjAjstates using the model, in order to approximate theexpectation above, can be computationally expensive.15.5 Connections between Policy and ValueIteration (Optional)In the policy iteration, line 3 of Algorithm 5, we typically use linear systemsolver to compute V. Alternatively, one can also the iterative Bellmanupdates, similarly to the value iteration, to evaluate V, as in the ProcedureVE() in Line 1 of Algorithm 6 below. Here if we take option 1 in Line 2 ofthe Procedure VE, then the dierence between the Procedure VE from the"
        ],
        "formulas": [
            "setk= 1.)In",
            "1=f(st;at)",
            "t= 0",
            "k= 1."
        ],
        "explanations": []
    },
    "page_203": {
        "content_preview": "202Algorithm 6 Variant of Policy Iteration1:procedure VE(,k) .To evaluate V2: Option 1: initialize V(s) := 0; Option 2: Initialize from the currentVin the main algorithm.3: fori= 0 tok1do4: For every state s, updateV(s) :=R(s) +Xs0Ps(s)(s0)V(s0): (15.12)returnV5:Require: hyperparameter k.6:Initializerandomly.7:foruntil convergence do8: LetV= VE(;k).9: For each state s, let(s) := arg maxa2AXs0Psa(s0)V(s0): (15.13)...",
        "python_code": [
            "202Algorithm 6 Variant of Policy Iteration1:procedure VE(,k) .To evaluate V2: Option 1: initialize V(s) := 0; Option 2: Initialize from the currentVin the main algorithm.3: fori= 0 tok1do4: For every state s, updateV(s) :=R(s) +Xs0Ps(s)(s0)V(s0): (15.12)returnV5:Require: hyperparameter k.6:Initializerandomly.7:foruntil convergence do8: LetV= VE(;k).9: For each state s, let(s) := arg maxa2AXs0Psa(s0)V(s0): (15.13)"
        ],
        "formulas": [
            "fori= 0",
            "LetV= VE(;k).9:"
        ],
        "explanations": []
    },
    "page_204": {
        "content_preview": "203value iteration (Algorithm 4) is that on line 4, the procedure is using theaction from instead of the greedy action.Using the Procedure VE, we can build Algorithm 6, which is a variantof policy iteration that serves an intermediate algorithm that connects pol-icy iteration and value iteration. Here we are going to use option 2 in VEto maximize the re-use of knowledge learned before. One can verify indeedthat if we take k= 1 and use option 2 in Line 2 in Algorithm 6, then Algo-rithm 6 is seman...",
        "python_code": [
            "203value iteration (Algorithm 4) is that on line 4, the procedure is using theaction from instead of the greedy action.Using the Procedure VE, we can build Algorithm 6, which is a variantof policy iteration that serves an intermediate algorithm that connects pol-icy iteration and value iteration. Here we are going to use option 2 in VEto maximize the re-use of knowledge learned before. One can verify indeedthat if we take k= 1 and use option 2 in Line 2 in Algorithm 6, then Algo-rithm 6 is semantically equivalent to value iteration (Algorithm 4). In otherwords, both Algorithm 6 and value iteration interleave the updates in (15.13)and (15.12). Algorithm 6 alternate between ksteps of update (15.12) andone step of (15.13), whereas value iteration alternates between 1 steps of up-date (15.12) and one step of (15.13). Therefore generally Algorithm 6 shouldnot be faster than value iteration, because assuming that update (15.12)and (15.13) are equally useful and time-consuming, then the optimal balanceof the update frequencies could be just k= 1 ork1.On the other hand, if ksteps of update (15.12) can be done much fasterthanktimes a single step of (15.12), then taking additional steps of equa-tion (15.12) in group might be useful. This is what policy iteration is lever-aging | the linear system solver can give us the result of Procedure VE withk=1much faster than using the Procedure VE for a large k. On the ipside, when such a speeding-up eect no longer exists, e.g.,, when the statespace is large and linear system solver is also not fast, then value iteration ismore preferable."
        ],
        "formulas": [
            "k= 1",
            "k= 1",
            "withk=1much"
        ],
        "explanations": []
    },
    "page_205": {
        "content_preview": "Chapter 16LQR, DDP and LQG16.1 Finite-horizon MDPsIn Chapter 15, we dened Markov Decision Processes (MDPs) and coveredValue Iteration / Policy Iteration in a simplied setting. More specically weintroduced the optimal Bellman equation that denes the optimal valuefunctionVof the optimal policy .V(s) =R(s) + maxa2AXs02SPsa(s0)V(s0)Recall that from the optimal value function, we were able to recover theoptimal policy with(s) = argmaxa2AXs02SPsa(s0)V(s0)In this chapter, we'll place ourselves in a mor...",
        "python_code": [
            "Chapter 16LQR, DDP and LQG16.1 Finite-horizon MDPsIn Chapter 15, we dened Markov Decision Processes (MDPs) and coveredValue Iteration / Policy Iteration in a simplied setting. More specically weintroduced the optimal Bellman equation that denes the optimal valuefunctionVof the optimal policy .V(s) =R(s) + maxa2AXs02SPsa(s0)V(s0)Recall that from the optimal value function, we were able to recover theoptimal policy with(s) = argmaxa2AXs02SPsa(s0)V(s0)In this chapter, we'll place ourselves in a more general setting:1. We want to write equations that make sense for both the discrete andthe continuous case. We'll therefore writeEs0PsaV(s0)instead ofXs02SPsa(s0)V(s0)meaning that we take the expectation of the value function at the nextstate. In the nite case, we can rewrite the expectation as a sum over204"
        ],
        "formulas": [],
        "explanations": []
    },
    "page_206": {
        "content_preview": "205states. In the continuous case, we can rewrite the expectation as anintegral. The notation s0Psameans that the state s0is sampled fromthe distribution Psa.2. We'll assume that the rewards depend on both states and actions . Inother words, R:SA! R. This implies that the previous mechanismfor computing the optimal action is changed into(s) = argmaxa2AR(s;a) +Es0PsaV(s0)3. Instead of considering an innite horizon MDP, we'll assume that wehave a nite horizon MDP that will be dened as a tuple(S;A;...",
        "python_code": [
            "205states. In the continuous case, we can rewrite the expectation as anintegral. The notation s0Psameans that the state s0is sampled fromthe distribution Psa.2. We'll assume that the rewards depend on both states and actions . Inother words, R:SA! R. This implies that the previous mechanismfor computing the optimal action is changed into(s) = argmaxa2AR(s;a) +Es0PsaV(s0)3. Instead of considering an innite horizon MDP, we'll assume that wehave a nite horizon MDP that will be dened as a tuple(S;A;Psa;T;R )withT > 0 the time horizon (for instance T= 100). In this setting,our denition of payo is going to be (slightly) dierent:R(s0;a0) +R(s1;a1) ++R(sT;aT)instead of (innite horizon case)R(s0;a0) +R(s1;a1) +2R(s2;a2) +:::1Xt=0R(st;at)tWhat happened to the discount factor ?Remember that the intro-duction ofwas (partly) justied by the necessity of making sure thatthe innite sum would be nite and well-dened. If the rewards arebounded by a constant R, the payo is indeed bounded byj1Xt=0R(st)tjR1Xt=0tand we recognize a geometric sum! Here, as the payo is a nite sum,the discount factor is not necessary anymore."
        ],
        "formulas": [
            "T= 100).",
            "1Xt=0R(st;at)tWhat",
            "byj1Xt=0R(st)tjR1Xt=0tand"
        ],
        "explanations": []
    },
    "page_207": {
        "content_preview": "206In this new setting, things behave quite dierently. First, the optimalpolicymight be non-stationary, meaning that it changes over time .In other words, now we have(t):S!Awhere the superscript ( t) denotes the policy at time step t. The dynam-ics of the nite horizon MDP following policy (t)proceeds as follows:we start in some state s0, take some action a0:=(0)(s0) according toour policy at time step 0. The MDP transitions to a successor s1, drawnaccording to Ps0a0. Then, we get to pick another...",
        "python_code": [
            "206In this new setting, things behave quite dierently. First, the optimalpolicymight be non-stationary, meaning that it changes over time .In other words, now we have(t):S!Awhere the superscript ( t) denotes the policy at time step t. The dynam-ics of the nite horizon MDP following policy (t)proceeds as follows:we start in some state s0, take some action a0:=(0)(s0) according toour policy at time step 0. The MDP transitions to a successor s1, drawnaccording to Ps0a0. Then, we get to pick another action a1:=(1)(s1)following our new policy at time step 1 and so on...Why does the optimal policy happen to be non-stationary in the nite-horizon setting? Intuitively, as we have a nite numbers of actions totake, we might want to adopt dierent strategies depending on wherewe are in the environment and how much time we have left. Imaginea grid with 2 goals with rewards +1 and +10. At the beginning, wemight want to take actions to aim for the +10 goal. But if after somesteps, dynamics somehow pushed us closer to the +1 goal and we don'thave enough steps left to be able to reach the +10 goal, then a betterstrategy would be to aim for the +1 goal...4. This observation allows us to use time dependent dynamicsst+1P(t)st;atmeaning that the transition's distribution P(t)st;atchanges over time. Thesame thing can be said about R(t). Note that this setting is a bettermodel for real life. In a car, the gas tank empties, trac changes,etc. Combining the previous remarks, we'll use the following generalformulation for our nite horizon MDPS;A;P(t)sa;T;R(t)Remark : notice that the above formulation would be equivalent toadding the time into the state."
        ],
        "formulas": [],
        "explanations": []
    },
    "page_208": {
        "content_preview": "207The value function at time tfor a policy is then dened in the sameway as before, as an expectation over trajectories generated followingpolicystarting in state s.Vt(s) =ER(t)(st;at) ++R(T)(sT;aT)jst=s;Now, the question isIn this nite-horizon setting, how do we nd the optimal value functionVt(s) = maxVt(s)It turns out that Bellman's equation for Value Iteration is made for Dy-namic Programming . This may come as no surprise as Bellman is one ofthe fathers of dynamic programming and the Bellman...",
        "python_code": [
            "207The value function at time tfor a policy is then dened in the sameway as before, as an expectation over trajectories generated followingpolicystarting in state s.Vt(s) =ER(t)(st;at) ++R(T)(sT;aT)jst=s;Now, the question isIn this nite-horizon setting, how do we nd the optimal value functionVt(s) = maxVt(s)It turns out that Bellman's equation for Value Iteration is made for Dy-namic Programming . This may come as no surprise as Bellman is one ofthe fathers of dynamic programming and the Bellman equation is stronglyrelated to the eld. To understand how we can simplify the problem byadopting an iteration-based approach, we make the following observations:1. Notice that at the end of the game (for time step T), the optimal valueis obvious8s2S:VT(s) := maxa2AR(T)(s;a) (16.1)2. For another time step 0 t < T , if we suppose that we know theoptimal value function for the next time step Vt+1, then we have8t<T;s2S:Vt(s) := maxa2AhR(t)(s;a) +Es0P(t)saVt+1(s0)i(16.2)With these observations in mind, we can come up with a clever algorithmto solve for the optimal value function:1. compute VTusing equation (16.1).2. fort=T1;:::; 0:computeVtusingVt+1using equation (16.2)"
        ],
        "formulas": [
            "jst=s;Now,",
            "fort=T1;:::;"
        ],
        "explanations": []
    },
    "page_209": {
        "content_preview": "208Side note We can interpret standard value iteration as a special caseof this general case, but without keeping track of time. It turns out thatin the standard setting, if we run value iteration for T steps, we get a Tapproximation of the optimal value iteration (geometric convergence). Seeproblem set 4 for a proof of the following result:Theorem LetBdenote the Bellman update and jjf(x)jj1:= supxjf(x)j.IfVtdenotes the value function at the t-th step, thenjjVt+1Vjj1=jjB(Vt)Vjj1jjVtVjj1tjjV1Vjj1...",
        "python_code": [
            "208Side note We can interpret standard value iteration as a special caseof this general case, but without keeping track of time. It turns out thatin the standard setting, if we run value iteration for T steps, we get a Tapproximation of the optimal value iteration (geometric convergence). Seeproblem set 4 for a proof of the following result:Theorem LetBdenote the Bellman update and jjf(x)jj1:= supxjf(x)j.IfVtdenotes the value function at the t-th step, thenjjVt+1Vjj1=jjB(Vt)Vjj1jjVtVjj1tjjV1Vjj1In other words, the Bellman operator Bis a-contracting operator.16.2 Linear Quadratic Regulation (LQR)In this section, we'll cover a special case of the nite-horizon setting describedin Section 16.1, for which the exact solution is (easily) tractable. Thismodel is widely used in robotics, and a common technique in many problemsis to reduce the formulation to this framework.First, let's describe the model's assumptions. We place ourselves in thecontinuous setting, withS=Rd;A=Rdand we'll assume linear transitions (with noise)st+1=Atst+Btat+wtwhereAt2Rdd;Bt2Rddare matrices and wtN (0;t) is somegaussian noise (with zero mean). As we'll show in the following paragraphs,it turns out that the noise, as long as it has zero mean, does not impact theoptimal policy!We'll also assume quadratic rewardsR(t)(st;at) =s>tUtsta>tWtat"
        ],
        "formulas": [
            "1Vjj1=jjB(Vt)Vjj1jjVtVjj1tjjV1Vjj1In",
            "withS=Rd;A=Rdand",
            "1=Atst+Btat+wtwhereAt2Rdd;Bt2Rddare"
        ],
        "explanations": []
    },
    "page_210": {
        "content_preview": "209whereUt2Rdn;Wt2Rddare positive denite matrices (meaning thatthe reward is always negative ).Remark Note that the quadratic formulation of the reward is equivalentto saying that we want our state to be close to the origin (where the rewardis higher). For example, if Ut=Id(the identity matrix) and Wt=Id, thenRt=jjstjj2jjatjj2, meaning that we want to take smooth actions (smallnorm ofat) to go back to the origin (small norm of st). This could model acar trying to stay in the middle of lane witho...",
        "python_code": [
            "209whereUt2Rdn;Wt2Rddare positive denite matrices (meaning thatthe reward is always negative ).Remark Note that the quadratic formulation of the reward is equivalentto saying that we want our state to be close to the origin (where the rewardis higher). For example, if Ut=Id(the identity matrix) and Wt=Id, thenRt=jjstjj2jjatjj2, meaning that we want to take smooth actions (smallnorm ofat) to go back to the origin (small norm of st). This could model acar trying to stay in the middle of lane without making impulsive moves...Now that we have dened the assumptions of our LQR model, let's coverthe 2 steps of the LQR algorithmstep 1 suppose that we don't know the matrices A;B; . To esti-mate them, we can follow the ideas outlined in the Value Ap-proximation section of the RL notes. First, collect transitionsfrom an arbitrary policy. Then, use linear regression to ndargminA;BPni=1PT1t=0s(i)t+1As(i)t+Ba(i)t2. Finally, use a tech-nique seen in Gaussian Discriminant Analysis to learn .step 2 assuming that the parameters of our model are known (given or esti-mated with step 1), we can derive the optimal policy using dynamicprogramming.In other words, given(st+1 =Atst+Btat+wtAt;Bt;Ut;Wt;tknownR(t)(st;at) =s>tUtsta>tWtatwe want to compute Vt. If we go back to section 16.1, we can applydynamic programming, which yields1.Initialization stepFor the last time step T,VT(sT) = maxaT2ART(sT;aT)= maxaT2As>TUTsTa>TWtaT=s>TUtsT (maximized for aT= 0)"
        ],
        "formulas": [
            "Ut=Id(the",
            "Wt=Id,",
            "thenRt=jjstjj2jjatjj2,",
            "BPni=1PT1t=0s(i)t+1As(i)t+Ba(i)t2.",
            "1 =Atst+Btat+wtAt;Bt;Ut;Wt;tknownR(t)(st;at)",
            "TWtaT=s>TUtsT",
            "aT= 0)"
        ],
        "explanations": []
    },
    "page_211": {
        "content_preview": "2102.Recurrence stepLett<T . Suppose we know Vt+1.Fact 1: It can be shown that if Vt+1is a quadratic function in st, thenVtis also a quadratic function. In other words, there exists some matrix and some scalar such thatifVt+1(st+1) =s>t+1t+1st+1+ t+1thenVt(st) =s>ttst+ tFor time step t=T, we had t=UTand T= 0.Fact 2: We can show that the optimal policy is just a linear function ofthe state.KnowingVt+1is equivalent to knowing t+1and t+1, so we just needto explain how we compute tand tfrom t+1and t...",
        "python_code": [
            "2102.Recurrence stepLett<T . Suppose we know Vt+1.Fact 1: It can be shown that if Vt+1is a quadratic function in st, thenVtis also a quadratic function. In other words, there exists some matrix and some scalar such thatifVt+1(st+1) =s>t+1t+1st+1+ t+1thenVt(st) =s>ttst+ tFor time step t=T, we had t=UTand T= 0.Fact 2: We can show that the optimal policy is just a linear function ofthe state.KnowingVt+1is equivalent to knowing t+1and t+1, so we just needto explain how we compute tand tfrom t+1and t+1and the otherparameters of the problem.Vt(st) =s>ttst+ t= maxathR(t)(st;at) +Est+1P(t)st;at[Vt+1(st+1)]i= maxats>tUtsta>tVtat+Est+1N(Atst+Btat;t)[s>t+1t+1st+1+ t+1]where the second line is just the denition of the optimal value functionand the third line is obtained by plugging in the dynamics of our modelalong with the quadratic assumption. Notice that the last expression isa quadratic function in atand can thus be (easily) optimized1. We getthe optimal action atat=(B>tt+1BtVt)1Btt+1Atst=LtstwhereLt:=(B>tt+1BtWt)1Btt+1At1Use the identity Ew>tt+1wt= Tr(tt+1) withwtN (0;t)"
        ],
        "formulas": [
            "t=T,",
            "t=UTand",
            "T= 0.Fact",
            "t= maxathR(t)(st;at)",
            "i= maxats>tUtsta>tVtat+Est+1N(Atst+Btat;t)[s>t+1t+1st+1+",
            "atat=(B>tt+1BtVt)1Btt+1Atst=LtstwhereLt:=(B>tt+1BtWt)1Btt+1At1Use",
            "1wt= Tr(tt+1)"
        ],
        "explanations": []
    },
    "page_212": {
        "content_preview": "211which is an impressive result: our optimal policy is linear inst. Givenatwe can solve for tand t. We nally get the Discrete Ricattiequationst=A>tt+1t+1BtB>tt+1BtWt1Btt+1AtUtt=tr (tt+1) + t+1Fact 3: we notice that tdepends on neither nor the noise t! AsLtis a function of At;Btand t+1, it implies that the optimal policy alsodoes not depend on the noise ! (But tdoes depend on t, whichimplies that Vtdepends on t.)Then, to summarize, the LQR algorithm works as follows1. (if necessary) estimate par...",
        "python_code": [
            "211which is an impressive result: our optimal policy is linear inst. Givenatwe can solve for tand t. We nally get the Discrete Ricattiequationst=A>tt+1t+1BtB>tt+1BtWt1Btt+1AtUtt=tr (tt+1) + t+1Fact 3: we notice that tdepends on neither nor the noise t! AsLtis a function of At;Btand t+1, it implies that the optimal policy alsodoes not depend on the noise ! (But tdoes depend on t, whichimplies that Vtdepends on t.)Then, to summarize, the LQR algorithm works as follows1. (if necessary) estimate parameters At;Bt;t2. initialize T:=UTand T:= 0.3. iterate from t=T1:::0 to update tand tusing t+1and t+1using the discrete Ricatti equations. If there exists a policy that drivesthe state towards zero, then convergence is guaranteed!Using Fact 3 , we can be even more clever and make our algorithm run(slightly) faster! As the optimal policy does not depend on t, and theupdate of tonly depends on t, it is sucient to update only t!16.3 From non-linear dynamics to LQRIt turns out that a lot of problems can be reduced to LQR, even if dynamicsare non-linear. While LQR is a nice formulation because we are able to comeup with a nice exact solution, it is far from being general. Let's take forinstance the case of the inverted pendulum. The transitions between stateslook like0BB@xt+1_xt+1t+1_t+11CCA=F0BB@0BB@xt_xtt_t1CCA;at1CCAwhere the function Fdepends on the cos of the angle etc. Now, thequestion we may ask isCan we linearize this system?"
        ],
        "formulas": [
            "Ricattiequationst=A>tt+1t+1BtB>tt+1BtWt1Btt+1AtUtt=tr",
            "t=T1:::0",
            "11CCA=F0BB@0BB@xt_xtt_t1CCA;at1CCAwhere"
        ],
        "explanations": []
    },
    "page_213": {
        "content_preview": "21216.3.1 Linearization of dynamicsLet's suppose that at time t, the system spends most of its time in some statestand the actions we perform are around at. For the inverted pendulum, ifwe reached some kind of optimal, this is true: our actions are small and wedon't deviate much from the vertical.We are going to use Taylor expansion to linearize the dynamics. In thesimple case where the state is one-dimensional and the transition function Fdoes not depend on the action, we would write something ...",
        "python_code": [
            "21216.3.1 Linearization of dynamicsLet's suppose that at time t, the system spends most of its time in some statestand the actions we perform are around at. For the inverted pendulum, ifwe reached some kind of optimal, this is true: our actions are small and wedon't deviate much from the vertical.We are going to use Taylor expansion to linearize the dynamics. In thesimple case where the state is one-dimensional and the transition function Fdoes not depend on the action, we would write something likest+1=F(st)F(st) +F0(st)(stst)In the more general setting, the formula looks the same, with gradientsinstead of simple derivativesst+1F(st;at) +rsF(st;at)(stst) +raF(st;at)(atat) (16.3)and now,st+1is linear instandat, because we can rewrite equation (16.3)asst+1Ast+Bst+whereis some constant and A;B are matrices. Now, this writing looksawfully similar to the assumptions made for LQR. We just have to get ridof the constant term ! It turns out that the constant term can be absorbedintostby articially increasing the dimension by one. This is the same trickthat we used at the beginning of the class for linear regression...16.3.2 Dierential Dynamic Programming (DDP)The previous method works well for cases where the goal is to stay aroundsome state s(think about the inverted pendulum, or a car having to stayin the middle of a lane). However, in some cases, the goal can be morecomplicated.We'll cover a method that applies when our system has to follow sometrajectory (think about a rocket). This method is going to discretize thetrajectory into discrete time steps, and create intermediary goals aroundwhich we will be able to use the previous technique! This method is calledDierential Dynamic Programming . The main steps are"
        ],
        "formulas": [
            "1=F(st)F(st)"
        ],
        "explanations": []
    },
    "page_214": {
        "content_preview": "213step 1 come up with a nominal trajectory using a naive controller, that approx-imate the trajectory we want to follow. In other words, our controlleris able to approximate the gold trajectory withs0;a0!s1;a1!:::step 2 linearize the dynamics around each trajectory point st, in other wordsst+1F(st;at) +rsF(st;at)(stst) +raF(st;at)(atat)wherest;atwould be our current state and action. Now that we havea linear approximation around each of these points, we can use theprevious section and rewritest...",
        "python_code": [
            "213step 1 come up with a nominal trajectory using a naive controller, that approx-imate the trajectory we want to follow. In other words, our controlleris able to approximate the gold trajectory withs0;a0!s1;a1!:::step 2 linearize the dynamics around each trajectory point st, in other wordsst+1F(st;at) +rsF(st;at)(stst) +raF(st;at)(atat)wherest;atwould be our current state and action. Now that we havea linear approximation around each of these points, we can use theprevious section and rewritest+1=Atst+Btat(notice that in that case, we use the non-stationary dynamics settingthat we mentioned at the beginning of these lecture notes)Note We can apply a similar derivation for the reward R(t), with asecond-order Taylor expansion.R(st;at)R(st;at) +rsR(st;at)(stst) +raR(st;at)(atat)+12(stst)>Hss(stst) + (stst)>Hsa(atat)+12(atat)>Haa(atat)whereHxyrefers to the entry of the Hessian of Rwith respect to xandyevaluated in ( st;at) (omitted for readability). This expression can bere-written asRt(st;at) =s>tUtsta>tWtatfor some matrices Ut;Wt, with the same trick of adding an extra dimen-sion of ones. To convince yourself, notice that1xa bb c1x=a+ 2bx+cx2"
        ],
        "formulas": [
            "1=Atst+Btat(notice",
            "c1x=a+"
        ],
        "explanations": []
    },
    "page_215": {
        "content_preview": "214step 3 Now, you can convince yourself that our problem is strictly re-writtenin the LQR framework. Let's just use LQR to nd the optimal policyt. As a result, our new controller will (hopefully) be better!Note: Some problems might arise if the LQR trajectory deviates toomuch from the linearized approximation of the trajectory, but that canbe xed with reward-shaping...step 4 Now that we get a new controller (our new policy t), we use it toproduce a new trajectorys0;0(s0)!s1;1(s1)!:::!sTnote tha...",
        "python_code": [
            "214step 3 Now, you can convince yourself that our problem is strictly re-writtenin the LQR framework. Let's just use LQR to nd the optimal policyt. As a result, our new controller will (hopefully) be better!Note: Some problems might arise if the LQR trajectory deviates toomuch from the linearized approximation of the trajectory, but that canbe xed with reward-shaping...step 4 Now that we get a new controller (our new policy t), we use it toproduce a new trajectorys0;0(s0)!s1;1(s1)!:::!sTnote that when we generate this new trajectory, we use the real Fandnot its linear approximation to compute transitions, meaning thatst+1=F(st;at)then, go back to step 2 and repeat until some stopping criterion.16.4 Linear Quadratic Gaussian (LQG)Often, in the real word, we don't get to observe the full state st. For example,an autonomous car could receive an image from a camera, which is merelyanobservation , and not the full state of the world. So far, we assumedthat the state was available. As this might not hold true for most of thereal-world problems, we need a new tool to model this situation: PartiallyObservable MDPs .A POMDP is an MDP with an extra observation layer. In other words,we introduce a new variable ot, that follows some conditional distributiongiven the current state stotjstO(ojs)Formally, a nite-horizon POMDP is given by a tuple(S;O;A;Psa;T;R )Within this framework, the general strategy is to maintain a belief state(distribution over states) based on the observation o1;:::;ot. Then, a policyin a POMDP maps this belief states to actions."
        ],
        "formulas": [
            "1=F(st;at)then,"
        ],
        "explanations": []
    },
    "page_216": {
        "content_preview": "215In this section, we'll present a extension of LQR to this new setting.Assume that we observe yt2Rnwithm<n such that(yt =Cst+vtst+1=Ast+Bat+wtwhereC2Rndis a compression matrix and vtis the sensor noise (alsogaussian, like wt). Note that the reward function R(t)is left unchanged, as afunction of the state (not the observation) and action. Also, as distributionsare gaussian, the belief state is also going to be gaussian. In this new frame-work, let's give an overview of the strategy we are going...",
        "python_code": [
            "215In this section, we'll present a extension of LQR to this new setting.Assume that we observe yt2Rnwithm<n such that(yt =Cst+vtst+1=Ast+Bat+wtwhereC2Rndis a compression matrix and vtis the sensor noise (alsogaussian, like wt). Note that the reward function R(t)is left unchanged, as afunction of the state (not the observation) and action. Also, as distributionsare gaussian, the belief state is also going to be gaussian. In this new frame-work, let's give an overview of the strategy we are going to adopt to nd theoptimal policy:step 1 rst, compute the distribution on the possible states (the belief state),based on the observations we have. In other words, we want to computethe meanstjtand the covariance tjtofstjy1;:::;ytNstjt;tjtto perform the computation eciently over time, we'll use the KalmanFilter algorithm (used on-board Apollo Lunar Module!).step 2 now that we have the distribution, we'll use the mean stjtas the bestapproximation for ststep 3 then set the action at:=LtstjtwhereLtcomes from the regular LQRalgorithm.Intuitively, to understand why this works, notice that stjtis a noisy ap-proximation of st(equivalent to adding more noise to LQR) but we provedthat LQR is independent of the noise!Step 1 needs to be explicated. We'll cover a simple case where there isno action dependence in our dynamics (but the general case follows the sameidea). Suppose that(st+1=Ast+wt; wtN(0;s)yt =Cst+vt; vtN(0;y)As noises are Gaussians, we can easily prove that the joint distribution isalso Gaussian"
        ],
        "formulas": [
            "yt =Cst+vtst+1=Ast+Bat+wtwhereC2Rndis",
            "1=Ast+wt;",
            "yt =Cst+vt;"
        ],
        "explanations": []
    },
    "page_217": {
        "content_preview": "2160BBBBBBB@s1...sty1...yt1CCCCCCCAN (;) for some ;then, using the marginal formulas of gaussians (see Factor Analysis notes),we would getstjy1;:::;ytNstjt;tjtHowever, computing the marginal distribution parameters using theseformulas would be computationally expensive! It would require manipulatingmatrices of shape tt. Recall that inverting a matrix can be done in O(t3),and it would then have to be repeated over the time steps, yielding a cost inO(t4)!TheKalman lter algorithm provides a much be...",
        "python_code": [],
        "formulas": [],
        "explanations": [
            "yt1CCCCCCCAN (;) for some ;then, using the marginal formulas of gaussians (see Factor Analysis notes),we would getstjy1;:::;ytNstjt;tjtHowever, computing the marginal distribution parameters using theseformulas would be computationally expensive! It would require manipulatingmatrices of shape tt",
            "Recall that inverting a matrix can be done in O(t3),and it would then have to be repeated over the time steps, yielding a cost inO(t4)!TheKalman lter algorithm provides a much better way of computingthe mean and variance, by updating them over time in constant time int! The kalman lter is based on two basics steps",
            "Assume that we know thedistribution of stjy1;:::;yt:predict step computest+1jy1;:::;ytupdate step computest+1jy1;:::;yt+1and iterate over time steps! The combination of the predict and updatesteps updates our belief states",
            "In other words, the process looks like(stjy1;:::;yt)predict! (st+1jy1;:::;yt)update! (st+1jy1;:::;yt+1)predict!:::predict step Suppose that we know the distribution ofstjy1;:::;ytNstjt;tjtthen, the distribution over the next state is also a gaussian distributionst+1jy1;:::;ytNst+1jt;t+1jtwhere"
        ]
    },
    "page_218": {
        "content_preview": "217(st+1jt=Astjtt+1jt=AtjtA>+ supdate step givenst+1jtand t+1jtsuch thatst+1jy1;:::;ytNst+1jt;t+1jtwe can prove thatst+1jy1;:::;yt+1Nst+1jt+1;t+1jt+1where(st+1jt+1 =st+1jt+Kt(yt+1Cst+1jt)t+1jt+1= t+1jtKtCt+1jtwithKt:= t+1jtC>(Ct+1jtC>+ y)1The matrix Ktis called the Kalman gain .Now, if we have a closer look at the formulas, we notice that we don'tneed the observations prior to time step t! The update steps only dependson the previous distribution. Putting it all together, the algorithm rst runsa...",
        "python_code": [
            "217(st+1jt=Astjtt+1jt=AtjtA>+ supdate step givenst+1jtand t+1jtsuch thatst+1jy1;:::;ytNst+1jt;t+1jtwe can prove thatst+1jy1;:::;yt+1Nst+1jt+1;t+1jt+1where(st+1jt+1 =st+1jt+Kt(yt+1Cst+1jt)t+1jt+1= t+1jtKtCt+1jtwithKt:= t+1jtC>(Ct+1jtC>+ y)1The matrix Ktis called the Kalman gain .Now, if we have a closer look at the formulas, we notice that we don'tneed the observations prior to time step t! The update steps only dependson the previous distribution. Putting it all together, the algorithm rst runsa forward pass to compute the Kt, tjtandstjt(sometimes referred to as^sin the literature). Then, it runs a backward pass (the LQR updates) tocompute the quantities t;tandLt. Finally, we recover the optimal policywithat=Ltstjt."
        ],
        "formulas": [
            "1jt=Astjtt+1jt=AtjtA>+",
            "1 =st+1jt+Kt(yt+1Cst+1jt)t+1jt+1=",
            "policywithat=Ltstjt."
        ],
        "explanations": []
    },
    "page_219": {
        "content_preview": "Chapter 17Policy Gradient(REINFORCE)We will present a model-free algorithm called REINFORCE that does notrequire the notion of value functions and Qfunctions. It turns out to be moreconvenient to introduce REINFORCE in the nite horizon case, which willbe assumed throughout this note: we use = (s0;a0;:::;sT1;aT1;sT) todenote a trajectory, where T <1is the length of the trajectory. Moreover,REINFORCE only applies to learning a randomized policy . We use(ajs)to denote the probability of the policy ...",
        "python_code": [
            "Chapter 17Policy Gradient(REINFORCE)We will present a model-free algorithm called REINFORCE that does notrequire the notion of value functions and Qfunctions. It turns out to be moreconvenient to introduce REINFORCE in the nite horizon case, which willbe assumed throughout this note: we use = (s0;a0;:::;sT1;aT1;sT) todenote a trajectory, where T <1is the length of the trajectory. Moreover,REINFORCE only applies to learning a randomized policy . We use(ajs)to denote the probability of the policy outputting the action aat states.The other notations will be the same as in previous lecture notes.The advantage of applying REINFORCE is that we only need to assumethat we can sample from the transition probabilities fPsagand can query thereward function R(s;a) at statesand actiona,1but we do not need to knowthe analytical form of the transition probabilities or the reward function.We do not explicitly learn the transition probabilities or the reward functioneither.Lets0be sampled from some distribution . We consider optimizing theexpected total payo of the policy over the parameter dened as.(),E\"T1Xt=0tR(st;at)#(17.1)Recall that stPst1at1andat(jst). Also note that () =Es0P[V(s0)] if we ignore the dierence between nite and innite hori-zon.1In this notes we will work with the general setting where the reward depends on boththe state and the action.218"
        ],
        "formulas": [
            "use = (s0;a0;:::;sT1;aT1;sT)",
            "T1Xt=0tR(st;at)#(17.1)Recall"
        ],
        "explanations": []
    },
    "page_220": {
        "content_preview": "219We aim to use gradient ascent to maximize (). The main challengewe face here is to compute (or estimate) the gradient of () without theknowledge of the form of the reward function and the transition probabilities.LetP() denote the distribution of (generated by the policy ), andletf() =PT1t=0tR(st;at). We can rewrite () as() = EP[f()] (17.2)We face a similar situations in the variational auto-encoder (VAE) settingcovered in the previous lectures, where the we need to take the gradient w.r.tto ...",
        "python_code": [
            "219We aim to use gradient ascent to maximize (). The main challengewe face here is to compute (or estimate) the gradient of () without theknowledge of the form of the reward function and the transition probabilities.LetP() denote the distribution of (generated by the policy ), andletf() =PT1t=0tR(st;at). We can rewrite () as() = EP[f()] (17.2)We face a similar situations in the variational auto-encoder (VAE) settingcovered in the previous lectures, where the we need to take the gradient w.r.tto a variable that shows up under the expectation | the distribution Pdepends on . Recall that in VAE, we used the re-parametrization techniquesto address this problem. However it does not apply here because we doknow not how to compute the gradient of the function f. (We only havean ecient way to evaluate the function fby taking a weighted sum of theobserved rewards, but we do not necessarily know the reward function itselfto compute the gradient.)The REINFORCE algorithm uses an another approach to estimate thegradient of (). We start with the following derivation:rEP[f()] =rZP()f()d=Zr(P()f())d (swap integration with gradient)=Z(rP())f()d (becauefdoes not depend on )=ZP()(rlogP())f()d(becauserlogP() =rP()P())= EP[(rlogP())f()] (17.3)Now we have a sample-based estimator for rEP[f()]. Let(1);:::;(n)benempirical samples from P(which are obtained by running the policyforntimes, with Tsteps for each run). We can estimate the gradient of() byrEP[f()] = EP[(rlogP())f()] (17.4)1nnXi=1(rlogP((i)))f((i)) (17.5)"
        ],
        "formulas": [
            "PT1t=0tR(st;at).",
            "d=Zr(P()f())d",
            "1nnXi=1(rlogP((i)))f((i))"
        ],
        "explanations": []
    },
    "page_221": {
        "content_preview": "220The next question is how to compute log P(). We derive an analyt-ical formula for log P() and compute its gradient w.r.t (using auto-dierentiation). Using the denition of , we haveP() =(s0)(a0js0)Ps0a0(s1)(a1js1)Ps1a1(s2)PsT1aT1(sT) (17.6)Here recall that to used to denote the density of the distribution of s0. Itfollows thatlogP() = log(s0) + log(a0js0) + logPs0a0(s1) + log(a1js1)+ logPs1a1(s2) ++ logPsT1aT1(sT) (17.7)Taking gradient w.r.t to , we obtainrlogP() =rlog(a0js0) +rlog(a1js1) ++rl...",
        "python_code": [
            "220The next question is how to compute log P(). We derive an analyt-ical formula for log P() and compute its gradient w.r.t (using auto-dierentiation). Using the denition of , we haveP() =(s0)(a0js0)Ps0a0(s1)(a1js1)Ps1a1(s2)PsT1aT1(sT) (17.6)Here recall that to used to denote the density of the distribution of s0. Itfollows thatlogP() = log(s0) + log(a0js0) + logPs0a0(s1) + log(a1js1)+ logPs1a1(s2) ++ logPsT1aT1(sT) (17.7)Taking gradient w.r.t to , we obtainrlogP() =rlog(a0js0) +rlog(a1js1) ++rlog(aT1jsT1)Note that many of the terms disappear because they don't depend on andthus have zero gradients. (This is somewhat important | we don't know howto evaluate those terms such as log Ps0a0(s1) because we don't have access tothe transition probabilities, but luckily those terms have zero gradients!)Plugging the equation above into equation (17.4), we conclude thatr() =rEP[f()] = EP\" T1Xt=0rlog(atjst)!f()#= EP\" T1Xt=0rlog(atjst)! T1Xt=0tR(st;at)!#(17.8)We estimate the RHS of the equation above by empirical sample trajectories,and the estimate is unbiased. The vanilla REINFORCE algorithm iterativelyupdates the parameter by gradient ascent using the estimated gradients.Interpretation of the policy gradient formula (17.8) .The quantityrP() =PT1t=0rlog(atjst) is intuitively the direction of the changeofthat will make the trajectory more likely to occur (or increase theprobability of choosing action a0;:::;at1), andf() is the total payo ofthis trajectory. Thus, by taking a gradient step, intuitively we are trying toimprove the likelihood of all the trajectories, but with a dierent emphasisor weight for each (or for each set of actions a0;a1;:::;at1). Ifis veryrewarding (that is, f() is large), we try very hard to move in the direction"
        ],
        "formulas": [
            "T1Xt=0rlog(atjst)!f()#=",
            "T1Xt=0rlog(atjst)!",
            "T1Xt=0tR(st;at)!#(17.8)We",
            "PT1t=0rlog(atjst)"
        ],
        "explanations": []
    },
    "page_222": {
        "content_preview": "221that can increase the probability of the trajectory (or the direction thatincreases the probability of choosing a0;:::;at1), and ifhas low payo,we try less hard with a smaller weight.An interesting fact that follows from formula (17.3) is thatEP\"T1Xt=0rlog(atjst)#= 0 (17.9)To see this, we take f() = 1 (that is, the reward is always a constant),then the LHS of (17.8) is zero because the payo is always a xed constantPTt=0t. Thus the RHS of (17.8) is also zero, which implies (17.9).In fact, one ...",
        "python_code": [
            "221that can increase the probability of the trajectory (or the direction thatincreases the probability of choosing a0;:::;at1), and ifhas low payo,we try less hard with a smaller weight.An interesting fact that follows from formula (17.3) is thatEP\"T1Xt=0rlog(atjst)#= 0 (17.9)To see this, we take f() = 1 (that is, the reward is always a constant),then the LHS of (17.8) is zero because the payo is always a xed constantPTt=0t. Thus the RHS of (17.8) is also zero, which implies (17.9).In fact, one can verify that E at(jst)rlog(atjst) = 0 for any xed tandst.2This fact has two consequences. First, we can simplify formula (17.8)tor() =T1Xt=0EP\"rlog(atjst) T1Xj=0jR(sj;aj)!#=T1Xt=0EP\"rlog(atjst) T1XjtjR(sj;aj)!#(17.10)where the second equality follows fromEP\"rlog(atjst) X0j<tjR(sj;aj)!#= E\"E [rlog(atjst)js0;a0;:::;st1;at1;st] X0j<tjR(sj;aj)!#= 0 (because E [ rlog(atjst)js0;a0;:::;st1;at1;st] = 0)Note that here we used the law of total expectation. The outer expecta-tion in the second line above is over the randomness of s0;a0;:::;at1;st,whereas the inner expectation is over the randomness of at(conditioned ons0;a0;:::;at1;st.) We see that we've made the estimator slightly simpler.The second consequence of E at(jst)rlog(atjst) = 0 is the following: forany valueB(st) that only depends on st, it holds thatEP[rlog(atjst)B(st)]= E [E [rlog(atjst)js0;a0;:::;st1;at1;st]B(st)]= 0 (because E [ rlog(atjst)js0;a0;:::;st1;at1;st] = 0)2In general, it's true that E xp[rlogp(x)] = 0."
        ],
        "formulas": [
            "T1Xt=0rlog(atjst)#=",
            "constantPTt=0t.",
            "T1Xt=0EP\"rlog(atjst)",
            "T1Xj=0jR(sj;aj)!#=T1Xt=0EP\"rlog(atjst)"
        ],
        "explanations": []
    },
    "page_223": {
        "content_preview": "222Again here we used the law of total expectation. The outer expecta-tion in the second line above is over the randomness of s0;a0;:::;at1;st,whereas the inner expectation is over the randomness of at(conditioned ons0;a0;:::;at1;st.) It follows from equation (17.10) and the equation abovethatr() =T1Xt=0EP\"rlog(atjst) T1XjtjR(sj;aj)tB(st)!#=T1Xt=0EP\"rlog(atjst)t T1XjtjtR(sj;aj)B(st)!#(17.11)Therefore, we will get a dierent estimator for estimating the r() with adierence choice of B(). The benet ...",
        "python_code": [
            "222Again here we used the law of total expectation. The outer expecta-tion in the second line above is over the randomness of s0;a0;:::;at1;st,whereas the inner expectation is over the randomness of at(conditioned ons0;a0;:::;at1;st.) It follows from equation (17.10) and the equation abovethatr() =T1Xt=0EP\"rlog(atjst) T1XjtjR(sj;aj)tB(st)!#=T1Xt=0EP\"rlog(atjst)t T1XjtjtR(sj;aj)B(st)!#(17.11)Therefore, we will get a dierent estimator for estimating the r() with adierence choice of B(). The benet of introducing a proper B() | whichis often referred to as a baseline | is that it helps reduce the variance of theestimator.3It turns out that a near optimal estimator would be the expectedfuture payo EhPT1jtjtR(sj;aj)jsti, which is pretty much the same as thevalue function V(st) (if we ignore the dierence between nite and innitehorizon.) Here one could estimate the value function V() in a crude way,because its precise value doesn't inuence the mean of the estimator but onlythe variance. This leads to a policy gradient algorithm with baselines statedin Algorithm 7.43As a heuristic but illustrating example, suppose for a xed t, the future rewardPT1jtjtR(sj;aj) randomly takes two values 1000 + 1 and 1000 2 with equal proba-bility, and the corresponding values for rlog(atjst) are vector zandz. (Note thatbecause E [rlog(atjst)] = 0, ifrlog(atjst) can only take two values uniformly,then the two values have to two vectors in an opposite direction.) In this case, withoutsubtracting the baseline, the estimators take two values (1000 + 1) zand(10002)z,whereas after subtracting a baseline of 1000, the estimator has two values zand 2z. Thelatter estimator has much lower variance compared to the original estimator.4We note that the estimator of the gradient in the algorithm does not exactly matchthe equation 17.11. If we multiply tin the summand of equation (17.13), then they willexactly match. Removing such discount factors empirically works well because it gives alarge update."
        ],
        "formulas": [
            "T1Xt=0EP\"rlog(atjst)",
            "T1Xt=0EP\"rlog(atjst)t"
        ],
        "explanations": []
    },
    "page_224": {
        "content_preview": "223Algorithm 7 Vanilla policy gradient with baselinefori= 1;doCollect a set of trajectories by executing the current policy. Use Rtas a shorthand forPT1jtjtR(sj;aj)Fit the baseline by nding a function Bthat minimizesXXt(RtB(st))2(17.12)Update the policy parameter with the gradient estimatorXXtrlog(atjst)(RtB(st)) (17.13)...",
        "python_code": [
            "223Algorithm 7 Vanilla policy gradient with baselinefori= 1;doCollect a set of trajectories by executing the current policy. Use Rtas a shorthand forPT1jtjtR(sj;aj)Fit the baseline by nding a function Bthat minimizesXXt(RtB(st))2(17.12)Update the policy parameter with the gradient estimatorXXtrlog(atjst)(RtB(st)) (17.13)"
        ],
        "formulas": [
            "baselinefori= 1;doCollect"
        ],
        "explanations": []
    },
    "page_225": {
        "content_preview": "BibliographyMikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconcilingmodern machine-learning practice and the classical bias{variance trade-o.Proceedings of the National Academy of Sciences , 116(32):15849{15854,2019.Mikhail Belkin, Daniel Hsu, and Ji Xu. Two models of double descent forweak features. SIAM Journal on Mathematics of Data Science , 2(4):1167{1180, 2020.David M Blei, Alp Kucukelbir, and Jon D McAulie. Variational inference:A review for statisticians. Journal of the Amer...",
        "python_code": [],
        "formulas": [],
        "explanations": [
            "BibliographyMikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal",
            "Reconcilingmodern machine-learning practice and the classical bias{variance trade-o",
            "Proceedings of the National Academy of Sciences , 116(32):15849{15854,2019",
            "Mikhail Belkin, Daniel Hsu, and Ji Xu",
            "Two models of double descent forweak features",
            "SIAM Journal on Mathematics of Data Science , 2(4):1167{1180, 2020",
            "David M Blei, Alp Kucukelbir, and Jon D McAulie",
            "Variational inference:A review for statisticians",
            "Journal of the American Statistical Association ,112(518):859{877, 2017",
            "Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, SimranArora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, AntoineBosselut, Emma Brunskill, et al",
            "On the opportunities and risks of foun-dation models",
            "arXiv preprint arXiv:2108",
            "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Ka-plan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sas-try, Amanda Askell, et al",
            "Language models are few-shot learners",
            "Advancesin neural information processing systems , 33:1877{1901, 2020",
            "Ting Chen, Simon Kornblith, Mohammad Norouzi, and Georey Hinton",
            "A simple framework for contrastive learning of visual representations",
            "InInternational Conference on Machine Learning , pages 1597{1607",
            "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova",
            "Bert:Pre-training of deep bidirectional transformers for language understand-ing",
            "In Proceedings of the 2019 Conference of the North American Chapterof the Association for Computational Linguistics: Human Language Tech-nologies, Volume 1 (Long and Short Papers) , pages 4171{4186, 2019"
        ]
    },
    "page_226": {
        "content_preview": "225Je Z HaoChen, Colin Wei, Jason D Lee, and Tengyu Ma. Shape matters:Understanding the implicit bias of the noise covariance. arXiv preprintarXiv:2006.08680 , 2020.Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani.Surprises in high-dimensional ridgeless least squares interpolation. 2019.Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani.Surprises in high-dimensional ridgeless least squares interpolation. TheAnnals of Statistics , 50(2):949{986, 2022.Kaim...",
        "python_code": [],
        "formulas": [],
        "explanations": [
            "225Je Z HaoChen, Colin Wei, Jason D Lee, and Tengyu Ma",
            "Shape matters:Understanding the implicit bias of the noise covariance",
            "arXiv preprintarXiv:2006",
            "Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani",
            "Surprises in high-dimensional ridgeless least squares interpolation",
            "Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani",
            "Surprises in high-dimensional ridgeless least squares interpolation",
            "TheAnnals of Statistics , 50(2):949{986, 2022",
            "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",
            "Deep residuallearning for image recognition",
            "In Proceedings of the IEEE conference oncomputer vision and pattern recognition , pages 770{778, 2016",
            "Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani",
            "Anintroduction to statistical learning, second edition , volume 112",
            "Diederik P Kingma and Jimmy Ba",
            "Adam: A method for stochastic opti-mization",
            "arXiv preprint arXiv:1412",
            "Diederik P Kingma and Max Welling",
            "Auto-encoding variational bayes",
            "arXivpreprint arXiv:1312",
            "Yuping Luo, Huazhe Xu, Yuanzhi Li, Yuandong Tian, Trevor Darrell, andTengyu Ma",
            "Algorithmic framework for model-based deep reinforcementlearning with theoretical guarantees",
            "In International Conference on Learn-ing Representations , 2018",
            "Song Mei and Andrea Montanari",
            "The generalization error of random featuresregression: Precise asymptotics and the double descent curve",
            "Communi-cations on Pure and Applied Mathematics , 75(4):667{766, 2022",
            "More data can hurt for linear regression: Sample-wisedouble descent",
            "Preetum Nakkiran, Prayaag Venkat, Sham Kakade, and Tengyu Ma",
            "Optimalregularization can mitigate double descent",
            "Statistical mechanics of learning: Generalization",
            "The hand-book of brain theory and neural networks , pages 922{925, 1995",
            "Learning to generalize",
            "Frontiers of Life , 3(part 2):763{775,2001"
        ]
    },
    "page_227": {
        "content_preview": "226Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all youneed. arXiv preprint arXiv:1706.03762 , 2017.Blake Woodworth, Suriya Gunasekar, Jason D Lee, Edward Moroshko, Pe-dro Savarese, Itay Golan, Daniel Soudry, and Nathan Srebro. Kernel andrich regimes in overparametrized models. arXiv preprint arXiv:2002.09277 ,2020....",
        "python_code": [],
        "formulas": [],
        "explanations": [
            "226Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin",
            "Attention is all youneed",
            "arXiv preprint arXiv:1706",
            "Blake Woodworth, Suriya Gunasekar, Jason D Lee, Edward Moroshko, Pe-dro Savarese, Itay Golan, Daniel Soudry, and Nathan Srebro",
            "Kernel andrich regimes in overparametrized models",
            "arXiv preprint arXiv:2002"
        ]
    }
}