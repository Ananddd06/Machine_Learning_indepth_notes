# ðŸ“š Foundational Papers for LLMs and NLP

This document outlines the most important foundational papers to master NLP and LLMs â€” organized by themes and including direct links.

---

## ðŸ”¹ 1. Word Embeddings

- **Word2Vec** (2013) â€“ [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781) â€” Tomas Mikolov et al.
- **GloVe** (2014) â€“ [GloVe: Global Vectors for Word Representation](https://aclanthology.org/D14-1162/) â€” Stanford NLP
- **FastText** (2016) â€“ [Enriching Word Vectors with Subword Information](https://arxiv.org/abs/1607.04606) â€” Facebook AI

---

## ðŸ”¹ 2. Contextual Embeddings

- **ELMo** (2018) â€“ [Deep Contextualized Word Representations](https://arxiv.org/abs/1802.05365)
- **ULMFiT** (2018) â€“ [Universal Language Model Fine-tuning for Text Classification](https://arxiv.org/abs/1801.06146)

---

## ðŸ”¹ 3. Sequence Models

- **LSTM** (1997) â€“ [Long Short-Term Memory](https://www.bioinf.jku.at/publications/older/2604.pdf) â€” Hochreiter & Schmidhuber
- **Seq2Seq** (2014) â€“ [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215) â€” Google Brain
- **Attention** (2015) â€“ [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)

---

## ðŸ”¹ 4. Transformer Era

- **Transformer** (2017) â€“ [Attention is All You Need](https://arxiv.org/abs/1706.03762)
- **BERT** (2018) â€“ [BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805)
- **GPT-1** (2018) â€“ [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
- **GPT-2** (2019) â€“ [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- **GPT-3** (2020) â€“ [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)

---

## ðŸ”¹ 5. LLM Optimization and Scaling

- **Transformer-XL** (2019) â€“ [Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860)
- **Reformer** (2020) â€“ [The Efficient Transformer](https://arxiv.org/abs/2001.04451)
- **T5** (2020) â€“ [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683)
- **Chinchilla** (2022) â€“ [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556)
- **PaLM** (2022) â€“ [Pathways Language Model](https://arxiv.org/abs/2204.02311)

---

## ðŸ”¹ 6. RLHF and Instruction Tuning

- **InstructGPT** (2022) â€“ [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155)
- **ChatGPT** (2022) â€“ Based on InstructGPT + GPT-3.5 + alignment tuning
- **RLHF Overview** â€“ [Deep RL from Human Preferences (2017)](https://arxiv.org/abs/1706.03741)

---

## ðŸ”¹ 7. Retrieval-Augmented Models

- **REALM** (2020) â€“ [Retrieval-Augmented Language Model Pretraining](https://arxiv.org/abs/2002.08909)
- **RAG** (2020) â€“ [Retrieval-Augmented Generation](https://arxiv.org/abs/2005.11401)
- **RETRO** (2021) â€“ [Improving Language Modeling with Retrieval](https://arxiv.org/abs/2112.04426)

---

## ðŸ”¹ 8. Multimodal & Vision + Language

- **CLIP** (2021) â€“ [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020)
- **BLIP** (2022) â€“ [Bootstrapping Language-Image Pretraining](https://arxiv.org/abs/2201.12086)
- **Flamingo** (2022) â€“ [A Visual Language Model for Few-Shot Learning](https://arxiv.org/abs/2204.14198)

---

## ðŸ”¹ 9. Open Models & Fine-Tuning

- **LLaMA** (2023) â€“ [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)
- **Alpaca** (2023) â€“ [Stanford Alpaca: An Instruction-following LLaMA Model](https://crfm.stanford.edu/2023/03/13/alpaca.html)
- **LoRA** (2021) â€“ [Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)
- **PEFT** (2022) â€“ [Parameter-Efficient Transfer Learning](https://arxiv.org/abs/2203.15556)

---

## ðŸ“Œ Bonus: Survey & Review Papers

- [A Survey of Transformer Models in NLP](https://arxiv.org/abs/2006.05138)
- [Pretrain Prompt Predict: A Systematic Survey of Prompting Methods](https://arxiv.org/abs/2107.13586)
- [A Survey on Large Language Models](https://arxiv.org/abs/2303.18223)

---

> âœ… Keep updating this list as the field evolves. Reading 1â€“2 papers/week with implementation is an ideal pace.
