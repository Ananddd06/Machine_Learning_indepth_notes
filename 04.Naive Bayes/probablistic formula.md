# Naive Bayes: A Probabilistic Generative Learning Algorithm

Naive Bayes is a **probabilistic classifier** based on applying **Bayes' Theorem** with the _naive assumption_ that features are conditionally independent given the class.

It belongs to the family of **generative learning algorithms**, because instead of directly learning a discriminative boundary between classes (like Logistic Regression), it models the **joint distribution** of inputs and outputs:

$$
P(X, Y) = P(Y) \\, P(X | Y)
$$

where:

- $X = (x_1, x_2, \\dots, x_n)$ are the features,
- $Y$ is the class label.

---

## Bayes’ Theorem (Foundation)

The posterior probability of a class given features is computed using Bayes’ theorem:

$$
P(Y | X) = \\frac{P(X | Y) \\, P(Y)}{P(X)}
$$

where:

- $P(Y)$ = prior probability of class $Y$,
- $P(X|Y)$ = likelihood of observing features $X$ given $Y$,
- $P(X)$ = evidence or normalization constant.

For classification, we only care about the **argmax**:

$$
\\hat{y} = \\underset{y}{\\arg\\max} \\, P(Y = y | X) = \\underset{y}{\\arg\\max} \\, P(X | Y = y) \\, P(Y = y)
$$

since $P(X)$ is constant across classes.

---

## The Naive Independence Assumption

The naive assumption is that **all features are conditionally independent given the class**:

$$
P(X | Y) = P(x_1, x_2, \\dots, x_n | Y) = \\prod_{i=1}^n P(x_i | Y)
$$

This simplifies computation drastically, because instead of estimating a high-dimensional joint distribution, we only need univariate conditional distributions.

---

## Naive Bayes Classification Rule

Thus, the decision rule becomes:

$$
\\hat{y} = \\underset{y}{\\arg\\max} \\, P(Y = y) \\, \\prod_{i=1}^n P(x_i | Y = y)
$$

This is the **Naive Bayes classifier**.

---

## Probabilistic Perspective

- **Generative View**: Naive Bayes models how the data is generated by learning:

  - Prior distribution $P(Y)$ over classes.
  - Likelihood distribution $P(x_i|Y)$ for each feature.

- **Classification**: Uses Bayes' rule to invert this generative model and predict the most probable class given features.

- **Why Generative?** Because it explicitly models the joint probability distribution $P(X, Y)$, unlike discriminative models (e.g., Logistic Regression) which model $P(Y|X)$ directly.

---

## Types of Naive Bayes Models

Depending on the feature distribution assumptions:

- **Multinomial Naive Bayes** – for word counts in documents (text classification).
- **Bernoulli Naive Bayes** – for binary features (presence/absence of a word).
- **Gaussian Naive Bayes** – for continuous features (assumed normal distribution).

---

## Summary

Naive Bayes is simple yet powerful:

- It is **probabilistic**: provides interpretable probability estimates.
- It is **generative**: learns how data is generated and uses Bayes' rule for classification.
- It is **naive**: assumes feature independence, which is rarely true in practice, but often works surprisingly well.
